<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>4. The Hessian Matrix</title>
  <link rel="stylesheet" href="/kyo-study/css/main.css">
  <link rel="stylesheet" href="/kyo-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-study/docs/ml_study/chapter05/4.html">
  <script src="/kyo-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-study/"><strong>kyo-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/kyo-study/categories/paper_study"><strong>Review Of Papers</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/opt_study"><strong>Introduction to Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">Machine Learning</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 5</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter05/0">0. Introduction</a>
      </li>
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter05/1">1. Feed-forward Network</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter05/2">2. Network Training</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter05/3">3. Error Backpropagation</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-study/docs/ml_study/chapter05/4">4. The Hessian Matrix</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter05/5">5. Regularization in NN.</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-study/blob/gh-pages/docs/ml_study/chapter05/4.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>4. The Hessian Matrix</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>지금까지 우리는 backprop 기법을 이용하여 에러 함수에 대한 \( {\bf w} \) 1차 미분값을 계산할 수 있었다.</li>
  <li>이를 확장하여 2차 미분값인 Hessian 행렬을 계산하는 방법을 살펴보도록 하자.</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial^2 E}{\partial w_{ji} \partial w_{lk} } \qquad{(5.78)}</script>

<ul>
  <li>여기서 \( i, j \in \{ 1, …, W \} \) 이고 \( W \) 는 모든 weight 와 bias를 포함한다.</li>
  <li>이 때 각각의 2차 미분 값을 \( H_{ij} \) 로 표기하고 이것으로 만들어지는 행렬을 헤시안(Hessian) 행렬 \( {\bf H} \) 라고 정의한다.</li>
  <li>신경망에서 헤시안 행렬은 중요하게 여겨진다.
    <ul>
      <li>일부 비선형 최적화 알고리즘에서 에러 곡면의 2차 미분 값을 사용한다.</li>
      <li>이미 학습이 완료된 신경망에 조금 변경된 데이터를 입력으로 주고 빠르게 재학습 시킬 때 사용된다.</li>
      <li>‘pruning’ 일고리즘의 일부로 <em>least significant weights</em> 를 식별할 때 헤시안 역행렬이 사용된다.</li>
      <li>베이지안 신경망을 위한 라플라스 근사식에서 중요한 역할을 차지한다. (5.7절 참고)</li>
    </ul>
  </li>
  <li>헤시안 행렬의 연산량은 매우 높다.
    <ul>
      <li>신경망에 \( W \) 개의 weight 가 존재한다면 헤시안은 \( W \times W \) 행렬이 된다.</li>
      <li>따라서 연산량은 입력 샘플당 \( O(W^2) \) 이 필요하다.</li>
    </ul>
  </li>
</ul>

<h3 id="diagonal-approximation">5.4.1. 대각 근사 ( Diagonal approximation)</h3>

<ul>
  <li>헤시안 행렬을 사용하는 많은 경우에서 헤시안 행렬의 역행렬(inverse)을 이용하는 경우가 많다.</li>
  <li>이 때 정방 행렬인 헤시안 행렬의 역행렬이 존재하려면 (즉, invertible or nonsingular) \(det({\bf H})\) 의 값이 \(0\) 이 아니어야 한다.</li>
  <li>이 때 헤시안 행렬을 대각 근사하면 \(det({\bf H})\) 가 \(0\) 인 경우가 발생하지 않는다.
    <ul>
      <li>즉, 대각만 남기고 다른 값들을 모두 0으로 치환</li>
      <li>대각 행렬의 경우 역행렬을 구할 수 있음이 보장된다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\frac{\partial^2 E_n}{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_j}z^2_i \qquad{(5.79)}</script>

<ul>
  <li>참고로 위의 식은 다음과 같이 전개하여 얻은 식이다.</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial^2 E_n}{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_{j}} \frac{\partial a^2_{j} }{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_j}z^2_i</script>

<ul>
  <li>식(5.48)과 식((5.49)를 사용하여 식(5.79)의 오른쪽 항을 체인(chain) 형태로 전개할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial^2 E_n}{\partial a^2_j} = h'(a_j)^2\sum_k\sum_{k'} w_{kj}w_{k'j}\frac{\partial^2 E_n}{\partial{a_k} \partial{a_{k'}} } + h''(a_j)\sum_k w_{kj}\frac{\partial E_n}{\partial a_k} \qquad{(5.80)}</script>

<ul>
  <li>참고로 \(a_k\) 와 \(a_{k’}\) 는 동일한 레이어의 유닛을 의미한다. \(k\) 와 \(k’\)가 다른 경우를 모두 무시하면 다음 식을 얻게 된다.
    <ul>
      <li>즉, 대각만 남기고 다른 요소들은 모두 삭제한다.</li>
      <li><em>Becker &amp; Le Cun</em> (1989)</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\frac{\partial^2 E_n}{\partial a^2_j} = h'(a_j)^2\sum_k w^2_{kj}\frac{\partial^2 E_n}{a^2_k} + h''(a_j)\sum_k w_{kj}\frac{\partial E_n}{\partial a_k} \qquad{(5.81)}</script>

<ul>
  <li>이 때의 연산 비용은 \(O(W)\) 가 된다. (실제 헤시안 행렬은 \(O(W^2)\)임을 이미 확인했다.)</li>
  <li>게다가 연산도 쉬운데 backprop 에서 얻어진 에러 \( \delta \) 를 이용하여 \( \frac {\partial^2 E_n }{\partial a^2_j} \) 만 구하면 헤시한 행렬을 구할 수 있다.</li>
  <li>하지만 현실적으로 헤시안 행렬 자체는 대각행렬만으로 구성되는 경우는 거의 없다.
    <ul>
      <li>따라서 헤시안 행렬의 대각 근사 자체를 잘 사용하지 않는다. (왜 다룬거야!)</li>
    </ul>
  </li>
</ul>

<h3 id="outer-product-approximation">5.4.2. 외적 근사 (Outer product approximation)</h3>

<ul>
  <li>회귀(regression) 문제로 돌아가 에러 함수를 잠시 꺼내와보자.</li>
</ul>

<script type="math/tex; mode=display">E = \frac{1}{2} \sum_{n=1}^N (y_n - t_n)^2 \qquad{(5.82)}</script>

<ul>
  <li>이에 대한 헤시안 행렬은 다음과 같이 구할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">{\bf H} = \nabla \nabla E = \sum_{n=1}^N \nabla y_n (\nabla y_n)^T + \sum_{n=1}^N(y_n - t_n)\nabla\nabla y_n \qquad{(5.83)}</script>

<ul>
  <li>만약 충분한 데이터로 학습이 잘 이루어져서 네크워크 망의 출력값 \(y_n\) 가 \(t_n\) 와 매우 비슷한 값을 내어준다면 위의 식에서 2번째 텀은 생략 가능하다.
    <ul>
      <li>물론 제대로 하려면 섹션 1.5.5 에서 다루었듯 출력 값의 조건부 평균을 사용해야 겠지만 일단 넘어가자.</li>
      <li>식(5.83)에서 두번째 텀을 생략한 식을 <em>Levenberg-Marquardt</em> 근사 또는 외적 근사(outer product approx.)라고 부른다.</li>
      <li>사실 헤시안 행렬 자체가 외적의 합으로 이루어진 행렬이다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\bf H} \simeq \sum_{n=1}^N {\bf b_n}{\bf b_n}^T \qquad{(5.84)}</script>

<ul>
  <li>여기서 \({\bf b_n} \equiv \nabla a_n = y_n\) 이다.
    <ul>
      <li>회귀 문제에서는 활성(activation) 함수가 identity 이기 때문이다.</li>
    </ul>
  </li>
  <li>이 근사법은 헤시안을 구하기 위해 오로지 1차 미분만을 요구하고 backprop 시 \(O(W)\) 에 값을 구할 수 있다.</li>
  <li>
    <p>그리고 \(W\) 차원의 두 벡터를 외적할 때 \( O(W^2) \) 이 요구된다.</p>
  </li>
  <li>로지스틱 시그모이드(logistic sigmoid) 활성 함수를 가지는 크로스 엔트로피(cross-entropy)를  사용하는 경우 근사식은 다음과 같이 된다.</li>
</ul>

<script type="math/tex; mode=display">{\bf H} \simeq \sum_{n=1}^N y_n(1-y_n){\bf b_n}{\bf b_n}^T \qquad{(5.85)}</script>

<ul>
  <li>다중부류(multi-class)로도 쉽게 확장된다.</li>
</ul>

<h3 id="inverse-hessian">5.4.3. 역 헤시안 (Inverse Hessian)</h3>

<ul>
  <li>헤시안을 응용하는 곳에서는 주로 역 헤시안을 사용하는 경우가 많다. (신경망에서도!)</li>
  <li>앞서 살펴보았던 외적 근사 기법을 통해 헤시안의 역행렬을 구하는 방법을 살펴볼 것이다.</li>
  <li>끝에 나오는 quasi-Newton 을 주의깊게 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">{\bf H}_N = \sum_{n=1}^N {\bf b_n}{\bf b_n}^T \qquad{(5.86)}</script>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoon'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
