<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>5. The Evidence Approximation</title>
  <link rel="stylesheet" href="/kyo-study/css/main.css">
  <link rel="stylesheet" href="/kyo-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-study/docs/ml_study/chapter03/5.html">
  <script src="/kyo-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-study/"><strong>kyo-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/kyo-study/categories/paper_study"><strong>Review Of Papers</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/opt_study"><strong>Introduction to Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">Machine Learning</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 3</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/0">0. Linear Models for Regression</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/1">1. Linear Basis Function Models</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/2">2. Bias-Variance Decomposition</a>
      </li>
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/3">3. Bayesian Linear Regression</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/4">4. Bayesian Model Comparison</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-study/docs/ml_study/chapter03/5">5. The Evidence Approximation</a>
      </li>
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/6">6. Limit. of Fixed Basis Functions</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-study/blob/gh-pages/docs/ml_study/chapter03/5.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>5. The Evidence Approximation</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>완전한 ( <em>full</em> ) 베이지언 패러다임의 선형 기저 함수 모델에서는 초모수 \( \alpha \) , \( \beta \) 가 사용된다.
    <ul>
      <li>여기서 \( \alpha \) 는 파라미터 \( {\bf w} \) 에 대한 분산을, \( \beta \) 는 \( {\bf t} \) 에 대한 분산을 의미한다.</li>
      <li>또한 파라미터 \( {\bf w} \) 를 추정하는 것 외에 하이퍼 파라미터(초모수)들을 이용하여 예측 분포를 만드는 것을 살펴보았다.</li>
      <li>하지만 이들을 주변화(marginalization)하는 것는 수학적으로 무척이나 힘든 것이라는 것도 확인하였다.</li>
      <li>그리고 이런 경우를 <em>analytically intractible</em> 하다고 한다.</li>
    </ul>
  </li>
  <li>따라서 이 식을 바로 구할 수 있는 방법이 없기 때문에 이 식을 근사(approximation)하는 방법에 대해 이야기할 것이다.
    <ul>
      <li>주변 가능도 함수(marginal likilihood function) 를 최대화시키는 방식을 이용할 것이다.</li>
    </ul>
  </li>
  <li>이 프레임워크는 <em>empirical Bayes</em> 라고도 알려져있다.
    <ul>
      <li>혹은 “<em>type2 MLE</em>” 혹은 “<em>evidence approximation</em>” 이라고도 한다.</li>
    </ul>
  </li>
  <li>초모수 \( \alpha \) 와 \( \beta \) 를 이용하여 완전한 형태의 베이지언 예측분포를 나타내어 보자.</li>
</ul>

<script type="math/tex; mode=display">p(t|{\bf t}) = \iiint p(t|{\bf w}, \beta) p({\bf w}|{\bf t}, \alpha, \beta) p(\alpha, \beta | {\bf t}) d{\bf w}d\alpha d\beta \qquad{(3.74)}</script>

<ul>
  <li>여기서 \( p(t|{\bf w}, \beta) \) 는 식 (3.8) 에서 주어졌다. 그리고 \( p({\bf w}|{\bf t}, \alpha, \beta) \) 는 식 (3.49) 를 살펴보면 된다.
    <ul>
      <li>이 때 \( {\bf m}_N \) , \( {\bf S}_n \)  은 식(3.53) 과 식(3.54)에 나와있다.</li>
    </ul>
  </li>
  <li>
    <p>여기서 입력 변수 \( {\bf x} \) 는 모든 영역에 대해 영향을 미치고 있으므로 생략한다.</p>
  </li>
  <li>이 때 사후분포 \( p(\alpha, \beta|{\bf t}) \) 가 특정한 지점 \( \widehat{\alpha} \) 와 \( \widehat{\beta} \) 에서 최고점을 만든다고 생각해보자.
    <ul>
      <li>이 때 우리는 \( \alpha \) 와 \( \beta \) 를 각각 \( \widehat{\alpha} \) 와 \( \widehat{\beta} \) 라 가정하고 파라미터 \( {\bf w} \) 를 주변화하면 된다.</li>
      <li>이런 가정을 근사(approximation) 과정이라고 생각하는 것이다.</li>
    </ul>
  </li>
  <li>이러면 식을 다음과 같이 좀 더 간단하게 기술할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p(t|{\bf t}) \simeq p(t|{\bf t}, \widehat{\alpha}, \widehat{\beta}) = \int p(t|{\bf w}, \widehat{\beta})p({\bf w}|{\bf t}, \widehat{\alpha}, \widehat{\beta})d{\bf w} \qquad{(3.75)}</script>

<ul>
  <li>이제 적당한 \( \widehat{\alpha} \) 와 \( \widehat{\beta} \) 를 구하면 된다. (당연히 mode 값 등을 사용할 것이다.)</li>
  <li>이를 구하기 위해 베이즈 이론으로 식을 전개하도록 하자.</li>
</ul>

<script type="math/tex; mode=display">p(\alpha, \beta | {\bf t}) \propto p({\bf t}|\alpha, \beta)p(\alpha, \beta) \qquad{(3.76)}</script>

<ul>
  <li>이 때 하이퍼 파라미터의 사전분포는 동일하다고 <strong>가정</strong>해보자.</li>
  <li>따라서 이제 첫번째 텀에만 집중한다. \( p({\bf t}|\alpha, \beta) \) 를 주변 가능도 함수(marginal likelihood function)라고 한다.
    <ul>
      <li>우리는 선형 기저 함수모델을 위한 가능도 함수 전개 방식을 그대로 사용할 것이다.</li>
      <li>이로부터 초모수를 추정할 수 있다.
        <ul>
          <li>cross-validation 등을 사용하지 않고 단일 샘플 집합으로부터 이 값을 추정하도록 한다.</li>
          <li>참고로 이 때 \( \alpha\;/\;\beta \) 식이 정칙화 파라미터로 사용된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이와 별개로, \( \alpha \) 와 \( \beta \) 의 사전 분포로 감마 분포를 사용하는 경우 식 (3.74)에서 \( {\bf w} \) 에 대한 함수는 스튜어트 \( {\bf t} \) 분포로 주어진다.
    <ul>
      <li>이런 경우 작업이 가능한 형태로 전개가 된다고 생각할 수 있으나 최종 \( {\bf w} \) 에 대한 적분식은 여전히 처리 불가능하다.(intractible)</li>
      <li>물론 이 때의 적분식은 가우시안 근사를 기본으로 하는 라플라스 근사 방식을 도입하여 풀 수 있기는 하다.
        <ul>
          <li>4.4절에 이와 관련된 내용을 설명한다.</li>
          <li>그리고 사실 이게 좀 더 현실적인 대안이기는 하다.</li>
          <li>하지만 라플라스 근사법도 분포가 한쪽으로 심하게 쏠린(skewed) 경우에는 잘 맞지 않는 근사법이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>다시 <em>evidence</em> 방식으로 돌아가 이야기를 해보자면 크게 2가지 방식으로 나눌 수 있다.
    <ul>
      <li>3.5.2 절에서 설명하고 있는 미분 최대화 방식과,</li>
      <li>9.3.4 절에서 설명하고 있는 <em>EM</em> 알고리즘을 사용하는 방식이다.</li>
      <li>걱정하지 말자. 이후에 모두 살펴볼 것이다.</li>
    </ul>
  </li>
</ul>

<h3 id="evidence--">3.5.1 증거(evidence) 함수의 평가</h3>

<ul>
  <li>주변 가능도 함수 \( p({\bf t}|\alpha, \beta) \) 는 다음의 식으로 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf t}|\alpha, \beta) = \int p({\bf t}|{\bf w}, \beta) p({\bf w}|\alpha) d{\bf w} \qquad{(3.77)}</script>

<ul>
  <li>위의 식은 식(2.15)에서 언급한 선형 가우시안 모델의 조건부 분포를 이용하여 구할 수 있다.</li>
  <li>하지만 여기서는 적분을 수행하는 대신 지수부에 포함된 제곱식을을 이용하여 일반화된 형태로 전개할 것이다.
    <ul>
      <li>정규화된 가우시안 계수를 가지는 표준 형태로 만든다.</li>
    </ul>
  </li>
  <li>책에는 전개 식이 안 나와있다. 최종 식을 보도록 하자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf t}|\alpha, \beta) = \left(\frac{\beta}{2\pi}\right)^{N/2}\left(\frac{\alpha}{2\pi}\right)^{M/2} \int \exp\{-E({\bf w})\}d{\bf w} \qquad{(3.78)}</script>

<ul>
  <li>여기서 \( M \) 은 \( {\bf w} \) 의 차원을 나타낸다.</li>
</ul>

<script type="math/tex; mode=display">E({\bf w}) = \beta E_D({\bf w}) + \alpha E_w({\bf w}) = \frac{\beta}{2}\|{\bf t} - \Phi{\bf w}\|^2 + \frac{\alpha}{2}{\bf w}^T{\bf w} \qquad{(3.79)}</script>

<ul>
  <li>이 식은 식(3.27)과 상수만 다를 뿐 동일한 식이다.</li>
  <li>참고삼아 식(3.27)을 간단히 보고 넘어가도록 하자.</li>
</ul>

<script type="math/tex; mode=display">E({\bf w})=\dfrac{1}{2}\sum_{n=1}^{N}\{t_n-{\bf w}^T\phi({\bf x}_n)\}^2+\dfrac{1}{2}{\bf w^Tw} \qquad{(3.27)}</script>

<ul>
  <li>따라서 이 식을 앞서 전개한 방식과 동일하게 처리 가능하다.</li>
</ul>

<script type="math/tex; mode=display">E({\bf w}) = E({\bf m}_N)+\frac{1}{2}({\bf w}-{\bf m}_N)^T {\bf A} ({\bf w} - {\bf m}_N) \qquad{(3.80)}</script>

<ul>
  <li>이 때,</li>
</ul>

<script type="math/tex; mode=display">{\bf A} = \alpha {\bf I} + \beta \Phi^T\Phi \qquad{(3.81)}</script>

<ul>
  <li>이 식은 사실 식(3.51) 에 대응되는 식이다.</li>
</ul>

<script type="math/tex; mode=display">E({\bf m}_N) = \frac{\beta}{2}\| {\bf t} - \Phi{\bf m}_N\|^2 + \frac{\alpha}{2}{\bf m}_N^T{\bf m}_N \qquad{(3.82)}</script>

<ul>
  <li>식을 잘 살펴보면 \( {\bf A} \) 는 다음과 같이 에러 함수를 두번 미분한 것과 같다.</li>
</ul>

<script type="math/tex; mode=display">{\bf A} = \nabla \nabla E({\bf w}) \qquad{(3.83)}</script>

<ul>
  <li>이 식은 헤시안 행렬(Hessian matrix)로 잘 알려져있다.</li>
  <li>이 때 \( {\bf m}_N \) 은 다음과 같이 정의된다.</li>
</ul>

<script type="math/tex; mode=display">{\bf m}_N = \beta {\bf A}^{-1}\Phi^T{\bf t} \qquad{(3.84)}</script>

<ul>
  <li>식(3.54)에서 보면 \( {\bf A} = {\bf S}_N^{-1} \) 로 생각할 수 있다.</li>
  <li>이제 \( {\bf w} \) 에 대한 적분식을 살펴보도록 한다.</li>
</ul>

<script type="math/tex; mode=display">\int \exp\left(-E({\bf w})\right) d{\bf w} = \exp(-E({\bf m}_N))\int \exp\left\{ -\frac{1}{2}({\bf w}-{\bf m}_N)^T {\bf A} ({\bf w}-{\bf m}_N) \right\}d{ \bf w}\\=\exp\left\{-E({\bf m}_N)\right\}(2\pi)^{M/2}|{\bf A}|^{-1/2} \qquad{(3.85)}</script>

<ul>
  <li>이제 최종적으로 로그 주변 가능도 함수를 구해보도록 한다.</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf t}|\alpha, \beta) = \frac{M}{2}\ln \alpha + \frac{N}{2}\ln \beta - E({\bf m}_n) - \frac{1}{2}\ln |{\bf A}| - \frac{N}{2}\ln(2\pi) \qquad{(3.86)}</script>

<ul>
  <li>이 값을 이용하여 적절한 \( M \) 값을 찾는 방법을 살펴보자.</li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure3.14.png" alt="figure3.14" class="center-block" height="200px" /></p>

<ul>
  <li>위의 그림은 polynomial regression 문제를 위의 식을 통해 구한 결과이다.</li>
  <li>이 때 \( \alpha = 5 \times 10^{-3} \) 으로 고정되어 있다.</li>
  <li>여러가지 \( M \) 값을 이용하여 로그 가능도 함수 값을 살펴본다.
    <ul>
      <li>식을 통해 \( M=3 \) 일 때 가장 높은 확률 값을 찾아낼 수 있었다.</li>
    </ul>
  </li>
</ul>

<h3 id="evidence---1">3.5.2 증거(evidence) 함수 최대화하기</h3>

<ul>
  <li>\( \alpha \) 에 관한 \( p({\bf t}|\alpha, \beta) \) 를 최대화하던 문제를 좀 더 생각해보자.</li>
</ul>

<script type="math/tex; mode=display">(\beta \Phi^T \Phi) {\bf \mu}_i = \lambda_i{\bf \mu}_i \qquad{(3.87)}</script>

<ul>
  <li>고유(eigen) 벡터 식에 의해 \( (\beta \Phi^T \Phi) \) 는 위와 같은 조건을 만족하게 된다.</li>
  <li>따라서 \( {\bf A} \) 는 식 (3.81) 식에 의해 \( {\bf A} = \alpha + \lambda_i \) 로 생각할 수 있다.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>이제 식 (3.86)에서 사용되는 \( \ln |{\bf A}</td>
          <td>\) 에 이를 대입하여 미분해보자.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<script type="math/tex; mode=display">\frac{d}{d\alpha}\ln |{\bf A}| = \frac{d}{d\alpha}\ln \prod_{i}(\lambda_i+\alpha) = \frac{d}{d\alpha}\sum_i \ln(\lambda_i+\alpha) = \sum_i \frac{1}{\lambda_i + \alpha} \qquad{(3.88)}</script>

<ul>
  <li>이제 위의 식을 포함하여 최종적으로 식 (3.86)을 \( \alpha \) 로 미분한 식을 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">0 = \frac{M}{2\alpha} - \frac{1}{2}{\bf m}_N^T{\bf m}_N - \frac{1}{2}\sum_i \frac{1}{\lambda_i+\alpha} \qquad{(3.89)}</script>

<ul>
  <li>우리는 \( \alpha \) 에 관한 식을 알고 싶기 때문에 이 변수에 대해 정리를 한다.</li>
</ul>

<script type="math/tex; mode=display">\alpha {\bf m}_N^T{\bf m}_N = M - \alpha\sum_i \frac{1}{\lambda_i+\alpha} = \gamma \qquad{(3.90)}</script>

<ul>
  <li>예전에 비슷한 식을 본 적이 있긴 하다.</li>
  <li>일단 한 변수로 완전히 정리가 되지 않기 때문에 \( \gamma \) 를 도입한다.</li>
  <li>이제 \( \gamma \) 를 다음과 같이 정리 가능하다.</li>
</ul>

<script type="math/tex; mode=display">\gamma = \sum_i \frac{\lambda_i}{\alpha + \lambda_i} \qquad{(3.91)}</script>

<ul>
  <li>이제 최종적으로 \( \alpha \) 에 대한 식을 \( \gamma \) 를 통해 정리 가능하다.</li>
</ul>

<script type="math/tex; mode=display">\alpha = \frac{\gamma}{ {\bf m}_N^T{\bf m}_N} \qquad{(3.92)}</script>

<ul>
  <li>식을 잘 보면 \( \alpha \) 값을 구하기 위해 \( \gamma \) 가 필요한데 다시 이 내부에서 \( \alpha \) 가 필요한 상황이다.</li>
  <li>결국은 <em>Iterative</em> 모델로 이 값을 결정하게 된다.</li>
  <li>값이 반복되기 때문에 \( \alpha \) 의 값이 수렴될 때까지 반복하면 된다.
    <ul>
      <li>우선 초기값으로 \( \alpha \) 값을 임의로 정한 다음 이 때의 \( {\bf m}_N \) 과 \( \gamma \) 값을 구해 다시 \( \alpha \) 값 구하는 것을 반복한다.</li>
      <li>계산이 복잡해보이지만 실제로는 \( \Phi^T\Phi \) 는 고정값이므로 그리 높은 연산을 요구하지는 않는다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>이제 \( \beta \) 에 대해서도 비슷한 방식으로 전개 가능하다.</li>
  <li>식 (3.87)에서 사용된 고유값(eigen value) \( \lambda_i \) 에 대한 \( \beta \) 의 미분식은 \( d\lambda_i/d\beta = \lambda_i/\beta \) 로 처리하면 된다.</li>
</ul>

<script type="math/tex; mode=display">\frac{d}{d\beta} \ln |{\bf A}| = \frac{d}{d\beta}\sum_i \ln(\lambda_i+\alpha) = \frac{1}{\beta}\sum_i\frac{\lambda_i}{\lambda_i+\alpha} = \frac{\gamma}{\beta} \qquad{(3.93)}</script>

<ul>
  <li>편의상 식 전개가 좀 생략되어 있다.</li>
  <li>이제 미분을 해보자. (stationary point 찾기)</li>
</ul>

<script type="math/tex; mode=display">0 = \frac{N}{2\beta} - \frac{1}{2}\sum_{n=1}^N\{t_n-{\bf m}_N^T\phi({\bf x}_n)\}^2 - \frac{\gamma}{2\beta} \qquad{(3.94)}</script>

<ul>
  <li>정리하면 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">\frac{1}{\beta} = \frac{1}{N-\gamma}\sum_{n=1}^N \{t_n-{\bf m}_N^T\phi({\bf x}_N)\}^2 \qquad{(3.95)}</script>

<ul>
  <li>앞선 식을 생각하여 \( \alpha \) 와 \( \beta \) 를 정한뒤 다시 \( \gamma \) 를 구하고 반복하는 방식을 사용할 수 있다.</li>
</ul>

<h3 id="effective-number-of-parameters">3.5.3 효율적인 파라미터 수 (Effective number of parameters)</h3>
<ul>
  <li>(참고) 이번 절은 내용을 대략적으로만 이해할 수 있어서 잘못된 내용이 포함될 가능성이 많다.</li>
  <li>베이지언 추론 방식에서 식 (3.92) 의 \( \alpha \) 가 주는 의미가 무엇인지를 살펴보기로 하자.</li>
  <li>아래 그림을 보자.</li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure3.15.png" alt="figure3.15" class="center-block" height="200px" /></p>

<ul>
  <li>붉은 색의 컨투어는 가능도 함수를 의미한다.
    <ul>
      <li>일단 \( {\bf w}_i \) 에 대하여 상관 계수가 0 인 형태의 컨투어를 형성하고 있다.</li>
      <li>이는 \( {\bf w}_i \) 가 \( \beta \Phi^T\Phi u_i = \lambda_i u_i \) 식과 같이 고유 값으로 변환 가능하므로 이를 고유 벡터 차원으로 변환한 뒤 나타낸 그림이다.</li>
      <li>개념적으로 보면 어차피 이 가능도 함수는 \( {\bf w} \) 에 대한 이차형식(quadratic)으로 주어진다.</li>
      <li>\( \beta \Phi^T\Phi \) 가 대칭 행렬이므로 양의 정방향 행렬(positive definite matrix)이기 때문에 때문에 고유 값은 양수이다.</li>
      <li>만약 \( \alpha = 0 \) 이면 사후 분포의 모드(mode) 값은 \( {\bf w}_{ML} \) 로 사용하면 된다.</li>
      <li>만약 \( \alpha \) 가 0 이 아니라면 모드(mode) 값은 \( {\bf w}_{MAP} = {\bf m}_N \) 이다.
        <ul>
          <li>가우시안 분포를 사용했기 때문에 모드와 기대값이 값다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>식 (3.92) 를 통해 \( \gamma \) 로부터 가장 효율적인 파라미터 수를 확인할 수 있다.
    <ul>
      <li>만약 \( \alpha « \lambda_i \) 인 경우 \( \gamma_i=1 \) 이 된다.</li>
      <li>반면 \( \alpha » \lambda_i \) 인 경우 \( \gamma_i=0 \) 이 된다.</li>
      <li>따라서 \( \gamma \) 는 \( 0 \le \gamma \le M \) 이 되게 된다. ( \( i=M \) 이므로 )</li>
    </ul>
  </li>
  <li>그리고 식 (3.95) 를 보면 \( \beta \) 를 추정하는데 있어서 \( N-\gamma \) 가 사용된다는 것을 알 수 있다.</li>
  <li>1장에서 다루었던 가우시안 분산에 대한 <em>MLE</em> 추적을 잠시 떠올려보자. (식 (1.56)이다.)
    <ul>
      <li>이 때 분산 값은 다음과 같이 추정되었다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\sigma_{ML}^2 = \frac{1}{N}\sum_{n=1}^N (x_n - \mu_{ML})^2 \qquad{(3.96)}</script>

<ul>
  <li>사실 이것은 샘플에 의해 편향(bias)된 결과이고 이미 비편향된 결과는 다음과 같다는 사실을 이미 알고 있다.</li>
</ul>

<script type="math/tex; mode=display">\sigma_{MAP}^2 = \frac{1}{N-1}\sum_{n=1}^N (x_n - \mu_{ML})^2 \qquad{(3.96)}</script>

<ul>
  <li>이것은 분명 우리가 얻은 결과인 \( \frac{1}{N-\gamma} \) 와 연결되어 있다.
    <ul>
      <li>바로 베이지언 방식은 이미 자유도(degree of freedom)의 개념이 식 자체에 포함되어 있다는 것이다.</li>
      <li>그리고 앞선 식에서 \( -1 \) 이 추가된 것은 파라미터 평균(means) 때문이다.</li>
      <li>결국 파라미터의 수를 제어하는 요소처럼 반영된다는 것.</li>
    </ul>
  </li>
  <li>지금까지 예로 들었던 것처럼 우리는 일단 \( 9 \) 개의 가우시언 기저 함수를 (Gauyssian basis fucntion) 사용할 것이다.
    <ul>
      <li>잘 기억이 나지 않는다면 1.1절을 참고해보자.</li>
      <li>따라서 bias 를 반영하여 총 \( 10 \) 개의 파라미터를 가진 모델을 생ㅇ각하자.</li>
      <li>그리고 베이지언 방식을 사용한다고 하였으므로 초모수(hyper-parameter) \( \alpha \) 와 \( \beta \) 도 사용된다.</li>
    </ul>
  </li>
  <li>일단 샘플의 개수는 \( N \) 이고 \( N » M \) 이라고 가정하자.
    <ul>
      <li>사용할 모델의 개수를 \( \gamma = M \) 이라고 해서 식을 전개해보면,</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\alpha = \frac{M}{2E_W({\bf m}_N)} \qquad{(3.98)}</script>

<script type="math/tex; mode=display">\beta = \frac{N}{2E_D({\bf m}_N)} \qquad{(3.99)}</script>

<ul>
  <li>여기서 \( E_W \) 와 \( E_D \) 는 각각 식 (3.25)와 식 (3.26) 에서 주어진 식이다.</li>
</ul>

<div class="text-center">
  <img src="/kyo-study/images/ml_study/Figure3.16a.png" alt="Figure 3.16a" height="180px" />
  <img src="/kyo-study/images/ml_study/Figure3.16b.png" alt="Figure 3.16b" height="180px" />
</div>

<ul>
  <li>왼쪽
    <ul>
      <li>붉은 색 선은 \( \gamma \) 값이고, 파란 색은 \( 2\alpha E_W({\bf m}_N) \) 의 값이다.</li>
      <li>두 점이 만나는 지점을 주목하자.</li>
    </ul>
  </li>
  <li>오른쪽
    <ul>
      <li>붉은 색 선은 \( \ln p(t|\alpha, \beta) \) 이고 파란 색은 테스트 에러 값이다.</li>
    </ul>
  </li>
  <li>두 그림 다 동일한 지점에서의 \( \alpha \) 값을 표현할 수 있다.
    <ul>
      <li>왼쪽은 두 점이 만나는 지점.</li>
      <li>오른 쪽은 \( \ln p(t|\alpha, \beta) \) 가 최대가 되는 지점.</li>
    </ul>
  </li>
  <li>마지막으로 \( \alpha \) 값이 어떻게 \( w_i \) 값을 조절하게 되는지 그림으로 확인해보자.</li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure3.17.png" alt="figure3.17" class="center-block" height="200px" /></p>

<ul>
  <li>총 10개의 파라미터가 있다. (가우시안 기저 함수에 대한 계수)</li>
  <li>\( \gamma \) 에 따라 \( w_i \) 값이 달라지게 된다.</li>
  <li>솔직하게 이번 절의 후반부는 자세히 보지를 못했다.</li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoon'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
