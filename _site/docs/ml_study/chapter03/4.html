<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>4. Bayesian Model Comparison</title>
  <link rel="stylesheet" href="/kyo-study/css/main.css">
  <link rel="stylesheet" href="/kyo-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-study/docs/ml_study/chapter03/4.html">
  <script src="/kyo-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-study/"><strong>kyo-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/kyo-study/categories/paper_study"><strong>Review Of Papers</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/opt_study"><strong>Introduction to Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">Machine Learning</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 3</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/0">0. Linear Models for Regression</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/1">1. Linear Basis Function Models</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/2">2. Bias-Variance Decomposition</a>
      </li>
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/3">3. Bayesian Linear Regression</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-study/docs/ml_study/chapter03/4">4. Bayesian Model Comparison</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/5">5. The Evidence Approximation</a>
      </li>
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter03/6">6. Limit. of Fixed Basis Functions</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-study/blob/gh-pages/docs/ml_study/chapter03/4.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>4. Bayesian Model Comparison</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>1장에서 오버-피팅(over-fitting)을 막을 수 있는 방안으로 Cross-Validation 기법이나 정칙화 파라미터를 사용하는 방법들을 설명했다.</li>
  <li>이번 절에서는 베이지안 관점에서의 모델 선택 문제를 다루어보고자 한다.
    <ul>
      <li>뜬금없이 왜 모델 선택 문제인가?</li>
      <li>1장에서 모델을 \( M \) 파라미터로 구별했고 \( M \) 값이 증가할수록 오버피팅 현상이 발생하는 것을 확인했다.</li>
      <li>이를 막기 위한 방법으로 \( M \) 값을 조절하는 방법이 아닌 Cross-Validation이나 정칙화를 다루었다.</li>
      <li>물론 모델을 변경하면 오버 피팅이 줄어든다는 것도 잠깐 보긴 봤다.</li>
      <li>이번 절에서는 모델 자체를 변경하는 것을 다루게 될 것이다.</li>
      <li>하지만 좀 더 구체적인 내용은 다음 절에서 다루도록 하고 여기서는 우선 일반적인 내용만 다루도록 한다.</li>
    </ul>
  </li>
  <li>베이지안 관점에서는 모델 선택의 문제 또한 확률적인 문제로 다루게 된다.
    <ul>
      <li>선택 가능한 모델의 수를 \( L \) 이라고 정의하면 모델 \( {M_i} \) 는 \( i=1,…,L \) 이 된다.</li>
      <li>즉, 이를 랜덤 변수로 놓는다.</li>
    </ul>
  </li>
  <li>다항 커브 피팅 문제에서는 타겟 \( t \) 를 랜덤 변수로 놓고 입력 데이터 \( {\bf X} \) 을 주어진 (알려진) 데이터라고 가정했다.</li>
  <li>이와는 조금 다른 모델로 입력 데이터도 하나의 랜덤 변수로 놓고 두 변수 \( {\bf X} \) 와 \( {\bf t} \) 에 대한 결합 분포를 구하는 모델도 있다.</li>
  <li>이제 데이터 \( D \) 가 주어졌을 때 사용할 하나의 모델을 표현해보자.
    <ul>
      <li>이 모델이 나올수 있는 확률을 확률 함수로 표현 가능하다. 즉, \( p(M_i) \)</li>
    </ul>
  </li>
  <li>자, 이제 데이터 집합 \( D \) 가 주어졌을 때 사후 확률을 다음과 같이 정의 가능하다.</li>
</ul>

<script type="math/tex; mode=display">p(M_i|D) \propto p(M_i)p(D|M_i) \qquad{(3.66)}</script>

<ul>
  <li>여기서 사전( <em>prior</em> ) 분포는 특정 모델의 초기 선호도가 된다.
    <ul>
      <li>물론 이 값이 모두 같다고 가정해도 크게 문제가 될게 없다.</li>
      <li>따라서 우리가 집중해서 볼 요소는 \( p(D|M_i) \) 이다.</li>
      <li>이 영역을 <em>model evidence</em> 라고 한다.</li>
    </ul>
  </li>
  <li><em>model evidence</em> 를 주변 가능도 ( <em>marginal likelihood</em> ) 함수라고도 부른다.
    <ul>
      <li>왜냐하면 주어진 모델 공간에서의 가능도 함수를 의미하기 때문</li>
    </ul>
  </li>
  <li><strong>Bayes Factor</strong>
    <ul>
      <li>한 모델의 <em>model evidence</em> 에 대한 다른 모델의 <em>model evidence</em> 비율을 <em>Bayes Factor</em> 라고 한다.</li>
      <li>\( p(D|M_i) / p(D|M_j) \) 로 정의된다.</li>
      <li>이는 모델 \( M_i \) 가 모델 \( M_j \) 보다 선호될 비율이라고 생각하면 된다.</li>
    </ul>
  </li>
  <li>모델을 통해 사후 분포를 알아내기만 하면 예측 분포를 만들어 낼 수도 있다.</li>
</ul>

<script type="math/tex; mode=display">p(t|{\bf x}, D) = \sum_{i=1}^{L} p(t|{\bf x}, M_i, D)p(M_i|D) \qquad{(3.67)}</script>

<ul>
  <li>사실 위와 같은 방식은 여러 모델을 이용하여 <strong>Mixture</strong> 분포를 만들어내는 형태라고 생각할 수 있다.
    <ul>
      <li>각각의 모델의 확률 \( p(t|{\bf x}, M_i, D) \) 의 평균 확률이 된다.</li>
      <li>즉 위의 식은 mixture 모델을 일반화한 식이라고 생각하면 된다.</li>
    </ul>
  </li>
  <li>만약 사후 확률 값이 동일한 2개의 모델이 주어졌다고 하자.
    <ul>
      <li>자신들의 파라미터의 예측값을 각각 \( \alpha \) 와 \( \beta \) 라 예측했다면,</li>
      <li>위의 결과에서는 \( t=(\alpha+\beta)/2 \) 와 같은 형태로 모수의 값이 얻어지는 것이 아니라,</li>
      <li>두 개의 봉우리가 생기는 형태(다봉)의 확률 분포를 얻게 될 것이다. (mixture model)</li>
    </ul>
  </li>
  <li>평균적인 모델을 사용하는 가장 손 쉬운 방법은 가장 적합한 모델 하나만을 선택하여 사용하는 것이다.
    <ul>
      <li>그리고 이렇게 여러 모델 중 하나의 모델을 선택하는 작업을 <em>model selection</em> 이라고 한다.</li>
      <li>어떤 모델이 더 좋은지는 주어진 데이터가 발현된 확률이 더 큰 모델이 무엇인지 확인하여 이를 선택하면 된다.</li>
    </ul>
  </li>
  <li>어떤 모델의 파라미터 집합 \( {\bf w} \) 가 주어졌을 때, <em>model evidence</em> 를 살펴보자.
    <ul>
      <li>이를 합의 법칙과 곱의 법칙을 이용하여 표현 가능하다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(D|M_i) = \int p(D|{\bf w}, M_i)p({\bf w}|M_i)d{\bf w} \qquad{(3.68)}</script>

<ul>
  <li>샘플링의 관점에서 본다면 사전 분포로 랜덤하게 샘플링된 파라미터를 가지는 모델로부터 데이터가 생성되었다고 생각할 수 있다.</li>
  <li>게다가 이 수식을 분모로 하는 (즉, 정규화 요소로 사용하는) 파라미터의 사후 확률을 기술할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf w}|D, M_i) = \frac{p(D|{\bf w}, M_i) p({\bf w}|M_i)}{p(D|M_i)}</script>

<ul>
  <li>위에서 사용한 <em>model evidence</em> 의 적분 식을 이용하여 우리는 파라미터와 관련한 통찰을 얻을 수 있다.
    <ul>
      <li>논의를 쉽게 하기 위해 일단은 1차원의 파라미터 \( w \) 만을 고려하도록 하자.</li>
      <li>그리고 특정 모델을 가정하고 나서 이를 잠시 수식에서 생략해보자. ( \( M_i \) 를 생략 )
        <ul>
          <li>그러면 우리가 구해야 할 사후 확률 값은 \( p(D|w)p(w) \) 로 생각하면 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(D) = \int p(D|w)p(w)dw \simeq p(D|w_{MAP})\frac{\Delta w_{posterior}}{\Delta w_{prior}} \qquad{(3.70)}</script>

<ul>
  <li>이 식은 모수 \( w \) 가 하나의 값으로 거의 수렴해서 높은 피크를 이루는 경우를 가정한 것이다.
    <ul>
      <li>또한 \( w \) 의 사전 분포는 모두 동일하다고 가정하여 \( p(w) = \frac{1}{\Delta w_{prior}} \) 로 놓았다.</li>
    </ul>
  </li>
  <li>이 식에 \( \ln \) 함수를 취한다. 식은 다음과 같이 전개된다.</li>
</ul>

<script type="math/tex; mode=display">\ln p(D) \simeq \ln p(D|w_{MAP}) + \ln \left( \frac{\Delta w_{posterior}}{\Delta w_{prior}} \right) \qquad{(3.71)}</script>

<p><img src="/kyo-study/images/ml_study/Figure3.12.png" alt="figure3.12" class="center-block" height="200px" /></p>

<ul>
  <li>위 그림은 \( w \) 의 사후 분포가 한 점에서 높은 피크를 그릴 때의 모습을 대략적으로 나타낸 것이다.</li>
</ul>

<hr />

<ul>
  <li>이제 이 식을 분석해보도록 하자.
    <ul>
      <li>첫번째 텀(term)은 이 데이터를 가장 잘 표현하는 파라미터에 대한 확률값으로 결국 로그 가능도 함수가 된다.</li>
      <li>두번째 텀(term)은 모델의 복잡성에서 기인하는 페널티 텀(penalty term)이 된다.
        <ul>
          <li>이 텀은 음수인데 사후 확률이 피크를 이루기 때문에 결국 로그로 들어가는 값이 1보다 작기 때문이다.</li>
          <li>이런 이유로 사후 확률이 고정된 특정 값으로 점점 수렴해 갈수록 두번째 텀은 값의 비율이 더 커지게 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이제 사용되는 파라미터의 개수가 \( M \) 개라고 생각해보자.
    <ul>
      <li>그리고 비슷한 방식으로 식을 전개하면 쉽게 다음의 식을 얻을 수 있다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\ln p(D) \simeq \ln p(D|w_{MAP}) + M\ln \left( \frac{\Delta w_{posterior}}{\Delta w_{prior}} \right) \qquad{(3.72)}</script>

<ul>
  <li>이 식은 \( p(D) \) 를 추정하기 위한 가장 손쉬운 방식의 전개이다.
    <ul>
      <li>식의 복잡도가 모델 파라미터 수 \( M \) 에 대해 선형으로 증가한다.</li>
    </ul>
  </li>
  <li>이제 \( M \) 의 수가 변함에 따라 어떻게 식이 달라질지 생각해보자.
    <ul>
      <li>보통 복잡한 모델을 사용하는 경우 파라미터의 수가 증가하게 된다.</li>
      <li>하지만 이 경우 가능도 함수의 확률 값이 증가하게 된다. (모델 피팅이 대개 더 잘되므로)</li>
      <li>하지만 위의 식에 의해 패널티 텀도 \( M \) 에 의해 증가하게 되므로 \( p(D) \) 값을 감소시키는 요인으로 작용된다.</li>
      <li>따라서 \( M \) 값을 통해 모델 파라미터의 수를 결정하는게 문제의 핵심이 된다.</li>
    </ul>
  </li>
  <li>결국 모델의 선택은 <em>maximum evidence</em> 와 관련이 있다.
    <ul>
      <li>모델에 의해 파라미터의 복잡도가 결정되기 때문.</li>
    </ul>
  </li>
  <li>참고로 실제 이 식은 다음과 같이 기술하는 경우가 많다.</li>
</ul>

<script type="math/tex; mode=display">\ln p(D|M_i) = accuracy(M_i) - complexity(M_i)</script>

<ul>
  <li>실제로는 변형 식이 꽤나 많은데, 이미 1장에서 AIC 를 살펴보았다. ( \( \ln p(D|w_{MAP})-M \) )</li>
</ul>

<hr />

<ul>
  <li>
    <p>이제 베이지언 모델에서 모델 비교를 통해 가능도 함수의 주변 확률이 어떻게 선호되는 모델을 찾을 수 있는 것인지를 이해하도록 해보자.</p>
  </li>
  <li>
    <p>우선 3개의 모델이 주어졌다고 가정해보자. ( \( M_1, M_2, M_3 \) )</p>
    <ul>
      <li>가로 축은 가능한 데이터 집합 공간을 하나의 1차원으로 표현한 정보이다.
        <ul>
          <li>모든 샘플 데이터 공간을 하나의 차원 값으로 매핑한 것이다.</li>
          <li>따라서 가로 축의 한 점은 얻을 수 있는 하나의 데이터 집합을 의미하게 된다.</li>
        </ul>
      </li>
      <li>이 모델들로부터 샘플을 얻을 수 있는 확률을 한 번 상상해 보자.</li>
    </ul>
  </li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure3.13.png" alt="figure3.13" class="center-block" height="150px" /></p>

<ul>
  <li>그림을 보면 \( M_1 \) 이 가장 단순한 모델이고, \( M_3 \) 가 가장 복합한 모델임을 알 수 있다.
    <ul>
      <li>모든 확률은 정규화되어 있으며 특정한 샘플 집합 \( D_0 \) 는 그림에서 보듯 모델 \( M_2 \) 에서 발현된 확률이 가장 크다.</li>
    </ul>
  </li>
  <li>위와 같이 주어진 모델에서 샘플을 생성하는 방법은 간단하다.
    <ul>
      <li>우선 먼저 모델별로 주어진 \( p(w) \) 로부터 임의의 \( w \) 값을 하나 구하고 \( p(D|w) \) 로부터 샘플을 얻어낸다.</li>
    </ul>
  </li>
  <li>단순한 모델(예를 들어 1차원 다항식 모델)에서는 대부분 비슷한 값을 가지는 데이터만을 얻을 수 있다.
    <ul>
      <li>따라서 이 경우는 위의 그림에서 가로 축의 일부 지역만을 포함하는 확률 분포를 얻게 될 것이다.</li>
    </ul>
  </li>
  <li>반면 복잡한 모델(예를 들어 9차수의 다항식 모델)에서는 \( p(w) \) 로부터 생성된는 \( w \) 가 다양하기 때문에 \( p(D|w) \) 로부터 얻어지는 샘플의 범위도 훨씬 다양하게 된다.</li>
  <li>결국 \( p(D|M_i) \) 는 확률 함수이고 이로인해 정규화되어야 하므로 데이터 공간에 대한 확률 분포를 얻을 수 있다.</li>
  <li>그림 3.13 에서 데이터 \( D_0 \) 의 경우 따라서 가장 가능성이 높은 모델은 \( M_2 \) 가 됨을 알 수 있다.
    <ul>
      <li><em>evidence</em> 의 확률 값이 가장 높기 때문이다.</li>
    </ul>
  </li>
  <li>본질적으로는,
    <ul>
      <li>간단한 모델의 경우 임의로 주어진 데이터에 적합하게 될 확률이 매우 작고,</li>
      <li>복잡한 모델의 경우 임의의 데이터에 대해 일정 부분 대응할 수 있으나 실제 모델일 가능성은 매우 낮아진다.</li>
    </ul>
  </li>
  <li>베이지언 모델 비교 프레임워크에서는,
    <ul>
      <li>암묵적으로 고려되는 모델 중에 진짜 모델 분포가 존재하고 여기로부터 데이터가 생성되었다고 가정한다.</li>
      <li>이 가정이 맞다면 베이지먼 모델 비교 프레임워크는 항상 가장 올바른 모델을 선호하게 된다.</li>
      <li>이걸 확인하기 위해서 두 개의 모델 \( M_1 \) 과 \( M_2 \) 를 고려하되 실제 진짜 모델은 \( M_1 \) 이라고 가정해보자.</li>
      <li>이 때의 베이즈 팩터의 <strong>평균</strong> 값은 다음과 같은 형태를 취하게 된다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\int p(D|M_1)\ln \frac{p(D|M_1)}{p(D|M_2)}dD \qquad{(3.73)}</script>

<ul>
  <li>이 식은 \( KL\;divergence \) 만족하므로 항상 양수이다.
    <ul>
      <li>물론 두 모델이 같은 경우 0이지만 여기서는 모델이 다르다는 가정이다.</li>
      <li>따라서 베이즈 펙터는 항상 올바른 모델을 선호하게 된다.</li>
    </ul>
  </li>
  <li>지금까지 우리는 베이지언 방식이 오버피팅을 피하고 모델간 비교를 하는 과정을 살펴보았다.</li>
  <li>하지만 베이지언 방식을 포함해 모든 패턴 인식 접근에 있어 모델의 형태를 가정하고 시작한다.
    <ul>
      <li>결국 잘못된 모델의 선택은 잘못된 결과를 얻게 된다.</li>
    </ul>
  </li>
  <li>게다가 그림 3.12에서 보았듯이 모델 evidence 영역은 사전 분포의 영향을 많이 받는다.
    <ul>
      <li>게다가 부적절한 사전 분포(improper prior)를 선택하는 경우 어떻게 될지 알 수가 없다.
        <ul>
          <li>여기서 부적절한 사전 분포는 사용자가 선택한 분포 중 확률 조건을 만족하지 못하는 분포를 의미한다.</li>
          <li>베이지언 관련 자료를 찾아보면 나온다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>현실적으로는 학습데이터와는 독립적인 테스트 데이터 집합을 이용하여 모델의 성능을 평가하게 된다.</li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoon'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
