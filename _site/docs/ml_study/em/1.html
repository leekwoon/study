<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>1. Kullback-Leibler divergence & Motivation of EM algorithm</title>
  <link rel="stylesheet" href="/study/css/main.css">
  <link rel="stylesheet" href="/study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/study/docs/ml_study/em/1.html">
  <script src="/study/js/jquery-1.12.0.min.js"></script>
  <script type="text/javascript"src="http://cdn.mathjax.org/mathjax/latest/MathJax.js">
  MathJax.Hub.Config({extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>
  <link rel="shortcut icon" href="/study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/study/"><strong>study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/study/categories/temp"><strong>Temp</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/statistical_inference"><strong>Statistical Inference</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/unity"><strong>Unity</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/rl_study"><strong>RL</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/opt_study"><strong>Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/piano"><strong>Piano</strong></a>
            </li>
            
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>EM algorithm</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/study/docs/ml_study/em/1">1. Kullback-Leibler divergence & Motivation of EM algorithm</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/study/blob/gh-pages/docs/ml_study/em/1.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>1. Kullback-Leibler divergence & Motivation of EM algorithm</h1>
    </header>
    <hr class="title">
    <article>
      <blockquote>
  <p>EM algorithm 에 대해 알아보고 Gaussian mixture model 에 적용해보자.
그전에 우선 Kullback-Leibler divergence 과 latent variable을 가진 model의 학습의 어려움을 배워보자.</p>
</blockquote>

<h2 id="kullback-leibler-divergence">Kullback-Leibler divergence</h2>

<ul>
  <li>우선 엔트로피에 대해 간단히 짚어보자.</li>
  <li>information theory에서는 <script type="math/tex">x</script>의 구체적인 값을 관찰하는 경우 얼마만큼의 정보를 얻는지를 정량화한다.</li>
  <li>이때 정보의 양은 <script type="math/tex">h(x)</script>로 나타냄.</li>
</ul>

<script type="math/tex; mode=display">h(x) = -\log p(x)</script>

<ul>
  <li>그리고 정보의 평균은 다음과 같이 정의 된다.</li>
</ul>

<script type="math/tex; mode=display">H[x] = -\sum_x p(x)\log p(x)</script>

<ul>
  <li>위 식을 <strong>엔트로피(entoropy)</strong>라고 정의한다.</li>
  <li>참고로, <strong>Noiseless coding theorem (Shannon, 1948)</strong>에 의하면…
    <ul>
      <li>엔트로피는 랜덤 변수의 상태를 전송하는데 필요한 비트 수의 Lower Bound가 된다.</li>
    </ul>
  </li>
  <li>정확한 형태를 모르는 확률 분포 <script type="math/tex">p(X)</script> 가 있을 때 이 확률 분포를 최대한 근사한 <script type="math/tex">q(X)</script>가 있다고 하자.</li>
  <li><script type="math/tex">p(x)</script>가 아닌 <script type="math/tex">q(x)</script>를 사용했을때 추가적으로 필요한 정보량의 기대값을 정의해보자.</li>
</ul>

<script type="math/tex; mode=display">KL(p\vert\vert q) = -\int p({\bf x})\ln q({\bf x})d{\bf x} - (-\int p({\bf x})\ln p({\bf x})d{\bf x }) \\ 
=  -\int p({\bf x})\ln\{\frac{q({\bf x})}{p({\bf x})}\}d{\bf x}</script>

<ul>
  <li>이러한 정의를 <strong>relative entropy</strong> 또는 <strong>Kullback-Leibler divergence</strong>라고 부른다.</li>
  <li>참고로 <script type="math/tex">KL(p\vert\vert q) \ne KL(q\vert\vert p)</script> 이다. (non-symmetric)</li>
  <li>또한 항상 <script type="math/tex">KL(p\vert\vert q) \ge 0</script> 을 만족한다.
    <ul>
      <li><script type="math/tex">p({\bf x}) = q({\bf x})</script> 일때 <script type="math/tex">KL(p\vert\vert q) = 0</script> 만족.</li>
      <li><strong>옌센부등식 (Jensen’s inequality)</strong> 를 통해 증명 가능.</li>
      <li><script type="math/tex">-\ln(\cdot)</script> 이 convex 함수 이므로 <script type="math/tex">f(E[x]) \le E[f(x)]</script> 만족.</li>
      <li><script type="math/tex">f(x) = \ln(\frac{q({\bf x})}{p({\bf x})})</script> 로 놓고 Jensen’s inequality을 이용하면</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">KL(p\vert\vert q) = -\int p({\bf x})\ln\{\frac{q({\bf x})}{p({\bf x})}\}d{\bf x} \ge - \ln \int q({\bf x})d{\bf x} = 0</script>

<h2 id="gmm-gaussian-mixture-model">GMM (Gaussian Mixture Model)</h2>

<ul>
  <li>EM 알고리즘의 최종 목적은 잠재 변수(latent variable)를 통해 likelihood 을 최대화 하는 것이다.</li>
  <li>잠재변수 모델의 예로 GMM을 살펴보자.</li>
  <li>GMM 의 graphical model 은 아래와 같다.</li>
</ul>

<p><img src="/study/images/ml_study/em/em1.png" alt="" class="center-block" /></p>

<ul>
  <li>간단히 말하면 GMM은 여러 개의 가우시안 분포를 선형 결합한 형태이다.</li>
  <li><script type="math/tex">K</script> 개의 가우시한 분포를 결합한 GMM의 경우 잠재변수 <script type="math/tex">z</script> 는
    <ul>
      <li><em>1-to-K</em> 코딩 스킴 형태의 변수다.</li>
      <li><script type="math/tex">K</script> 개의 요소 중 1개의 값만 1이고 나머지는 0인 값이다.</li>
      <li>따라서 \( z_k \in {0,1} \) 이고 \( \sum_k z_k=1 \) 이다.</li>
    </ul>
  </li>
  <li>잠재변수와의 joint probability 를 고려해보자.
    <ul>
      <li>Graphical model에서 보듯이</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p({\bf x}, {\bf z}) = p({\bf z})p({\bf x}\|{\bf z})</script>

<ul>
  <li>이제 \( z_k \) 의 marginal probability 를 혼합계수(mixing coefficients) \( \pi_k \) 를 사용하여 정의할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p(z_k=1) = \pi_k</script>

<ul>
  <li>파라미터 \( \pi_k \) 는 다음과 같은 성질을 만족해야 한다.</li>
</ul>

<script type="math/tex; mode=display">0 \le \pi_k \le 1</script>

<script type="math/tex; mode=display">\sum_{k=1}^K \pi_k = 1</script>

<ul>
  <li>\( {\bf z} \) 가 <em>1-of-K</em> 코딩 스킴을 사용하기 때문에 이에 대한 확률 식을 다음과 같이 나타낼 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf z})=\prod_{k=1}^K \pi_k^{z_k}</script>

<ul>
  <li>이와 유사하게 이제 \( x \) 에 대한 condtional probability 을 표현해보자. \( x \) 는 \( z_k \) 가 주어졌을 때 가우시안 분포를 따른다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|z_k=1) = N({\bf x}|{\bf \mu_k}, \Sigma_k)</script>

<ul>
  <li>이를 일반화 하면</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf z}) = \prod_{k=1}^{K} N({\bf x}|{\bf \mu}_k, \Sigma_k)^{z_k} \qquad{(9.11)}</script>

<ul>
  <li>이제 \( {\bf x} \) 에 대한 marginal probability 을 계산할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}) = \sum_z p({\bf z})p({\bf x}|{\bf z}) = \sum_{k=1}^{K} \pi_k N({\bf x}|{\bf \mu}_k, \Sigma_k)</script>

<ul>
  <li>이제 \( {\bf x} \) 가 주어졌을 때의 conditional probability 을 정의해보자.</li>
</ul>

<script type="math/tex; mode=display">\gamma(z_k) \equiv p(z_k=1|{\bf x}) = \frac{p(z_k=1)p({\bf x}|z_k=1)}{\sum_j^K p(z_j=1)p({\bf x}|z_j=1)}=\frac{\pi_k N({\bf x}|{\bf \mu}_k, \Sigma_k)}{\sum_{j=1}^K \pi_j N({\bf x}|{\bf \mu}_j, \sigma_j)}</script>

<ul>
  <li>우리는 \( \pi_k \) 가 \( z_k=1 \) 일 때의 prior 이라는 것을 알고 있다.</li>
  <li>이제 \( \gamma(z_k) \) 는 관찰 데이터 \( {\bf x} \) 가 주어졌을 때의 posterior probability 이 된다.</li>
  <li>
    <p>이후에 살펴볼 것이지만 \( \gamma(z_k) \) 를 성분 \( k \) 에 대한 <em>responsibility</em> 라고도 한다.</p>
  </li>
  <li>이제 로그 가능도 함수(likelihood function)를 기술해보자.</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}|{\bf \pi}, {\bf \mu}, \Sigma) = \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)\right\}</script>

<h2 id="em-algorithm">EM algorithm</h2>

<ul>
  <li>SGD (stochastic gradient descent)같은 방법이 있는데 굳이 EM algorithm 을 왜 알아야 할까?</li>
  <li>그 이유를 알기 위해서 잠재변수를 가진 모델 학습의 어려움을 살펴보자.</li>
</ul>

<p><img src="/study/images/ml_study/em/em2.png" alt="" class="center-block" /></p>

<ul>
  <li>이전에 언급했듯이 GMM은 위의 Graphical model로 표현될 수 있다.
    <ul>
      <li>이때 <script type="math/tex">\theta_z</script> 는 <script type="math/tex">\pi</script> 를 의미하고, <script type="math/tex">\theta_x</script> 는 <script type="math/tex">\mu</script> 와 <script type="math/tex">\sum</script> 을 의미한다.</li>
    </ul>
  </li>
  <li><script type="math/tex">x_i</script> 와 <script type="math/tex">z_i</script> 가 관찰됬다면, D-separation에 의해서 <script type="math/tex">\theta_z</script> 와 <script type="math/tex">\theta_x</script> 는 independent 하고, parameter의 posterior도 factorize 될 수 있다.</li>
  <li>하지만 <script type="math/tex">z_i</script>는 잠재변수로써 관찰이 되지 않으므로 parameter들의 posterior은 factorize 되지 않는다.
    <ul>
      <li>참고로 잠재변수의 실제 값이 주어졌을 때 complete하다고 하고 주어지지 않았을 때 incomplete하다고 한다.</li>
      <li>Complete Data : \( { {\bf X}, {\bf Z} } \)
        <ul>
          <li>모든 관찰 데이터에 대해 잠재 변수 \( {\bf Z} \) 가 주어진 모델</li>
          <li>이 때의 로그 가능도 함수는 \( \ln p({\bf X}, {\bf Z} | {\bf \theta}) \) 가 된다.</li>
          <li>각각에 대해 가능도 함수를 구하게 되므로 어렵지 않다.</li>
        </ul>
      </li>
      <li>Incomplete Data : \( { {\bf X}} \)
        <ul>
          <li>모든 관찰 데이터에 대해서만 주어지고 잠재 변수는 주어지지 않는다.</li>
          <li>결국 가능한 모든 잠재 변수의 값에 대한 분포를 고려해야 한다.</li>
          <li>로그 가능도 함수는 \( \ln p({\bf X} | {\bf \theta}) =  \ln { \sum_Z p( {\bf X}, {\bf Z} | {\bf \theta} ) } \) 로 주어진다.</li>
          <li>로그 함수 내에 합(sum) 에 대한 수식이 존재하기 때문에 최대화 시키는 지점을 찾기가 어렵다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이는 MAP, MLE의 계산을 어렵게 만든다.</li>
  <li>또한 <script type="math/tex">\sum_{k=1}^K \pi_k = 1</script> 같은 제약조건은 SGD (stochastic gradient descent) 적용을 어렵게 한다.</li>
  <li>잠재변수 모델에서 parameter들의 posterior을 계산할때 가장 큰 문제는 다중 봉우리(multiple mode)를 가졌다는 것이다.
    <ul>
      <li>예를 들어 GMM에서, <script type="math/tex">z_i</script> 가 관찰되었다면 unimodal posterior을 갖게되어 쉽게 parameter을 추정할 수 있다.</li>
      <li>하지만 잠재변수가 관찰 되지 않았을 경우 다양한 봉우리를 가지기 때문에 어떤 잠재변수 기준으로 optimize하는지가 중요하다.</li>
      <li>쉽게 생각하면, 봉우리(<script type="math/tex">z</script>)를 정하고 남은 parameter들을 optimize 해야 할 것이다.</li>
    </ul>
  </li>
  <li>GMM 같이 joint probability 분포가 지수족(exponential family)일때, 잠재변수를 가지는 모델의 likelihood는 다중 봉우리(multiple model)를 가질 수 있음을 대수적으로 보여보자.</li>
  <li>우선 joint probability가 지수족(exponential family)이라면 다음과 같이 나타낼 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p(x,z\vert \theta)=\frac{1}{Z(\theta)}\exp[\theta^T\phi({\bf x},{\bf z})]</script>

<ul>
  <li>모든 data가 관찰되는 complete data log likelihood는 다음과 같이 나타낼 수 있다.</li>
</ul>

<script type="math/tex; mode=display">L(\theta)=\sum_i \ln p({\bf x}_i,{\bf z}_i\vert \theta)=\theta^T(\sum_i \phi({\bf x_i},{\bf z_i}))-N\ln Z(\theta)</script>

<ul>
  <li>첫번째 term은 <script type="math/tex">\theta</script> 에 linear 하고 뒤의 log partition function <script type="math/tex">\ln Z(\theta)</script> 는 convex하다. (왜 그런지는 따로 찾아보기)</li>
  <li>뒤의 term에 마이너스가 붙어 있어서 전체적으로 concave하고 따라서 하나의 maximum이 존재한다.</li>
  <li>하지만 잠재 변수를 포함하는 모델의 경우 잠재변수는 관찰되지 못하므로 likelihood는 아래와 같이 표현된다. (incomplete data log likelihood)</li>
</ul>

<script type="math/tex; mode=display">L(\theta)=\sum_i \ln p({\bf x}_i\vert \theta)=\sum_i \ln (\sum_{ {\bf z}_i}p({\bf x}_i,{\bf z}_i\vert \theta))</script>

<ul>
  <li><script type="math/tex">p({\bf x},{\bf z})</script> 는 exponential family라고 가정하였으므로,</li>
</ul>

<script type="math/tex; mode=display">L(\theta)=\sum_i \ln [\sum_{ {\bf z}_i}p({\bf x}_i,{\bf z}_i\vert \theta)]=\sum_i \ln (\sum_{ {\bf z}_i}\exp[\theta^T\phi({\bf x},{\bf z})])-N\ln Z(\theta)</script>

<ul>
  <li>첫번째 term의 log-sum-exp 함수는 convex하고 뒤의 log partition function <script type="math/tex">\ln Z(\theta)</script> 는 convex하다.</li>
  <li>하지만 두 convex 함수의 차는 일반적으로 convex하지 않다.</li>
  <li>따라서 여러개의 optimal solution을 가진다. (다중 봉우리를 가진다.)</li>
</ul>


    </article>
  </div>
</div>

    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
