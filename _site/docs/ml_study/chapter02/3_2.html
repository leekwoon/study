<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>3. The Gaussian Distribution [II]</title>
  <link rel="stylesheet" href="/kyo-study/css/main.css">
  <link rel="stylesheet" href="/kyo-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-study/docs/ml_study/chapter02/3_2.html">
  <script src="/kyo-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-study/"><strong>kyo-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/kyo-study/categories/paper_study"><strong>Review Of Papers</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/opt_study"><strong>Introduction to Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">Machine Learning</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 2</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter02/0">0. Probability Distributions</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter02/1">1. Binary Variables</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter02/2">2. Multinomial Variables</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter02/3_1">3. The Gaussian Distribution [I]</a>
      </li>
      
      
      
      <li class="active " >
        <a href="/kyo-study/docs/ml_study/chapter02/3_2">3. The Gaussian Distribution [II]</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter02/3_3">3. The Gaussian Distribution [III]</a>
      </li>
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter02/4">4. The Exponential Family</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter02/5">5. Nonparametric Methods</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-study/blob/gh-pages/docs/ml_study/chapter02/3_2.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>3. The Gaussian Distribution [II]</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>2.3 절의 가우시안 분포 내용이 너무 길어서 몇 개의 파트로 나누어 설명한다.</li>
  <li>앞절의 내용은 Part.I 을 참고하기 바란다.</li>
</ul>

<h2 id="mle-maximum-likelihood-for-the-gaussian">2.3.4. 가우시안 MLE (Maximum likelihood for the Gaussian)</h2>
<ul>
  <li>관찰 데이터 집합 \( {\bf X}=({\bf x}_1,…,{\bf x}_n)^T \) 가 주어졌을 때 데이터 \( { {\bf x}_n} \) 은 서로 독립적으로 발현된다. (<em>i.i.d</em>)</li>
  <li>각각의 관찰 데이터는 가우시안 분포를 따르게 되며 이를 가능도 함수로 이용할 때에는 보통 로그를 취해 사용하게 된다.</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}|{\pmb \mu}, \Sigma) = -\frac{ND}{2}\ln(2\pi) - \frac{N}{2}\ln|\Sigma|-\frac{1}{2}\sum_{n=1}^{N}({\bf x}_n-{\pmb \mu})^T\Sigma^{-1}({\bf x}_n-{\pmb \mu}) \qquad{(2.118)}</script>

<ul>
  <li>이 식은 사실 최종적으로는 다음 두가지 값에만 영향을 받게 된다.</li>
</ul>

<script type="math/tex; mode=display">\sum_{n=1}^{N}{\bf x}_n \qquad{(2.119)}</script>

<script type="math/tex; mode=display">\sum_{n=1}^{N}{\bf x}_n{\bf x}_n^T \qquad{(2.119)}</script>

<ul>
  <li>이를 충분통계량(<em>sufficient statistics</em>)라고 부른다.</li>
  <li>가우시안 분포를 따르기 때문에 미분을 통해 최대값을 구할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">\dfrac{\partial}{\partial {\pmb \mu}}\ln p({\bf X}|{\pmb \mu}, \Sigma) = \sum_{n=1}^{N}\Sigma^{-1}({\bf x}_n-{\pmb \mu}) \qquad{(2.120)}</script>

<ul>
  <li>이 값이 0이 되는 지점에서 최대값을 가지게 된다.</li>
  <li>따라서 이 때의 평균 값은 다음과 같게 된다.</li>
</ul>

<script type="math/tex; mode=display">{\pmb \mu}_{ML} = \frac{1}{N}\sum_{n=1}^{N}{\bf x}_n \qquad{(2.121)}</script>

<ul>
  <li>공분산도 별거 없다. 계산의 편리성을 위해 \( \Lambda=\Sigma^{-1} \) 로 놓고 풀면 된다.</li>
</ul>

<script type="math/tex; mode=display">\Sigma_{ML} = \frac{1}{N}\sum_{n=1}{N}({\bf x}-{\pmb \mu}_{ML})({\bf x}-{\pmb \mu}_{ML})^T \qquad{(2.122)}</script>

<ul>
  <li>
    <p>이 식에서는 \( {\pmb \mu}_{ML} \) 이 사용된다. 먼저 \( {\pmb \mu}_{ML} \) 을 구하고 \( \Sigma_{ML} \) 을 구하면 된다.</p>
  </li>
  <li>
    <p>이 때의 평균 값은 다음과 같다.</p>
  </li>
</ul>

<script type="math/tex; mode=display">E[{\pmb \mu}_{ML}] = {\pmb \mu} \qquad{(2.123)}</script>

<script type="math/tex; mode=display">E[\Sigma_{ML}] = \frac{N-1}{N}\Sigma \qquad{(2.124)}</script>

<ul>
  <li>이 내용은 1.2.4 절에서도 다룬 내용이다.
    <ul>
      <li>MLE의 기대값에 대한 평균은 그냥 평균이 된다. (unbias)</li>
      <li>하지만 분산값의 평균은 실제 분산 값보다 작다. (bias)</li>
      <li>따라서 보통 이런 요소를 수정해서 분산값으로 다음 값을 사용한다. ( \( \tilde{\Sigma} \) 로 표기)</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\tilde{\Sigma} = \frac{1}{N-1}\sum_{n=1}^{N}({\bf x}_n - {\pmb \mu}_{ML})({\bf x}_n - {\pmb \mu}_{ML})^T \qquad{(2.125)}</script>

<ul>
  <li>사실 이 내용은 자유도(degree of freedom)와 관련이 깊은 내용이다.</li>
  <li>통계학 서적을 참고해도 되나 사실 이후 과정을 전개함에 있어 위의 식만 알아도 큰 무리가 없기 때문에, 굳이 찾아서 볼 필요까지는 없다.</li>
</ul>

<hr />

<p><strong>(참고)</strong></p>

<ul>
  <li>교재에는 없지만 가우시안 분포에 대한 MLE 유도를 간단히 정리해 놓는다.</li>
</ul>

<p><em>Log Likelihood</em></p>

<script type="math/tex; mode=display">\ln p({\bf X}|{\pmb \mu}, \Sigma) = -\frac{ND}{2}\ln(2\pi) - \frac{N}{2}\ln|\Sigma|-\frac{1}{2}\sum_{n=1}^{N}({\bf x}_n-{\pmb \mu})^T\Sigma^{-1}({\bf x}_n-{\pmb \mu})</script>

<p><em>Basic Equation</em></p>

<script type="math/tex; mode=display">\frac{\partial (b^Ta)}{\partial a} = b</script>

<script type="math/tex; mode=display">\frac{\partial (a^TAa)}{\partial a} = (A+A^T)a</script>

<script type="math/tex; mode=display">\frac{\partial}{\partial A} tr(BA)=B^T</script>

<script type="math/tex; mode=display">\frac{\partial}{\partial A} \log|A|=(A^{-1})^T</script>

<script type="math/tex; mode=display">tr(ABC)=tr(CAB)=tr(BCA)</script>

<p><em>Mean</em></p>

<ul>
  <li>\( {\bf y}=({\bf x}-{\pmb \mu}) \) 라 하면 다음의 식이 유도된다.</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial}{\partial {\pmb \mu}}({\bf x}-{\pmb \mu})^T\Sigma^{-1}({\bf x}-{\pmb \mu}) = \frac{\partial}{\partial {\bf y}}{\bf y}^T\Sigma^{-1}{\bf y} \frac{\partial {\bf y}}{\partial {\pmb \mu}} = -(\Sigma^{-1}+(\Sigma^{-1})^{T}){\bf y} \equiv -(\Lambda-\Lambda^T){\bf y}</script>

<ul>
  <li>따라서</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial}{\partial {\pmb \mu}}l({\pmb \mu}, \Sigma) = -\frac{1}{2}\sum_{i=1}^{N}-2\Lambda({\bf x}_i-{\pmb \mu})=\Lambda\sum_{i=1}^{N}({\bf x}_i-{\pmb \mu}) = 0</script>

<ul>
  <li>참고로 공분산은 대칭행렬이라서 위와 같이 전개된다. ( \( \Sigma^{-1}=(\Sigma^{-1})^T \))</li>
  <li>이제 <em>MLE</em> 평균은 다음과 같아진다.</li>
</ul>

<script type="math/tex; mode=display">{\pmb \mu}_{ML} = \frac{1}{N}\sum_{i=1}^{N}{\bf x}_i = \bar{\bf x}</script>

<p><em>Covariance</em></p>

<ul>
  <li>참고로 \( \Lambda \equiv \Sigma^{-1} \) 이고 \( {\vert}\Lambda{\vert} = {\vert}\Sigma{\vert} \) 이다.</li>
</ul>

<script type="math/tex; mode=display">l(\Lambda) = \frac{N}{2}\ln|\Lambda| - \frac{1}{2}\sum_i tr[({\bf x}_i-{\pmb \mu})({\bf x}_i-{\pmb \mu})^T\Lambda] =  \frac{N}{2}\ln|\Lambda| - \frac{1}{2} tr[S_{\mu}\Lambda]</script>

<script type="math/tex; mode=display">S_{\mu} = \sum_{i=1}^{N}({\bf x}_i-{\pmb \mu})({\bf x}_i-{\pmb \mu})^T</script>

<script type="math/tex; mode=display">\frac{\partial l(\Lambda)}{\partial \Lambda} = \frac{N}{2}(\Lambda^{-1})^{T}-\frac{1}{2}S_{\mu}^T=0</script>

<script type="math/tex; mode=display">(\Lambda^{-1})^T = \Lambda^{-1}=\Sigma = \frac{1}{N}S_{\mu}</script>

<script type="math/tex; mode=display">\Sigma_{ML} = \frac{1}{N}\sum_{i=1}^{N}({\bf x}_i-\mu)({\bf x}_i-\mu)^T</script>

<h2 id="sequential-estimation">2.3.5. 순차 추정 (Sequential estimation)</h2>
<ul>
  <li>순차 추정의 방법
    <ul>
      <li>한번에 한 샘플을 연산하고 버림</li>
      <li>관찰 데이터 집합이 매우 커서 한번에 계산이 불가능할 때 사용하기 좋다. (on-memory 불가 상황)</li>
    </ul>
  </li>
  <li><em>MLE</em> 로 얻어진 \( {\pmb \mu}_{ML} \) 식을 업데이트 방식으로 바꾸어보자.
    <ul>
      <li>\( {\pmb \mu}_{ML} \) 에서 마지막 샘플을 추출해보자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\pmb \mu}_{ML}^{(N)} = \frac{1}{N} \sum_{n=1}^{N}{\bf x}_n = \frac{1}{N}{\bf x}_N + \frac{1}{N}\sum_{n=1}^{N-1}{\bf x}_n\\
= \frac{1}{N}{\bf x}_N + \frac{N-1}{N}{\pmb \mu}_{ML}^{(N-1)}={\pmb \mu}_{ML}^{(N-1)}+\frac{1}{N}({\bf x}_N-{\pmb \mu}_{ML}^{(N-1)}) \qquad{(2.126)}</script>

<ul>
  <li>\( N-1 \) 개의 데이터로부터 추정된 \( {\pmb \mu}_{ML}^{(N-1)} \) 와 \( N \) 번째 관측된 데이터를 이용하여 \( {\pmb \mu}_{ML}^{(N)} \) 을 구한다.</li>
  <li>\( N \) 의 값이 증가할수록 새로 관측되는 데이터의 기여도가 점점 작아지게 된다.</li>
  <li>한번에 계산을 처리하는 배치 방식으로부터 식을 유도해 내었기 때문에 실제 결과는 동일하게 된다.</li>
  <li>이런 방식은 매우 유용하지만 배치 방식의 식에서 업데이트 방식의 식을 항상 유도할 수 있는 것은 아니다.</li>
  <li>따라서 좀 더 일반화된 방식의 순차 처리 방식에 대해 알아볼 것이다.</li>
</ul>

<hr />

<p><strong>Robbins &amp; Monro</strong> 알고리즘</p>

<ul>
  <li>랜덤 변수 \( \theta \) 와 \( z \) 가 주어졌다고 하자.</li>
  <li>이 변수들에 대한 결합 분포는 \( p(z, \theta) \) 이다.</li>
  <li>\( \theta \) 가 주어졌을 때 \( z \) 에 대한 평균의 함수를 정의해보자.</li>
</ul>

<script type="math/tex; mode=display">f(\theta)\equiv E[z|\theta] = \int zp(z|\theta)dz \qquad{(2.127)}</script>

<ul>
  <li>사실 이러한 형태의 함수를 회귀(regression) 함수라고 한다.
    <ul>
      <li>회귀와 관련된 내용은 3장에 더 자세히 나오게 된다.</li>
    </ul>
  </li>
  <li>우리의 목표는 \( f(\theta^{*}) = 0 \) 을 만족하는 \( \theta^{*} \) 를 찾는 것이다.
    <ul>
      <li>이 때 \( \theta^* \) 를 <strong>root</strong> 라고 하자.</li>
    </ul>
  </li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure2.10.png" alt="figure2.10" class="center-block" height="200px" /></p>

<ul>
  <li>여기서 관찰 데이터는 \( z \) 이다. (파란색 점)</li>
  <li>\( z \) 와 \( \theta \) 와 관련된 데이터가 충분히 주어진다면 직접적으로 우리가 원하는 값을 구할 수 있을 것이다.</li>
  <li>하지만 관찰 데이터가 너무 커서 전체로 주어지지 않고 하나씩 업데이트 된다고 가정해보자.</li>
  <li>이제 몇 가지 가정을 한다.
    <ul>
      <li>\( z \) 에 대한 조건부 분산(conditional variance)은 유한한 값을 가진다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
E[(z-f)^2|\theta]<\infty \qquad{(2.128)} %]]></script>

<ul>
  <li>또 다른 가정은 다음과 같다.
    <ul>
      <li>임의의 \( \theta \) 에 대해 \( \theta &gt; \theta^* \) 이면 \( f(\theta) &gt; 0 \)</li>
      <li>임의의 \( \theta \) 에 대해 \( \theta &lt; \theta^* \) 이면 \( f(\theta) &lt; 0 \)</li>
      <li>정확히 그림과 같은 \( f \) 함수를 상상하면 된다.</li>
    </ul>
  </li>
  <li>이 상황에서 \( \theta^* \) 를 추정하는 방법을 생각해보자.</li>
</ul>

<script type="math/tex; mode=display">\theta^{(N)} = \theta^{(N-1)} - a_{N-1} z(\theta^{N-1}) \qquad{(2.129)}</script>

<ul>
  <li>이 식을 이용하여 순차적으로 들어오는 데이터를 넣어 root 값을 추정할 수 있다.
    <ul>
      <li>이 식이 바로 Robbins &amp; Monro 의 식이다.</li>
    </ul>
  </li>
  <li>여기서 \( z(\theta(N)) \) 은 \( N \) 번째의 \( \theta \) 값이 들어왔을 때의 출력값을 의미한다.</li>
  <li>계수 \( {a_N} \) 은 연속적인 양의 실수이며 다음과 같은 조건을 만족한다.</li>
</ul>

<script type="math/tex; mode=display">\lim_{N\rightarrow\infty}a_N=0 \qquad{(2.130)}</script>

<script type="math/tex; mode=display">\sum_{N=1}^{\infty}a_N=\infty \qquad{(2.131)}</script>

<script type="math/tex; mode=display">% <![CDATA[
\sum_{N-1}^{\infty}a_N^2<\infty \qquad{(2.132)} %]]></script>

<ul>
  <li>위의 식이 의미하는 바는 다음과 같다.
    <ul>
      <li>위의 세 식은 추정이 반복될수록 (즉, \( N \) 이 커질수록) 다음의 특성을 갖는다.
        <ul>
          <li>\( \lim_{N\rightarrow\infty}a_N=0 \) : \( \theta \) 가 특정 값에 수렴</li>
          <li>\( \sum_{N=1}^{\infty}a_N=\infty \) : root 를 찾기도 전에 임의 값에 수렴하지 않도록</li>
          <li>\( \sum_{N-1}^{\infty}a_N^2&lt;\infty \) : 축적되는 노이즈가 유한하다는 가정에 의해 수렴된 상태를 깨지 않는다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>갑자기 위와 같은 설명이 좀 당황스러울 수도 있지만,
    <ul>
      <li>Robbins &amp; Monro 가 위의 식이 성립한다는 것을 증명했고,</li>
      <li>위와 같은 식 전개를 가우시안 모델에도 그대로 적용하여,</li>
      <li>파라미터의 온라인 업데이트가 가능한 모델로 변환할수 있다고 생각하면 된다.</li>
    </ul>
  </li>
</ul>

<p><strong>MLE 적용</strong></p>

<ul>
  <li>가우시안 모델의 MLE를 구하는 과정에서 위의 식을 사용해보자.</li>
  <li>가우시안 모델에서 \( \theta_{ML} \) 의 값은 음의 로그 가능도 함수를 한 번 미분하여 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial}{\partial\theta}\left.\left\{-\frac{1}{N}\sum_{n=1}^{N}\ln p(x_n|\theta)\right\}\right|_{\theta_{MLE}}=0 \qquad{(2.133)}</script>

<ul>
  <li>\( N\rightarrow\infty \) 로 하여 미분 식과 합의 식을 교환한 뒤 일반화하여 보자.</li>
</ul>

<script type="math/tex; mode=display">-\lim_{n\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}\frac{\partial}{\partial\theta}\ln p(x_n|\theta) = E_x\left[-\frac{\partial}{\partial\theta}\ln p(x|\theta)\right] \qquad{(2.134)}</script>

<ul>
  <li>이제 Robbins &amp; Monro 의 식과 동일해졌다는 것을 알 수 있다.
    <ul>
      <li>가능도 함수의 해를 구하는 것은 회귀 함수의 해를 구하는 문제와 동일하다.</li>
      <li>이제 Robbins &amp; Monro 식을 적용해보자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\theta^{(N)} = \theta^{(N-1)} - a_{N-1}\frac{\partial}{\partial\theta^{(N-1)}}\left[-\ln p(x_N|\theta^{(N-1)})\right] \qquad{(2.135)}</script>

<ul>
  <li>겨우 일반화된 식을 만들어내기는 했다.</li>
  <li>가우시안 분포에서 사용되는 모수는 평균과 분산인데, 평균은 배치 계산 방식에서 순차 계산 방식으로의 변환식을 유도하는 것을 이미 앞서 살펴보았다.</li>
  <li>하지만 Robbins &amp; Monro 식을 써도 되는걸 이미 알고 있으므로 이제 이 방식으로 변환해 보도록 하자.
    <ul>
      <li>그리고 당연히 그 둘의 결과가 동일해야 할 것이다.</li>
    </ul>
  </li>
  <li>우선 \( \ln p(x_n|\theta) \) 는 \( \ln p(x_n|\theta)=\frac{1}{2\sigma^2}(x_n-\theta)^2 \) 임을 알고 있다.</li>
  <li>이제 앞서 구한 식에 대입을 하면 다음과 같은 결과를 얻게 된다.</li>
</ul>

<script type="math/tex; mode=display">z=\frac{\partial}{\partial\mu_{ML}}[-\ln p(x|\mu_{ML}, \sigma^2)]=-\frac{1}{\sigma^2}(x-\mu_{ML}) \qquad{(2.136)}</script>

<ul>
  <li>우리에게 필요한 식은 \( E[z|\theta] \) 이다.</li>
  <li>따라서 모든 \( x \) 에 대해 평균 공식을 대입하면 \( -\frac{1}{\sigma^2}(\mu-\mu_{ML}) \) 을 얻는다.
    <ul>
      <li>이는 \( \frac{1}{N}\sum x_i=\mu \) 이기 때문이다.</li>
    </ul>
  </li>
  <li>사실 \( E[z|\theta] \) 는 회귀 식이므로 \( z \) 에 대해 정규분포로 표현이 가능하다.
    <ul>
      <li>이 때 이 정규 분포의 평균 값은 실제 회귀 함수가 되며, 따라서 \( -\frac{1}{\sigma^2}(\mu-\mu_{ML}) \)</li>
    </ul>
  </li>
  <li>정규 분포의 평균 값에 대한 연결선은 직선의 함수가 된다. (붉은색)</li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure2.11.png" alt="figure2.11" class="center-block" height="200px" /></p>

<ul>
  <li>이 때 이 값이 0 을 만족하는 값이 우리가 찾고자 하는 \( \mu_{ML} \) 의 해가 된다.</li>
  <li>여기서 \( a_N=\sigma^2/N \) 으로 놓고 식을 전개하면 배치 방식의 전개와 동일한 식을 얻을 수 있다.
    <ul>
      <li>자세한 식 전개는 생략하도록 한다.</li>
    </ul>
  </li>
</ul>

<h2 id="bayesian-inference-for-the-gaussian">2.3.6. 가우시안에서의 베이지안 추론 (Bayesian inference for the Gaussian)</h2>
<ul>
  <li>가우시안의 <em>MLE</em> 는 파라미터인 평균과 공분산에 대한 점추정(point estimation) 값이다.</li>
  <li>이제 사전 확률 분포를 추가하여 베이지안 접근 방식으로 이해해 보도록 하자.</li>
  <li>미리 좀 언급을 해보자면, 다양한 상황의 가우시안 분포가 주어졌을 때 베이지안 추론 방식을 살펴보는 것이다.
    <ul>
      <li>분산 값을 알고 있을 때 평균 값의 추론</li>
      <li>평균 값을 알고 있을 때 분산 값의 추론</li>
      <li>평균, 분산 둘 다 모를 때의 두 값에 대한 추론</li>
    </ul>
  </li>
  <li>우선 가장 간단한 형태의 식으로부터 학습을 진행해 보도록 한다.
    <ul>
      <li>따라서 1차원인 단변량 가우시안 분포부터 시작하도록 한다.</li>
    </ul>
  </li>
  <li>우선 가우시안 분포가 하나 주어져있고, 이 때의 분산 값은 이미 알고 있다고 생각한다.</li>
  <li>
    <p>데이터는 \( N \) 번의 관찰 데이터 \( {\bf x} = (x_1,…,x_N)^T \) 가 주어졌다.</p>
  </li>
  <li>가능도 함수를 구해보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|\mu) = \prod_{n=1}^{N}p(x_n|\mu) = \dfrac{1}{(2\pi\sigma^2)^{N/2}}\exp\left\{-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2\right\} \qquad{(2.137)}</script>

<ul>
  <li>당연히 이 함수는 \( \mu \) 에 대해서는 더 이상 확률 분포가 아니다.
    <ul>
      <li>가능도 함수가 반드시 확률 함수가 될 필요는 없다.</li>
    </ul>
  </li>
  <li>이 식을 잘 보면 \( \mu \) 에 대해 이차형식(<code class="highlighter-rouge">quadrtatic</code>)의 식이 된다.</li>
  <li>이제 \( p(\mu) \) 에 대해 고민해보자.
    <ul>
      <li>당연하겠지만 이 함수는 \( \mu \) 의 사전 확률 함수로 공액(<code class="highlighter-rouge">conjugate</code>) 분포를 사용하게 될 것이다.</li>
      <li>따라서 여기서는 당연히 가우시안 분포로 고려한다.</li>
      <li>베이지안 추론 방식이 이해되지 않는 경우 별도의 교재를 참고하거나 3장을 살펴보도록 하자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(\mu) = N(\mu|\mu_0, \sigma_0^2) \qquad{(2.138)}</script>

<ul>
  <li>이렇게 하면 사후 확률 분포를 다음을 통해 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p(u|{\bf x}) \propto p({\bf x}|\mu)p(\mu) \qquad{(2.139)}</script>

<ul>
  <li>이 확률 함수는 공액적 특성에 의해 가우시안 분포가 된다.</li>
</ul>

<script type="math/tex; mode=display">p(\mu|{\bf x}) = N(\mu|\mu_N, \sigma_N^2) \qquad{(2.140)}</script>

<ul>
  <li>단,</li>
</ul>

<script type="math/tex; mode=display">\mu_N = \frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_{ML} \qquad{(2.141)}</script>

<script type="math/tex; mode=display">\frac{1}{\sigma_N^2}=\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2} \qquad{(2.142)}</script>

<ul>
  <li>앞서 <em>MLE</em> 를 통한 평균 값의 추론 결과는 아래와 같았었다.
    <ul>
      <li>위에서 얻어진 평균 식과의 차이가 어떠한지를 생각해보자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_n \qquad{(2.143)}</script>

<ul>
  <li>베이지안 추론을 통해 얻어진 평균값과 분산에 대해 고찰하는 것은 매우 의미있는 일이다.</li>
</ul>

<p><strong>사후 확률의 평균</strong></p>

<ul>
  <li>우선 베이지안 추론을 통해 얻어진 사후 확률에 대한 평균값에 대해 생각해 보자.
    <ul>
      <li>만약 \( N \) 이 \( N=0 \) 이라면 평균값은 그냥 \( \mu_0 \) 가 된다. (즉, 최초 설정한 평균이 된다.)</li>
      <li>만약 \( N \) 이 \( N\rightarrow\infty \) 라면 평균 값은 결국 <em>MLE</em> 의 결과와 같아지게 된다. ( \( \mu_{N\rightarrow\infty}=\mu_{ML} \))</li>
    </ul>
  </li>
</ul>

<p><strong>사후 확률의 분산</strong></p>

<ul>
  <li>이제 베이지안 추론을 통해 얻어진 사후 확률의 분산값에 대해 생각해보자.
    <ul>
      <li>만약 \( N \) 이 \( N=0 \) 이라면 분산도 평균과 마찬가지로 \( \sigma_0^2 \) 이 된다.</li>
      <li>만약 \( N \) 이 \( N\rightarrow\infty \) 라면 분산 값은 점점 0에 가까워진다. ( \( \sigma_{N\rightarrow\infty}^2=\sigma^2/N \))
        <ul>
          <li>이 의미는 정확도(precision) 가 계속 증가한다는 의미이고,</li>
          <li>결국 분포의 모양이 점점 높은 피크를 가지게 된다는 의미가 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure2.12.png" alt="figure2.12" class="center-block" height="200px" /></p>

<ul>
  <li>위의 그림은 실제 가우시안 분포를 따르는 확률 분포에 대한 사후 확률 분포를 도식화한 것이다.</li>
  <li>\( N \) 이 증가할 수록 분산 값이 작아지는 것을 확인할 수 있다.</li>
  <li>이를 \( D \) 차원의 가우시안 모델에 적용하는 것도 그리 어렵지 않다.</li>
</ul>

<hr />

<ul>
  <li>우리는 이미 가우시안 분포에서 평균 값을 순차 추정의 식으로 변환하는 것을 살펴보았다.
    <ul>
      <li>이 식에서는 입력 데이터 \( x_N \) 이 주어졌을 때 \( N-1 \) 번째 데이터 관찰된 데이터와의 조합으로 업데이트 식을 작성하였다.</li>
      <li>사실 베이지안 패러다임에서는 자연스럽게 이러한 순차 처리 방식을 적용할 수 있도록 해준다. (predictive model)</li>
      <li>이를 이용하여 가우시안 평균에 대한 추론 방법을 확인해 보도록 하자.</li>
      <li>식에서 우선 \( x_N \) 데이터만을 분리하는 과정이 필요하다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(\mu|{\bf x}) \propto \left[p(\mu)\prod_{n=1}^{N-1}p(x_n|\mu)\right]p(x_N|\mu) \qquad{(2.144)}</script>

<ul>
  <li>위의 식에서 네모난 영역으로 된 곳이 \( N-1 \) 번째 까지의 관찰 데이터에 관한 식이 된다.
    <ul>
      <li>물론 위의 식을 그대로 사용하기 위해서는 모든 관찰 데이터는 독립적(i.i.d)으로 생성되어야 한다.</li>
    </ul>
  </li>
  <li>어쨌거나 베이지안 추론 방식에서는 구하고자 하는 평균의 값이 점추정 값이 아니라 사후 확률( \(p(\mu|{\bf x}) \) ) 분포로 제공되게 되므로 위의 식을 그대로 업데이트 식으로 사용하면 된다.
    <ul>
      <li>만약 평균 값을 점추정하고 싶은 경우 분포식의 평균 값을 사용하면 된다.</li>
      <li>현재는 가우시안 분포를 따르므로 최빈값(mode)를 구하거나 평균을 구하거나 얻어지는 값은 동일하다.</li>
      <li>공분산의 값은 고정된 값이라 가정하여 식에서 생략했다.</li>
    </ul>
  </li>
  <li>다음으로 공분산 추론하는 방법을 알아보도록 한다.
    <ul>
      <li>평균을 구할 때 공분산 값이 고정되어있다고 가정한 것과 마찬가지로 공분산을 추론할 때에는 고정된 평균값이 주어졌다고 가정한다.</li>
      <li>계산의 편리성을 위해 다시 한번 공액 사전 분포 (conjugate prior distribution)를 사용한다.</li>
      <li>실제 계산에서는 공분산 값을 직접 추론하는 것보다 공분산의 역수, 즉 정확도(precision) \( \lambda \equiv 1/\sigma^2 \) 를 구하는게 낫다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|\lambda) = \prod_{n=1}^{N} N(x_n|\mu, \lambda^{-1}) \propto \lambda^{N/2} \exp\left\{ -\frac{\lambda}{2} \sum_{n=1}^{N}(x_n-\mu)^2\right\} \qquad{(2.145)}</script>

<ul>
  <li>정확도에 대한 사후 확률 분포에서 사용되는 공액 사전 분포는 <strong>감마</strong> (<em>gamma</em>) 분포이다.</li>
  <li>이에 대한 식은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">Gam(\lambda|a,b)=\frac{1}{\Gamma(a)}b^a\lambda^{a-1}\exp(-b\lambda) \qquad{(2.146)}</script>

<ul>
  <li>여기서 \( \Gamma(a) \) 는 감마 함수로 다음과 같이 정의되어 있다.</li>
</ul>

<script type="math/tex; mode=display">\Gamma(x) = \int_{0}^{\infty}u^{x-1}e^{-u}du</script>

<ul>
  <li>감마 분포의 평균과 분산은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">E[\lambda] = \frac{a}{b} \qquad{(2.147)}</script>

<script type="math/tex; mode=display">var[\lambda] = \frac{a}{b^2} \qquad{(2.148)}</script>

<ul>
  <li>\( a \) 와 \( b \) 의 여러 값에 대한 감마 분포를 그림으로 나타내면 다음과 같다.</li>
</ul>

<div class="text-center">
  <img src="/kyo-study/images/ml_study/Figure2.13a.png" alt="Figure 2.13a" height="150px" />
  <img src="/kyo-study/images/ml_study/Figure2.13b.png" alt="Figure 2.13b" height="150px" />
  <img src="/kyo-study/images/ml_study/Figure2.13b.png" alt="Figure 2.13c" height="150px" />
</div>

<ul>
  <li>\( a&gt;0 \) 인 경우 감마 분포는 유한 적분(finite integral)이며 \( a \ge 1 \) 인 경우 분포 자체는 유한값이다.
    <ul>
      <li>교재에는 이렇게 간단하게 적혀있으나 이게 무슨 말인지 모르겠다.
        <ul>
          <li>실제로 해당 기능을 구현할 때 수렴 여부로 결과 값을 만들 수 있는지 없는지 체크할 때 쓴다는 건 어디서 들었다.</li>
        </ul>
      </li>
      <li>우선 \( a \ge 1 \) 여야 \( \infty \) 로 수렴하는 값이 없다는 것은 알 수 있다.</li>
      <li>\( \Gamma(a) \) 함수는 \( a&gt;0 \) 이어야 유한 적분(finite integral)을 만족한다.
        <ul>
          <li>이 부분이 조금 이해가 안된다. 수학적 배경이 있는 듯.</li>
          <li>대략적으로 적분 가능 구간이 존재한다는 것은 알 수 있다. (a의 값에 따라 달라진다.)</li>
          <li>이후에 이에 대한 전개가 없으므로 그냥 넘어가기로 하자.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>이제 사전 분포를 \( Gam(\lambda|a_0, b_0) \) 로 고려하여 전개한다.</li>
  <li>그러면 최종적으로 사후 확률을 다음과 같이 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p(\lambda|{\bf x}) \propto \lambda^{a_0-1}\lambda^{N/2} \exp \left\{-b_0\lambda-\frac{\lambda}{2}\sum_{n=1}^{N}(x_n-\mu)^2\right\} \qquad{(2.149)}</script>

<ul>
  <li>공액(conjugate)적인 속성에 의해 위의 분포도 감마 분포를 따르게 된다.
    <ul>
      <li>이 때의 \( a \) 와 \( b \) 값을 구해보도록 하자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">a_N = a_0 + \frac{N}{2} \qquad{(2.150)}</script>

<script type="math/tex; mode=display">b_N = b_0 + \frac{1}{2}\sum_{n=1}^{N}(x_n-\mu)^2 = b_0 + \frac{N}{2}\sigma_{ML}^2 \qquad{(2.151)}</script>

<ul>
  <li>위의 사후 확률 분포 식을 유심히 보면 정규화를 위한 상수가 생략되어 있는 것을 알 수 있는데, 필요하다면 \( \Gamma(a_N) \) 을 이용하여 계산할 수 있다.</li>
  <li>\( a_N \) 을 구하는 식으로부터 \( N \) 개의 관찰 데이터가 \( a_N \) 값에 미치는 영향을 확인해볼 수 있다.
    <ul>
      <li>\( a_N \) 의 식을 보면 \( N/2 \) 만큼 값이 보정되고 있다.</li>
      <li>만약 \( N=2a_0 \) 라면 어떻게 될까? 이러면 \( a_N = a_0 + \frac{2a_0}{2} = 2a_0 \) 가 된다.
        <ul>
          <li>즉, 초기 값에서 \( a_0 \) 만큼 증가함을 알 수 있음.</li>
        </ul>
      </li>
      <li>\( b_N \) 도 마찬가지인데 \( N=2a_0 \) 라면 사전 확률의 분산 값은 \( 2b_0/(2a_0)=b_0/a_0 \) 이고 이를 대입하면 \( b_N=2b_0 \) 가 된다.
        <ul>
          <li>즉, 초기 값에서 \( b_0 \) 만큼 증가함을 알 수 있음</li>
        </ul>
      </li>
      <li>결국 effective number 는 \( N=2a_0 \) 지점임
        <ul>
          <li>여기서 effective number는 관찰 데이터의 영향력이 지정된 사전 확률의 영향력을 넘어서는 지점에서의 관찰 데이터의 수라고 생각하면 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>참고로 지금까지 분산을 구하기 위해 정확도(precision)를 이용하여 식을 전개하였는데, 반대로 공분산을 이용하여 식을 전개할수도 있다.
    <ul>
      <li>이 때에는 공액 분포로 감마 분포가 아니라 역감마 분포 (<em>inverse gamma distribution</em>)를 사용한다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>이제 평균과 공분산(실제로 구하는 것은 정확도)을 둘 다 모른다고 가정할 때의 파라미터 추정 방식을 살펴보도록 하자.</li>
  <li>이 때의 공액 사전 분포를 찾기 위해 가능도 함수에서 \( \mu \) 과 \( \lambda \) 의 의존성을 확인해야 한다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|\mu, \lambda) = \prod_{n=1}^{N}\left(\frac{\lambda}{2\pi}\right)^{1/2} \exp\left\{-\frac{\lambda}{2}(x_n-\mu)^2\right\}\\
\propto \left[\lambda^{1/2}\exp\left(-\frac{\lambda\mu^2}{2}\right)\right]^N\exp\left\{\lambda\mu\sum_{n=1}^{N}x_n-\frac{\lambda}{2}\sum_{n=1}^{N}x_n^2\right\} \qquad{(2.152)}</script>

<ul>
  <li>위 식은 가능도 함수로 이로 부터 \( \mu \) 와 \( \lambda \) 값을 추론해야 한다.</li>
  <li>식을 완전히 분해할 수는 없고, 식의 전개를 통해 어느 정도 분해를 한다.</li>
  <li>이제 사전 확률(prior distribution)을 좀 살펴보도록 하자.
    <ul>
      <li>여기서 관심을 가져야 하는 모수는 2개. 즉 평균과 분산을 동시에 랜덤 변수로 고려해야 한다.</li>
      <li>따라서 사용되는 사전 확률은 \( p(\mu, \lambda) \) 가 된다.</li>
      <li>원래 공액적 관계를 만들어내기 위해서는 사전 확률의 분포와 사후 확률의 분포를 같도록 만들어야 한다.</li>
      <li>여기서는 가능도 함수에 내포된 \( \mu \) 와 \( \lambda \) 의 함수적인 의존성을 그대로 유지한 사전 분포를 생각해본다.
        <ul>
          <li>그냥 가능도 함수 꼴로 사전 분포를 만들어본다는 이야기.</li>
          <li>근데 이게 가능한건가 하는 생각이 들기는 하지만 어쨌거나 현재 가우시안 분포를 다루고 있기 때문에,</li>
          <li>사전 분포를 가우시안이나 감마 분포의 형태로 취급하는 것도 이상한 것은 아님 (원래 공액 관계의 분포임)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(\mu, \lambda) \propto \left[\lambda^{1/2}\exp\left(-\frac{\lambda\mu^2}{2}\right)\right]^\beta\exp\{c\lambda\mu-d\lambda\}\\
= \exp\left\{-\frac{\beta\lambda}{2}(\mu-c/\beta)^2\right\}\lambda^{\beta/2}\exp\left\{-(d-\frac{c^2}{2\beta})\lambda\right\} \qquad{(2.153)}</script>

<ul>
  <li>가능도 함수 꼴로 만들되 자잘한 상수 값은 항상 동일할 수 없기 때문에 추가적인 상수를 도입한다. ( \( c \) , \( d \) , \( \beta \) )</li>
  <li>결합 확률 식에 의해 \( p(\mu, \lambda) = p(\mu|\lambda)p(\lambda) \) 가 성립하므로 식을 다음과 같이 기술할 수도 있다.</li>
</ul>

<script type="math/tex; mode=display">p(\mu, \lambda) = N(\mu|\mu_0, (\beta\lambda)^{-1})Gam(\lambda|a,b) \qquad{(2.154)}</script>

<ul>
  <li>위의 식을 보아하니 가정한 분포 꼴로 대충 떨어지는 것 같다.</li>
  <li>각각을 대입하여 전개하면 다음과 같은 식을 얻게 된다.</li>
</ul>

<script type="math/tex; mode=display">\mu_0 = \frac{c}{\beta}</script>

<script type="math/tex; mode=display">a=(1+\beta)/2</script>

<script type="math/tex; mode=display">b=d-\frac{c^2}{2\beta}</script>

<ul>
  <li>이 분포는 <em>Normal-Gamma</em> 분포의 형태 꼴이다.</li>
  <li>이를 도식화하면 다음과 같다.</li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure2.14.png" alt="figure2.14" class="center-block" height="200px" /></p>

<ul>
  <li>위의 그림은 \( \mu_0=0 \) , \( \beta=2 \) , \( a=5 \) , \( b=6 \)  일 때의 컨투어이다.</li>
</ul>

<hr />

<ul>
  <li>만약 1차원이 아니라 \( D \) 차원일 때에는 어떻게 처리해야 하나?
    <ul>
      <li>사실 별로 어렵지는 않다.</li>
      <li>다변량 가우시안 분포 \( N({\bf x}|{\pmb \mu}, \Lambda^{-1}) \) 을 도입하여 전개하면 된다.</li>
    </ul>
  </li>
  <li>이 때 사용되는 사전 확률 분포는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">W(\Lambda|{\bf W}, v) = B|\Lambda|^{(v-D-1/2)}\exp\left(-\frac{1}{2}Tr({\bf W}^{-1}\Lambda\right) \qquad{(2.155)}</script>

<ul>
  <li>
    <p>이런 분포를 위샤트 분포 (<em>Wishart distribution</em>)라고 한다.</p>
  </li>
  <li>여기서 \( v \) 는 자유도(degrees of freedom)라고 하고, \( {\bf W} \) 는 \( D \times D \) 크기의 행렬이 된다.</li>
  <li>정규화 상수인 \( B \) 는 다음과 같이 정의된다.</li>
</ul>

<script type="math/tex; mode=display">B({\bf W}, v) = |{\bf W}|^{-v/2}\left(2^{vD/2}\pi^{D(D-1)/4}\prod_{i=1}^{D}\left(\frac{v+1-i}{2}\right)\right)^{-1} \qquad{(2.156)}</script>

<ul>
  <li>1차식에서와 마찬가지로 정확도 행렬이 아닌 공분산 행렬로 이를 전개 가능하다.
    <ul>
      <li>이 때에는 역 위샤트 분포 (<em>inverse Wishart distribution</em>)를 사용해야 한다.</li>
    </ul>
  </li>
  <li>앞서 설명한 바와 같이 공액 사전 분포를 다음과 같이 기술할 수도 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\pmb \mu}, \Lambda|\mu_0, \beta, {\bf W}, v) = N({\pmb \mu}|{\pmb \mu}_0, (\beta\Lambda)^{-1})W(\Lambda|{\bf W}, v)</script>

<ul>
  <li>이를 <em>normal-Wishart</em> 분포 또는 <em>Gaussian-Wishart</em> 분포라고 부른다.</li>
</ul>

<hr />

<ul>
  <li>2.3절의 분량이 너무 길어지는 관계로 이후 절은 Part.III 으로 넘긴다.</li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoon'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
