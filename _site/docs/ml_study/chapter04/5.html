<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>5. Bayesian Logistic Regression</title>
  <link rel="stylesheet" href="/kyo-study/css/main.css">
  <link rel="stylesheet" href="/kyo-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-study/docs/ml_study/chapter04/5.html">
  <script src="/kyo-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-study/"><strong>kyo-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/kyo-study/categories/paper_study"><strong>Review Of Papers</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/opt_study"><strong>Introduction to Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">Machine Learning</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 4</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/0">0. Linear Models for Classification</a>
      </li>
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/1">1. Discriminant Functions</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/2">2. Prob. Generative Models</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/3">3. Prob. Discriminative Models</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/4">4. Laplace Approximation</a>
      </li>
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-study/docs/ml_study/chapter04/5">5. Bayesian Logistic Regression</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-study/blob/gh-pages/docs/ml_study/chapter04/5.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>5. Bayesian Logistic Regression</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>이제 로지스틱 회귀 문제에 베이지안 추론을 도입할 차례이다. (알다시피 Bishop 은 베이지안 덕후)</li>
  <li>사실 로지스틱 회귀에 베이지언 추론을 적용하기는 쉽지 않다. (원래 intractable 속성이다.)
    <ul>
      <li>따라서 사전 확률 분포와  <em>likelihood</em> 함수의 곱을 정규화 하는 과정이 필요하다.</li>
    </ul>
  </li>
  <li>또한 베이지안 추론에서 빠질 수 없는 예측 분포(predictive distribution)도 도출해 보도록 하자.</li>
</ul>

<h2 id="laplace-approximation">4.5.1 라플라스 근사 (Laplace approximation)</h2>

<ul>
  <li>4.4 절에서 소개한 라플라스 근사법을 파라미터 \( {\bf w} \) 의 사후 분포를 근사하는데 사용할 것이다.
    <ul>
      <li>사후 분포를 가우시안의 중앙에 맞추는 작업을 진행하게 된다.</li>
      <li>이러한 작업에는 로그 사후 분포를 두번 미분해야 하는 작업이 필요하다. (헤시안 행렬을 구하기 위해)</li>
    </ul>
  </li>
  <li>일단 \( {\bf w} \) 의 사전 분포로 가우시안 분포를 고려한다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf w}) = N({\bf w}|{\bf m}_0, {\bf S}_0) \qquad{(4.140)}</script>

<ul>
  <li>여기서 \( {\bf m}_0 \) 와 \({\bf S}_0 \) 는 고정된 초모수(hyper-parameter)이다.</li>
  <li>이제 \( {\bf w} \) 의 사후 분포를 생각해보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf w}|{\bf t}) \propto p({\bf w})p({\bf t}|{\bf w}) \qquad{(4.141)}</script>

<ul>
  <li>여기서 \( {\bf t} \) 는 \( {\bf t} = (t_1,…,t_N)^T \) 이다.</li>
  <li>사후 분포에 로그를 취해 식을 전개해 보도록 하자.</li>
</ul>

<script type="math/tex; mode=display">\ln({\bf w}|{\bf t}) = - \frac{1}{2}({\bf w}-{\bf m}_0)^T{\bf S}_0^{-1}({\bf w}-{\bf m}_0) + \sum_{n=1 }^{N} \{t_n \ln y_n + (1-t_n)\ln(1-y_n) \} + const \qquad{(4.142)}</script>

<ul>
  <li>이 때 \( y_n = \sigma({\bf w}^T \phi_n) \) 이다.</li>
  <li>가우시안 근사 사후 분포를 얻기 위해서 가장 먼저 사부 분포의 값을 최대화 하는(MAP : maximum a posterior) 지점을 찾아야 한다.
    <ul>
      <li>이 위치를 \( {\bf w}_{MAP} \) 이라고 하자. 이는 근사할 가우시안 함수의 중앙 위치가 된다. (기대값으로 사용된다.)</li>
    </ul>
  </li>
  <li>가우시안 근사 분포의 분산 값을 고려해보자.
    <ul>
      <li>여기서는 계산의 편리성을 위해 정확도(precision) 단위로 식을 전개하도록 하자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">S_N^{-1} = -\nabla\nabla \ln p({\bf w}|{\bf t}) = {\bf S}_0^{-1} + \sum_{n=1}^{N} y_n(1-y_n)\phi_n\phi_n^T \qquad{(4.143)}</script>

<ul>
  <li>이제 최종적으로 가우시안 근사 분포를 다음과 같이 정의하면 된다.</li>
</ul>

<script type="math/tex; mode=display">q({\bf w}) = N({\bf w}|{\bf w}_{MAP}, {\bf S}_N) \qquad{(4.144)}</script>

<ul>
  <li>이 값을 활용하여 예측 분포를 작성하면 된다.</li>
</ul>

<h2 id="predictive-distribution">4.5.2 예측 분포 (Predictive distribution)</h2>

<ul>
  <li>새로운 데이터 \( {\bf x} \) 가 주어졌을 때 클래스 \( C_1 \) 에 속하는 확률을 알아보기 위한 예측 분포를 작성하는 식을 만들어보자.
    <ul>
      <li>문제를 단순하게 하기 위해서 2-class 문제를 다루어보도록 하자.</li>
    </ul>
  </li>
  <li>앞서 사후 분포 \( p( {\bf w} | {\bf t}) \) 를 가우시안 근사 함수로 처리하면 된다고 이야기했었다.
    <ul>
      <li>참고로 입력 속성(feature)은 \( \phi({\bf x}) \) 로 입력되게 된다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(C_1|\phi, {\bf t}) = \int p(C_1|\phi, {\bf w})p({\bf w}|{\bf t})d{\bf w} \simeq \int \sigma({\bf w}^T \phi)q({\bf w})d{\bf w} \qquad{(4.145)}</script>

<ul>
  <li>식을 잘 보면 사후 분포가 근사 분포인 \( q \) 로 대치되었다는 것을 알 수 있다.</li>
  <li>
    <p>마찬가지로 \( C_2 \) 에 대응되는 사후 확률은 \( p(C_2|\phi, {\bf t}) = 1 - p(C_1|\phi, {\bf t}) \) 로 생각할 수 있다.</p>
  </li>
  <li>예측 확률 분포를 작성하기 위해서 먼저 \( {\bf w} \) 에 대한 함수 \( \sigma({\bf w}^T\phi) \) 를 \( \phi \) 로 투영(projection)해야 한다.
    <ul>
      <li>입력 벡터 \( {\bf x} \) 가 기저 함수인 \( \phi \) 로 변환되므로 일단 이 단위로 데이터를 변환시킨다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\sigma({\bf w}^T\phi) = \int \delta(a-{\bf w}^T\phi)\sigma(a)da \qquad{(4.146)}</script>

<ul>
  <li>뜬금없이 이러한 변형식이 나왔는데 교재에는 자세한 설명이 없다.
    <ul>
      <li>이와 관련된 내용은 조금 뒤에 하기로 하고 우선은 이런 식으로 변환이 가능하다고 생각하고 전개를 해보도록 하자.</li>
      <li>여기서 \( \delta(\cdot) \) 는 <code class="highlighter-rouge">Dirac delta</code> 라고 불리우는 함수이다.</li>
      <li>어쨌거나 이 식을 다시 식 (4.145) 에 대입하면 다음 식을 얻게 된다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\int \sigma({\bf w}^T\phi)q({\bf w})d{\bf w} = \int \sigma(a)p(a)da \qquad{(4.147)}</script>

<script type="math/tex; mode=display">p(a) = \int \delta(a-{\bf w}^T\phi)q({\bf w})d{\bf w} \qquad{(4.148)}</script>

<ul>
  <li>위와 같은 식을 얻는다.</li>
  <li>이 식에서 \( p(a) \) 는 결국 가우시안 분포가 된다.
    <ul>
      <li>일단 \( q({\bf w}) \) 는 정의에 의해 가우시안 분포.</li>
      <li>\( \delta(a - {\bf w}^T\phi) \) 는 \( {\bf w} \) 에 대한 선형 제약식(linear constraint)으로 생각할 수 있다.</li>
      <li>\( p(a) \) 는 결국 \( \phi \) 의 방향과 수직(orthogonal)인 모든 방향에 대해 결합 분포 \( q({\bf w}) \) 를 주변화(marginal) 하는 식이 된다.</li>
      <li>이 분포는 결국 가우시안 분포가 되는데 이는 이미 2.3.2 절에서 다룬 내용이다.</li>
    </ul>
  </li>
  <li>이 가우시안 분포의 평균과 분산은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">\mu_a = E[a] = \int p(a)a da = \int q({\bf w}){\bf w}^T \phi d{\bf w} = {\bf w}_{MAP}^T\phi \qquad{(4.149)}</script>

<script type="math/tex; mode=display">\sigma_a^2 = var[a] = \int p(a)\{ a^2 - E[a]^2 \}da = \int q({\bf w}) \{({\bf w}^T\phi)^2 - ({\bf m}_N^T\phi)^2 \}d{\bf w} = \phi^T{\bf S}_N\phi \qquad{(4.150)}</script>

<ul>
  <li>이 식이 (식 3.58) 과 동일함을 확인하도록 하자.
    <ul>
      <li>물론 현재는 분류 문제를 다루고 있기 때문에 노이즈의 분산 \( \beta \) 는 \( 0 \) 인 경우와 같다.</li>
    </ul>
  </li>
  <li>이제 예측 분포의 변분 근사(variational approximation)는 다음과 같이 생각할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p(C_1|{\bf t}) = \int \sigma(a)p(a)da = \int \sigma(a)N(a|\mu_a, \sigma_a^2)da \qquad{(4.151)}</script>

<ul>
  <li>이 식도 마챁가지로 2.3.2 절에서 다룬 식을 통해 직접 유도가 가능하다.</li>
</ul>

<h4 id="inverse-probit-function---">Inverse probit function 을 사용한 근사법</h4>

<ul>
  <li>(식 4.151)은 사실 logistic sigmoid 와 가우시안의 콘볼루션(convoltion) 식인데 이 식은 (식 4.114)에 정의된 Invert Probit 함수로 근사 가능하다.
    <ul>
      <li>즉, \( \sigma(a) \simeq \Phi(\lambda a) \) 로 근사하게 된다.</li>
      <li>로지스틱 시그모이드와 probit 함수의 유사성은 이미 (그림 4.9)로 살펴보았다.</li>
      <li>참고로 \( \lambda = \pi / 8 \) 인 경우 (그림 4.9) 에서처럼 함수의 결과가 유사해진다.</li>
    </ul>
  </li>
  <li>근사 식으로 전개하여보자.</li>
</ul>

<script type="math/tex; mode=display">\int \Phi(\lambda a)N(a|\mu, \sigma^2)da = \Phi \left(\frac{\mu}{(\lambda^{-2}+\sigma^2)^{1/2}}\right) \qquad{(4.152)}</script>

<script type="math/tex; mode=display">\int \sigma(a)N(a|\mu, \sigma^2)da \simeq \sigma(k(\sigma^2)\mu) \qquad{(4.153)}</script>

<script type="math/tex; mode=display">k(\sigma^2) = (1+\pi \sigma^2 / 8)^{-1/2} \qquad{(4.154)}</script>

<ul>
  <li>여기서 (식 4.151) 에 적용하면 다음의 식을 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p(C_1|\phi, {\bf t}) = \sigma(k(\sigma_a^{2})\mu_a) \qquad{(4.155)}</script>

<ul>
  <li>참고로 결정 경게는 \( p(C_1 | \phi, {\bf t}) = 0.5 \) 인 지점으로 찾을 수 있다. ( \( \mu_a = 0 \) )</li>
  <li>이 경우 \( {\bf w} \) 의 MAP 값을 사용하여 얻은 결정 경계와 동일한 결과를 얻게 된다.</li>
</ul>

    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoon'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
