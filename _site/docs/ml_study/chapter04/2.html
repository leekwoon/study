<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2. Prob. Generative Models</title>
  <link rel="stylesheet" href="/kyo-study/css/main.css">
  <link rel="stylesheet" href="/kyo-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-study/docs/ml_study/chapter04/2.html">
  <script src="/kyo-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-study/"><strong>kyo-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/kyo-study/categories/paper_study"><strong>Review Of Papers</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/opt_study"><strong>Introduction to Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">Machine Learning</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 4</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/0">0. Linear Models for Classification</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/1">1. Discriminant Functions</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-study/docs/ml_study/chapter04/2">2. Prob. Generative Models</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/3">3. Prob. Discriminative Models</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/4">4. Laplace Approximation</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-study/docs/ml_study/chapter04/5">5. Bayesian Logistic Regression</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-study/blob/gh-pages/docs/ml_study/chapter04/2.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>2. Prob. Generative Models</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>교재 1.5.4 절에서 간단하게나마 Generative 모델을 이용한 분류 방식에 대해 다루긴 했었다.</li>
  <li>우선 시작하기에 앞서 용어를 좀 정리하자.
    <ul>
      <li>이 교재에서는 용어를 좀 엄격하게 쓰는 편인데, 지겹지만 잘 구분해서 쓰자</li>
      <li>사후 확률 (posterior) : \( p(C_k|{\bf x}) \)
        <ul>
          <li>임의의 데이터 \( x \) 가 주어졌을 때 이 데이터가 특정 클래스 \( C_k \) 에 속할 확률</li>
          <li>\( x \) 는 샘플 하나를 의미하며 벡터로 표기된 이유는 여러 개의 feature를 가지기 때문임</li>
        </ul>
      </li>
      <li>클래스-조건부 밀도 (class-conditional density) : \( p({\bf x}|C_k) \)
        <ul>
          <li>가능도 함수(likelihood function)가 아니냐고 묻는 분이 계시는데, 아니다.</li>
          <li>특정 클래스에서 입력된 하나의 데이터 \( x \) 가 발현될 확률을 의미하며, 각각의 클래스별로 계산되기 때문에
            <ul>
              <li>데이터를 클래스 단위로 나누어 놓고 나면 각 클래스에 대한 \( p({\bf x}) \) 를 의미하는 것과 마찬가지가 된다.</li>
              <li>보통 가능도 함수(<em>likelihood</em>) 등을 통해 얻어진 모수 값을 이용하여 분포의 모양을 선택한 뒤 \( x \) 의 확률 값을 구하게 된다.
                <ul>
                  <li>\(p({\bf x}|\theta_{ml}) \)</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>가능도 함수 (<em>likelihood</em>) : \( p({\bf X}|C_k) = \prod p({\bf x}_i|C_k) \)
        <ul>
          <li>주어진 샘플 데이터가 실제 발현될 확률값을 주로 사용하며, 로그를 붙이는게 일반적이다.</li>
          <li>샘플 데이터는 i.i.d 를 가정하므로 보통은 확률 곱으로 표현 가능하다.</li>
          <li>특정 분포(distribution)를 사용하는 경우 주로 모수 추정에 사용된다.</li>
          <li>모수 추정이 완료되면 클래스-조건부 밀도 등의 식에서 이를 모수 값으로 사용한다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>클래스-조건부 밀도 \( p({\bf x}|C_k) \) 를 이용한 접근 방식에 대해 자세히 알아보도록 하자.
    <ul>
      <li>Generative 모델은 사후 확률 \( p(C_k|{\bf x}) \) 를 직접 구하는 것이 아니라 간접적 사후 확률을 예측하는 모델이다.</li>
      <li>따라서 사후 확률 대신 클래스-조건부 밀도와 사전 확률 등으로 사후 확률을 예측한다.</li>
      <li>즉, 임의의 \( x \) 가 특정 클래스에 속할 확률 값을 확인하고 이 중 가장 큰 확률 값을 가지는 클래스로 \( x \) 가 속할 클래스를 결정할 수 있다.</li>
      <li>가장 먼저 <strong>2-class</strong> 문제를 살펴보자. 다음은 \( x \) 가 클래스 \( C_1 \) 에 속할 확률을 모델링하는 식이다. (베이즈 룰)</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(C_1|{\bf x}) = \dfrac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_1)p(C_1)+p({\bf x}|C_2)p(C_2)}=\dfrac{1}{1+\exp(-a)} = \sigma\;(a) \qquad{(4.57)}</script>

<script type="math/tex; mode=display">a=\ln{\dfrac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_2)p(C_2)}} \qquad{(4.58)}</script>

<ul>
  <li>\( \sigma\;(a) \) 는 로지스틱 시그모이드(logistic sigmoid) 함수이며, 다음과 같이 정의된다.</li>
</ul>

<script type="math/tex; mode=display">\sigma\;(a)=\dfrac{1}{1+\exp(-a)} \qquad{(4.59)}</script>

<ul>
  <li>수식이 왜 이렇게 전개되는지 궁금할수도 있는데, 생각보다 쉽다.</li>
</ul>

<script type="math/tex; mode=display">\alpha = p({\bf x}|C_1)p(C_1)</script>

<script type="math/tex; mode=display">\beta = p({\bf x}|C_2)p(C_2)</script>

<script type="math/tex; mode=display">\dfrac{\alpha}{\alpha+\beta}=\dfrac{1}{\frac{\alpha+\beta}{\alpha}} = \dfrac{1}{1+\frac{\beta}{\alpha}}=\dfrac{1}{1+\exp({-\ln(\frac{\alpha}{\beta})})}</script>

<ul>
  <li>시그모이드 함수를 그리면 다음과 같다.</li>
</ul>

<p><img src="/kyo-study/images/ml_study/Figure4.9.png" alt="figure4.9" class="center-block" height="200px" /></p>

<ul>
  <li>그림에서 보듯 <em>sigmoid</em> 라는 용어는 함수 식이 \( S \) 자 형태를 취하기 때문에 붙여진 이름이다.
    <ul>
      <li>이런 함수들을 가끔 <em>squashing function</em> 이라고도 부르는데,</li>
      <li>\( x \) 축 영역의 모든 값에 대응되는 함수 출력 값이 특정 범위에만 존재하기 때문이다. (여기서는 0 ~ 1 사이)</li>
    </ul>
  </li>
  <li>붉은 색 실선이 시그모이드(sigmoid) 함수이며, 청색 점선은 프로빗(probit) 함수로 이 함수는 이후에 설명된다.
    <ul>
      <li>프로빗 함수가 시그모이드 함수와 거의 유사한 특징이 있음을 보이기 위해 함께 표현한 것이다.</li>
    </ul>
  </li>
  <li>시그모이드 함수는 다음과 같은 특징이 있다.</li>
</ul>

<script type="math/tex; mode=display">\sigma\;(-a) = 1 - \sigma\;(a) \qquad{(4.60)}</script>

<ul>
  <li>시그모이드가 도입된 이유는,
    <ul>
      <li>시그모이드 자체가 특정 값으로 수렴되는 성질이 있으며 (0~1 사이의 값)</li>
      <li>따라서 이 값을 확률 값으로 고려를 해도 되기 때문이다.</li>
      <li>게다가 모든 점에서 연속이며 미분 가능하므로 수학적 전개에도 매우 편리하다.</li>
    </ul>
  </li>
  <li>다음으로 로지스틱 시그모이드의 역(inverse)은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">a=\ln\left(\dfrac{\sigma}{1-\sigma}\right) \qquad{(4.61)}</script>

<ul>
  <li>이를 <em>로짓(logit)</em> 이라고 부른다.</li>
  <li>2-class 문제에서는 \( \ln \frac{p(C_1|{\bf x})}{p(C_2|{\bf x})} \) . 즉 각각의 확률에 대한 비율(ratio)에 로그(log)를 붙인 것과 같다.
    <ul>
      <li>이를 로그 오즈(log odds) 라고 한다.</li>
      <li>좀 더 자세히 설명하자면, 성공 확률 \( p \) 와 실패 확률 \( (1-p) \) 에 대한 odds 는 \( \frac{p}{(1-p)} \) 이므로 여기에 로그를 붙인 것과 같다.</li>
      <li>로짓이라는 개념이 좀 뜬금없어 보이지만 이후 식에서도 자주 등장하므로 잊지는 말자.</li>
    </ul>
  </li>
  <li>\( p(C_1|x) \) 는 여기서 사후(posterior) 확률이 되는데 이를 로지스틱 회귀식을 이용해서 기술한다.</li>
  <li>이 식을 가지고 좀 더 일반화하면 \( K&gt;2 \) 인 경우에서도 식을 확장할 수 있다.
    <ul>
      <li>이를 일반 선형 모델(generalized linear model)이라고 한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(C_k|{\bf x}) = \dfrac{p({\bf x}|C_k)p(C_k)}{\sum_j{p({\bf x}|C_j)p(C_j)}}=\dfrac{\exp(a_k)}{\sum_j{\exp(a_j)}} \qquad{(4.62)}</script>

<ul>
  <li>이를 <em>normalized exponential</em> 함수라고 부르며, 다중 클래스 분류에 사용되는 시그모이드 식이 된다.
    <ul>
      <li>이 때 \( a({\bf x}) \) 는 \( {\bf x} \) 에 대한 선형 함수로 처리 가능하다.</li>
      <li>사실 맨 처음에 설명했던 2-class 모델도 위의 식으로 전개하면 동일한 식을 얻어낼 수 있다.
        <ul>
          <li>위 식을 \( \exp(a_1)/(\exp(a_1)+\exp(a_2)) \) 로 놓고 전개하면 2-class 시그모이드가 나온다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>어쨌거나 여기서 \( a_k \) 는 다음과 같이 정의된다.</li>
</ul>

<script type="math/tex; mode=display">a_k = \ln(p({\bf x}|C_k)p(C_k)) \qquad{(4.63)}</script>

<ul>
  <li><em>normalized exponential</em> 함수를 소프트 맥스 (<em>softmax function</em>) 함수라고 부른다.</li>
  <li>이는 <em>max</em> 함수에 대한 평활화(smoothed) 버전이기 때문이다.
    <ul>
      <li>평활화의 의미를 현재 고려할 필요는 없고, 일단 모든 점에서 미분 가능한 식으로 변환된다고 생각하자.</li>
      <li>만약 \( a_k \gg a_j \) 라면 \( p(C_k|{\bf x}) \simeq 1 \) 이고 \( p(C_j|{\bf x}) \simeq 0 \) 이 된다. (단, \( j \neq k \))</li>
    </ul>
  </li>
  <li>다음 절에서는 이러한 클래스 조건 밀도를 특정 분포로 가정해서 처리하는 과정을 살펴볼 것이다.
    <ul>
      <li>우선 연속적인 입력 값을 확인한 뒤에 이산적인 값을 처리하는 방법을 살펴보자.</li>
    </ul>
  </li>
  <li><strong>짧은 요약</strong>
    <ul>
      <li>이번 절이 좀 두서없이 설명되는 것들이 많아 혼동되는 부분이 있는데, 간단한 사실만을 기억하면 된다.
        <ul>
          <li>2-class 에서는 <strong>sigmoid</strong> 를 이용해서 분류.</li>
          <li>K-class 에서는 <strong>softmax</strong> 를 이용해서 분류.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="continuous-inputs">4.2.1 연속적인 입력값 (Continuous inputs)</h2>

<ul>
  <li>일단 클래스-조건부 밀도(class-conditional density)가 가우시안 형태라고 가정하자.</li>
  <li>또한 가장 간단한 구조를 고려하여 모든 클래스 사이의 공분산(covariance) 값은 모두 동일하다고 가정한다. (중요한 제약)</li>
  <li>그러면 어떤 클래스가 주어졌을 때 해당 데이터가 나올 확률은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|C_k) = \dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp\left\{-\dfrac{1}{2}({\bf x}-{\bf \mu}_k)^T\Sigma^{-1}({\bf x}-{\bf \mu}_k)\right\} \qquad{(4.64)}</script>

<ul>
  <li><strong>2-class</strong> 문제로 이를 고려해보자.</li>
  <li>최초 조건부 확률 식에 판별식을 넣는다.</li>
</ul>

<script type="math/tex; mode=display">p(C_1|{\bf x})=\sigma({\bf w}^T{\bf x}+w_0) \qquad{(4.65)}</script>

<ul>
  <li>이 식은 위에서  \( \sigma(a) \) 로 정의되어 있었다. 마찬가지로 \( a=\ln\frac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_2)p(C_2)} \) 였다.</li>
  <li>가우시안 분포 식을 위에 넣고 대입한다.</li>
</ul>

<script type="math/tex; mode=display">{\bf w}^T{\bf x}+w_0 = \ln\frac{p({\bf x}|C_1)p(C_1)}{p({\bf x}|C_2)p(C_2)}</script>

<ul>
  <li>이를 전개하면 다음의 식이 얻어진다.</li>
</ul>

<script type="math/tex; mode=display">{\bf w} = \Sigma^{-1}({\bf \mu_1}-{\bf \mu_2}) \qquad{(4.66)}</script>

<script type="math/tex; mode=display">w_0 = -\frac{1}{2}{\bf \mu_1}^T\Sigma^{-1}{\bf \mu_1} + \frac{1}{2}{\bf \mu_2}^T\Sigma^{-1}{\bf \mu_2} + \ln{\frac{p(C_1)}{p(C_2)}} \qquad{(4.67)}</script>

<ul>
  <li>\( x \) 에 대한 2차 텀이 모두 사라지면서 직선 식이 얻어진다.
    <ul>
      <li>이는 두 클래스 사이의 공분산이 동일하기 때문에 이차항의 계수가 부호만 다르고 크기가 일치하여 약분되기 때문이다.</li>
      <li>자세한 전개 방식은 생략한다.</li>
    </ul>
  </li>
  <li>여기서 조금만 상상을 해보면, 같은 분산을 가지는 두개의 가우시안 분포가 만나는 지점은 당연히 직선의 형태일 것이라 생각해볼 수 있다.</li>
</ul>

<div class="text-center">
  <img src="/kyo-study/images/ml_study/Figure4.10a.png" alt="Figure 4.10a" height="200px" />
  <img src="/kyo-study/images/ml_study/Figure4.10b.png" alt="Figure 4.10b" height="200px" />
</div>

<ul>
  <li>왼쪽 그림은 2-class 조건에서의 확률 밀도를 표현한 것이다.
    <ul>
      <li>공분산이 동일하므로 모양은 같다. (평균 위치만 틀림)</li>
      <li>따라서 이를 분할하는 경계면은 당연히 직선이 된다.</li>
    </ul>
  </li>
  <li>오른쪽은 \( x \) 에 대한 시그모이드 함수로 어떤 클래스에 속하는지에 대한 부분은 색깔로 확인할 수 있다
    <ul>
      <li>딱 중간 위치에서 클래스가 나누어지는 것을 알 수 있다 (2-클래스 문제이므로)</li>
    </ul>
  </li>
  <li>만약 이를 \( K \) 클래스 문제로 확장하면 어떻게 될까?
    <ul>
      <li>식 (4.62)와 식 (4.63)을 일반화시키면 \( K \) 클래스 문제도 풀어낼 수 있다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\bf w}_k = \Sigma^{-1}{\bf \mu}_k \qquad{(4.69)}</script>

<script type="math/tex; mode=display">w_{k0} = -\frac{1}{2}{\bf \mu}_{k}^{T}\Sigma^{-1}{\bf \mu}_k + \ln p(C_k)\qquad{(4.70)}</script>

<ul>
  <li>사실 우리는 식을 간단히 만들기 위해 각 클래스에서 사용되는 공분산이 동일하다고 가정했다.</li>
  <li>그러나 만약 각각의 클래스들의 공분산이 다르다면 어떻게 될까?
    <ul>
      <li>이 경우 경계면을 구하는 수식에서 이차항이 사라지지 않고 남게 된다.</li>
      <li>따라서 경계 면이 곡선이 될 수도 있다.</li>
    </ul>
  </li>
</ul>

<div class="text-center">
  <img src="/kyo-study/images/ml_study/Figure4.11a.png" alt="Figure 4.11a" height="200px" />
  <img src="/kyo-study/images/ml_study/Figure4.11b.png" alt="Figure 4.11b" height="200px" />
</div>

<ul>
  <li>위의 그림을 보면 녹색과 적색의 클래스는 동일한 공분산을 가지므로 경계면이 직선이 됨을 알 수 있다.</li>
  <li>그러나 청색 클래스 데이터는 다른 공분산을 가지고 있기 때문에 경계면이 곡선이 된다.</li>
  <li>교재에서는 위의 그림 말고는 공분산이 다른 경우에 대한 자세한 설명이 나와있지 않다.
    <ul>
      <li>하지만 다른 패턴 인식 책에는 잘 나와 있는 경우가 많으니 참고할 것. (예로 Duda 패턴인식 2장)</li>
    </ul>
  </li>
</ul>

<h3 id="mle--maximum-likelihood-solution">4.2.2 최대 가능도(MLE) 풀이법 (Maximum likelihood solution)</h3>
<ul>
  <li>지금까지 우리는 조건부 확률에 대한 모델로 \( p({\bf x}|C_k) \) 를 고려했었다.</li>
  <li>자, 이제 MLE를 이용하여 모수를 추정하는 방법을 살펴보자.</li>
  <li>더불어 이번 절에서는 사전(prior) 확률 \( p(C_k) \) 를 함께 고려하여 전개하도록 한다.
    <ul>
      <li>물론 착각하지 말아야 할 것은 모수(parameter)에 대한 사전 확률을 의미하는 것이 아니다.</li>
      <li>즉, 베이지안 방식의 모수 추정을 사용한다는 의미는 아니라는 것이다.</li>
      <li>여기서 말하는 사전 확률은 클래스에 대한 사전 확률을 의미한다. \( p(C_k) \)</li>
    </ul>
  </li>
  <li>여기서도 마찬가지로 2-class 문제를 먼저 살펴보고, \( K&gt;2 \) 인 경우로 확장하도록 한다.</li>
</ul>

<h3 id="class--">2-class 문제인 경우</h3>

<ul>
  <li>결합 확률로 문제를 정의해 보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}_n, C_1) = p(C_1)p({\bf x}_n|C_1) = \pi N({\bf x}_n\;\mu_1, \Sigma)</script>

<script type="math/tex; mode=display">p({\bf x}_n, C_2) = p(C_2)p({\bf x}_n|C_2) = (1-\pi) N({\bf x}_n\;\mu_2, \Sigma)</script>

<ul>
  <li>여기서도 마찬가지로 공분산은 서로 같다고 가정한다.</li>
  <li>실제 데이터는 \( ({\bf x}_n, t_n) \) 으로 \( t_n=1 \) 인 경우 \( C_1 \)으로, \( t_n=0 \) 인 경우 \( C_2 \) 로 분류한다.</li>
  <li>이제 가능도 함수(likelihood)를 정의해보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf t}, {\bf X} \;| \pi, {\bf \mu}_1, {\bf \mu}_2, \Sigma) = \prod_{n=1}^N\left(\pi N({\bf x}_n\;|{\bf \mu}_1, \Sigma)\right)^{t_n}\left((1-\pi)N({\bf x}_n\;|{\bf \mu}_2, \Sigma)\right)^{1-t_n} \qquad{(4.71)}</script>

<ul>
  <li>2-class 에서는 가능도 함수를 binomial 분포와 같은 식으로 정의할 수 있다.</li>
  <li>
    <p>여기서 \( {\bf t} \) 는 \( {\bf t} = (t_1,…,t_N)^T \) 로 정의된다.</p>
  </li>
  <li>\( \pi \) 구하기
    <ul>
      <li>\( \pi \) 를 구하는 식은 별로 안 어렵다.</li>
      <li>로그 가능도 함수를 \( \pi \) 에 대해 미분하여 관련된 항목만 모으면 다음과 같아진다.</li>
      <li>이 값을 0으로 놓고 \( \pi \) 를 구하면 된다. (지금까지 많이 해왔던 작업이다.)</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\sum_{n=1}^{N}\left\{ t_n\ln\pi + (1-t_n)\ln(1-\pi) \right\} \qquad{(4.72)}</script>

<script type="math/tex; mode=display">\pi = \dfrac{1}{N}\sum_{n=1}^{N}t_n = \dfrac{N_1}{N} = \dfrac{N_1}{N_1+N_2} \qquad{(4.73)}</script>

<ul>
  <li>여기서 \( N_1 \) 은 \( C_1 \) 에 속하는 샘플의 수이고 \( N_2 \) 는 \( C_2 \) 에 속하는 샘플의 수이다.</li>
  <li>\( \pi \) 는 정확히 \( C_1 \) 에 속하는 샘플의 비율을 의미하게 된다.</li>
  <li>이 식은 K-class 문제로 쉽게 확장 가능하다. ( \( K&gt;2 \))
    <ul>
      <li>이를 일반화하면 \( \pi_k = N_k / N \) 을 얻을 수 있다.</li>
      <li>이에 대한 유도는 연습문제 4.9 에서 확인 가능하다.</li>
    </ul>
  </li>
  <li>\( {\bf \mu} \) 구하기
    <ul>
      <li>마찬가지로 로그 가능도 함수로부터 각각의 \( \mu \) 값으로 미분하여 값을 구한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\sum_{n=1}^{N}t_n\ln N({\bf x}_n\;|{\bf \mu}_1, \Sigma) = -\dfrac{1}{2}\sum_{n=1}^{N}t_n({\bf x}_n-{\bf \mu}_1)^T\Sigma^{-1}({\bf x}_n-{\bf \mu}_1) + const \qquad{(4.74)}</script>

<ul>
  <li>이 식을 0으로 놓고 \( {\bf \mu}_1 \) 에 대해 풀면 다음을 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_1=\dfrac{1}{N_1}\sum_{n=1}^{N}t_n{\bf x}_n \qquad{(4.75)}</script>

<ul>
  <li>마찬가지로 \( {\bf \mu}_2 \) 에 대해서도 동일한 해법으로 계산 가능하다.</li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_2=\dfrac{1}{N_2}\sum_{n=1}^{N}(1-t_n){\bf x}_n \qquad{(4.76)}</script>

<ul>
  <li>\( \Sigma \) 구하기
    <ul>
      <li>마지막으로 \( \Sigma \) 를 구하는 과정을 살펴보자.</li>
      <li>여기서도 동일하게 로그 가능도 함수로부터 공분산으로 미분하여 값을 구한다.</li>
      <li>공분산을 구하는 식은 좀 복잡하기는 하다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">-\dfrac{1}{2}\sum_{n=1}^{N}t_n\ln |\Sigma| -\dfrac{1}{2}\sum_{n=1}^{N}t_n({\bf x}_n-{\bf \mu}_1)^T\Sigma^{-1}({\bf x}_n-{\bf \mu}_1) -\dfrac{1}{2}\sum_{n=1}^{N}(1-t_n)\ln |\Sigma| -\dfrac{1}{2}\sum_{n=1}^{N}(1-t_n)({\bf x}_n-{\bf \mu}_1)^T\Sigma^{-1}({\bf x}_n-{\bf \mu}_1)\\\\
= -\dfrac{N}{2}\ln |\Sigma| - \dfrac{N}{2}Tr\left\{\Sigma^{-1}{\bf S}\right\} \qquad{(4.77)}</script>

<ul>
  <li>여기서 \( {\bf S} \) 는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">{\bf S}=\dfrac{N_1}{N}{\bf S}_1+\dfrac{N_2}{N}{\bf S}_2 \qquad{(4.78)}</script>

<script type="math/tex; mode=display">{\bf S}_1 = \dfrac{1}{N_1}\sum_{n \in C_1} ({\bf x}_n-{\bf \mu}_1)({\bf x}_n-{\bf \mu}_1)^T \qquad{(4.79)}</script>

<script type="math/tex; mode=display">{\bf S}_2 = \dfrac{1}{N_2}\sum_{n \in C_2} ({\bf x}_n-{\bf \mu}_2)({\bf x}_n-{\bf \mu}_2)^T \qquad{(4.80)}</script>

<ul>
  <li>이는 교재 2.3.4 절에서 다루었던 가우시언 공분산 MLE 추정 과정과 거의 같다.</li>
</ul>

<h3 id="k-class----k2-">K-class 구하기 ( \( K&gt;2 \))</h3>

<ul>
  <li>\( \pi \) 구하기 : 간단하게 \( \pi \) 를 구하는 방법만을 살펴보자.
    <ul>
      <li>1-to-K 이진 타겟 값 \( {\bf t} \) 를 이용하여 가능도함수를 정의한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(\phi_n, {\bf t}_n\;|{\bf \pi}) = \prod_{n=1}^{N}\prod_{k=1}^{K}(p(\phi_n\;|C_k)\cdot\pi_k)^{t_{nk}}</script>

<script type="math/tex; mode=display">\ln p(\phi_n, {\bf t}_n\;|{\bf \pi}) = \sum_{n=1}^N\sum_{k=1}^Kt_{nk}\left\{\ln p(\phi_n|C_k) + \ln\pi_k \right\}</script>

<ul>
  <li>\( \sum_k\pi_k = 1 \) 을 활용하여 라그랑지안 승수를 도입한다.</li>
</ul>

<script type="math/tex; mode=display">\ln p(\phi_n, {\bf t}_n\;|{\bf \pi}) + \lambda\left(\sum_{k=1}^K\pi_k-1\right)</script>

<ul>
  <li>\( \pi_k \) 에 대해 미분하면 식을 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">-\pi_k\lambda = \sum_{n=1}^Nt_{nk}=N_k</script>

<ul>
  <li>양 번에 \( \sum_k \) 를 씌우면 \( \pi_k=\frac{N_k}{N} \) 를 얻을 수 있다.</li>
  <li>그 외 K-class 환경 하에서 \( \mu \) 와 \( \Sigma \) 를 구하는 것은 별도의 연습 문제를 참고하기 바란다.</li>
  <li>
    <p>여기서는 간단하게 요약만 해 놓는다.</p>
  </li>
  <li>평균</li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_k = \dfrac{1}{N_k}\sum_{n=1}^N t_{nk}{\bf x}_n</script>

<ul>
  <li>공분산</li>
</ul>

<script type="math/tex; mode=display">\Sigma = \sum_{k=1}^K\dfrac{N_k}{N}{\bf S}_k</script>

<script type="math/tex; mode=display">{\bf S}_k = \dfrac{1}{N_k}\sum_{n=1}^{N}t_{nk}(\phi_n-{\bf \mu}_k)(\phi_n-{\bf \mu}_k)^T</script>

<ul>
  <li>위의 식은 마찬가지로 \( \Sigma \) 가 모든 클래스마다 동일하다는 가정 하에서 얻어진 식이다.</li>
</ul>

<h2 id="discrete-features">4.2.3 데이터가 이산 값일 때의 처리 (Discrete Features)</h2>
<ul>
  <li>이제 입력 값이 연속적인 값이 아니라 이산적인 값이라고 생각해보자. ( \( x_i \))</li>
  <li>문제를 단순화하기 위해 \( x_i \) 가 가질 수 있는 값은 \( x_i \in {0, 1} \) 뿐이다.</li>
  <li>입력 데이터가 \( D \) 차원이라면 각 클래스별로 얻을 수 있는 확률 분포의 실제 \( x \) 의 이산적인 범위는 \( 2^D \) 개이다.
    <ul>
      <li>이 중 독립 변수는 \( 2^D-1 \) 이며 확률의 총 합이 1이기 때문에 변수 하나가 줄어들었다. (summation constraint)</li>
    </ul>
  </li>
  <li>여기서는 \( x \) 의 각 속성(feature)이 독립적이라고 가정하여 계산의 범위를 축소하도록 한다.</li>
  <li>각 속성(feature)들이 모두 독립적이라는 가정을 <em>Naive Bayes</em> 가정이라고도 한다.</li>
  <li>이제 샘플 하나의 클래스-조건부 확률 모델을 다음과 같이 기술할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|C_k) = \prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i} \qquad{(4.81)}</script>

<ul>
  <li>이 식을 K-class 문제에서의 \( a_k \) 함수에 대입하면</li>
</ul>

<script type="math/tex; mode=display">a_k({\bf x})=\ln(p({\bf x}|C_k)p(C_k))</script>

<script type="math/tex; mode=display">a_k({\bf x})=\sum_{i=1}^{D}\left\{x_i\ln \mu_{ki}+(1-x_i)\ln(1-\mu_{ki})\right\}+\ln p(C_k) \qquad{(4.82)}</script>

<ul>
  <li>여기서도 \( a_k \) 함수는 \( x_k \) 에 대해 선형 함수이다. (feature가 독립적이라 가정했으므로)</li>
  <li>2-class에서는 시그모이드를 도임하면 동일한 식을 얻을 수 있다.</li>
  <li>여기는 \( x_i \) 의 값이 이진 값인 경우만 고려했다. 하지만 \( M&gt;2 \) 이 상태를 가지는 \( x_i \) 에 대해서도 유사한 결과를 얻을 수 있다.</li>
</ul>

<h2 id="exponential-family">4.2.4 지수족 패밀리 (Exponential family)</h2>
<ul>
  <li>지금까지 살펴본 내용에 따르면 지금까지 유도한 식들이 일정한 규칙을 따르고 있다는 것을 알 수 있는데,
    <ul>
      <li>사후 확률(posterior)의 경우 가우시안 분포든 이항 분포든 상관 없이 sigmoid 또는 softmax를 이용해서 모델링 할 수 있다.</li>
      <li>당연히 2-class는 시그모이드(sigmoid), K-class ( \( K\ge2 \))는 소프트맥스(softmax)였다.</li>
    </ul>
  </li>
  <li>이걸 더 일반화시킬 수 없을까?
    <ul>
      <li>클래스-조건부 밀도(class-conditional density)가 지수족(exponential family) 분포를 따를 경우 가능하다.</li>
      <li>교재 2.4 절에서 지수족 분포에 대해 설명을 이미 했다.</li>
      <li>즉, 지수족 분포를 따를 경우의 확률 분포는 다음과 같은 일반식으로 기술 가능함을 이미 배웠다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p({\bf x}\;|\lambda_k) = h({\bf x})g(\lambda_k)\exp(\lambda_k^T u({\bf x})) \qquad{(4.83)}</script>

<ul>
  <li>여기서는 \( u({\bf x})={\bf x} \) 로 제한한다.
    <ul>
      <li>이는 앞서 공분산 값을 클래스마다 동일하다고 가정했던 것과 같은 맥락의 제약 조건이다.</li>
      <li>계산을 손쉽게 하기 위한 가정일 뿐이다.</li>
    </ul>
  </li>
  <li>다음으로 scale 파라미터 \( s \) 를 추가한다.
    <ul>
      <li>사실 모든 지수족 분포에는 고유의 파라미터가 있다.</li>
      <li>이게 클래스별로 달라야 하는데 이게 바로 \( \lambda_k \) 이다.</li>
      <li>하지만 우리는 모든 클래스가 동일한 스케일 파라미터를 공유한다고 가정했다.
        <ul>
          <li>예를 들어 공분산이 모든 클래스마다 동일하다고 가정했던 것.</li>
        </ul>
      </li>
      <li>이런 제약 조건을 넣기 위해 임의의 공유 파라미터 \( s \) 를 추가한다.</li>
    </ul>
  </li>
  <li>파라미터를 추가하는 것은 2.4.3절의 (식 2.236)에서 이미 소개하였다.</li>
  <li>이제 기본 식에 \( s \) 파라미터를 추가하자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}\;|\lambda_k, s) = \dfrac{1}{s}h\left(\dfrac{1}{s}{\bf x}\right)g\left(\lambda_k\right)\exp \left(\dfrac{1}{s}\lambda_k^T u({\bf x})\right) \qquad{(4.84)}</script>

<ul>
  <li>
    <p>기존의 \( \lambda \) 와는 별도로 조건절에 파라미터가 추가되었다.</p>
  </li>
  <li>
    <p>2-class 에서 사용한 \( a_k \) 함수에 이를 대입하여 전개해보자.</p>
  </li>
</ul>

<script type="math/tex; mode=display">a({\bf x})=\dfrac{1}{s}(\lambda_1-\lambda_2)^T{\bf x}+\ln g(\lambda_1) - \ln g(\lambda_2) + \ln p(C_1) - \ln p(C_2) \qquad{(4.85)}</script>

<ul>
  <li>이걸 K-class 에도 확장할 수 있는데 간단하게 식만 적어본다.</li>
</ul>

<script type="math/tex; mode=display">a_k({\bf x}) = \dfrac{1}{s}\lambda_k^T{\bf x}+\ln g(\lambda_k) + \ln p(C_k) \qquad{(4.86)}</script>

<ul>
  <li>가정에 의해 모두 선형 식임을 확인할 수 있다.</li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoon'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
