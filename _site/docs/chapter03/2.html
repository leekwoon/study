<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2. Bias-Variance Decomposition</title>
  <link rel="stylesheet" href="/kyo-ml-study/css/main.css">
  <link rel="stylesheet" href="/kyo-ml-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-ml-study/docs/chapter03/2.html">
  <script src="/kyo-ml-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-ml-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-ml-study/"><strong>kyo-ml-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <!-- for drop-down navi -->
            <li class="dropdown">
              <a href="/kyo-ml-study" class="dropdown-toggle" data-toggle="dropdown"><strong>Chapter
                  <b class="caret"></b></strong>
              </a>
              <ul class="dropdown-menu">
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter01/0">1. Introduction</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter02/0">2. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter03/0">3. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter04/0">4. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter05/0">5. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter06/0">6. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter07/0">7. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter08/0">8. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter09/0">9. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter10/0">10. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter11/0">11. Neural Networks</a>
                </li>
                
                
              </ul>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter01/0"><strong>1</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter02/0"><strong>2</strong></a>
            </li>
            
                      
          
          
            
            <li  class="active" >
              <a href="/kyo-ml-study/docs/chapter03/0"><strong>3</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter04/0"><strong>4</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter05/0"><strong>5</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">6</strong></a></li>
                      
          
          
            <li><a href="#"><strong class="text-danger">7</strong></a></li>
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter08/0"><strong>8</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter09/0"><strong>9</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter10/0"><strong>10</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">11</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 3</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter03/0">0. Linear Models for Regression</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter03/1">1. Linear Basis Function Models</a>
      </li>
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-ml-study/docs/chapter03/2">2. Bias-Variance Decomposition</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter03/3">3. Bayesian Linear Regression</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter03/4">4. Bayesian Model Comparison</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter03/5">5. The Evidence Approximation</a>
      </li>
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter03/6">6. Limit. of Fixed Basis Functions</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-ml-study/blob/gh-pages/docs/chapter03/2.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>2. Bias-Variance Decomposition</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>우리는 지금까지 회귀모형을 작성할 때 해당 모델의 형태와 이 때 사용되는 기저함수의 개수가 고정되어 있다고 가정했다.</li>
  <li>그리고 이 문제를 풀기 위해 최소 제곱법을 도입하여 식을 전개했다.</li>
  <li>작은 크기의 샘플 데이터에 대해 복잡한 모델을 사용하는 경우 오버 피팅이 발생하는 현상도 확인했다.
    <ul>
      <li>그러나 오버 피팅을 줄이기 위해 모델의 파라미터 수를 줄이면 모델의 유연성이 떨어지는 현상이 발생한다.</li>
      <li>따라서 파라미터의 수는 유지하되 오버피팅을 막기 위한 수단으로 정칙화(regularization) 기능을 도입했다.</li>
    </ul>
  </li>
  <li>이 때 정칙화 계수 \( \lambda \) 값의 변화에 따라 어떻게 모델이 제약되는지도 살펴보았다.
    <ul>
      <li>하지만 어떻게 하면 가장 좋은 \( \lambda \) 값을 찾을 수 있는지는 아직 언급되지 않았다.</li>
      <li>만약 \( \lambda \) 의 값이 0이 되면 어떻게 될까?</li>
      <li>이런 경우 정칙화(regularization) 요소가 사라짐으로써 비정칙화(unregularization)와 결과와 동일하게 된다.</li>
    </ul>
  </li>
  <li>우리는 이전 장에서 <em>MLE</em> 에서 발생하는 오버 피팅 현상이 썩 좋은 현상은 아니며</li>
  <li>베이즈 추론을 사용하는 경우 이 문제를 어느 정도 해결할 수 있다는 것을 배웠다.</li>
  <li>따라서 당연히 지금까지 했던 내용들을 자연스럽게 베이즈 추론을 사용하는 버전까지 확장을 하게 될 것이다.</li>
  <li>그러나 이번 장에서는 이를 살펴보기에 앞서,
    <ul>
      <li>우선 빈도론자(frequentist)의 입장에서 bias-variance 트레이드-오프(trade-off)에 대해 좀 살펴보기로 하자.</li>
    </ul>
  </li>
  <li>우리는 이미 1.5.5절에서 회귀식에서 사용하는 결정 이론에 대해 논의했다.
    <ul>
      <li>기억이 나지 않는다면 반드시 해당 절에서 관련 내용 확인할 것.</li>
    </ul>
  </li>
  <li>우리는 최적의 예측 모델을 찾기 위한 방안으로 조건부 분포 \( p(t|x) \) 함수를 이용하여 Loss 함수를 정의하여 식을 전개했다.</li>
  <li>최종적으로 다음과 같은 식을 얻었음을 상기하자.</li>
</ul>

<script type="math/tex; mode=display">h(x)=E[t|x]=\int{tp(t|x)\;dt} \qquad{(3.36)}</script>

<ul>
  <li>여기서 중요한 포인트는 다음의 두 가지를 구별해 볼 필요가 있다는 것이다.
    <ul>
      <li>결정 이론으로 부터 얻어진 Squared Loss 함수</li>
      <li>파라미터 MLE 추정 과정에서 얻어진 Sum-of-Squre 에러 함수</li>
    </ul>
  </li>
  <li>우리가 1.1.5절에서 다룬 식을 잠시 꺼내어 보자.</li>
</ul>

<script type="math/tex; mode=display">E[L]=\int{\{y(x)-h(x)\}^2p(x)\;dx}+\iint{\{h(x)-t\}^2p(x,t)}\;dx\;dt \qquad{(3.37)}</script>

<ul>
  <li>이 중에서 두번 째 텀(term)은 \( y(x) \) 와 직접적인 관련이 없다.
    <ul>
      <li>이 영역은 데이터의 노이즈(noise)를 의미하게 된다.</li>
    </ul>
  </li>
  <li>첫번 째 텀(term)을 보면 우리가 선택해야 할 \( y(x) \) 가 어떤 값으로 수렴해야 하는지를 이해할 수 있다.
    <ul>
      <li>왜냐하면 제곱 식에 의해 이 값은 항상 0이거나 0보다 큰 값을 가지게 된다.</li>
      <li>우리의 목표는 에러를 최소화해야 하는 것이기 때문에 최대한 \( h(x) \) 와 동일한 \( y(x) \) 를 찾아야 한다.</li>
      <li>데이터가 충분히 많다면, \( h(x) \) 에 매우 근사된 \( y(x) \) 를 쉽게 찾을 수 있을 것이다.</li>
      <li>하지만 우리에게 주어진 데이터는 고정된 크기 \( N \) 을 가진 데이터 \( D \) 일 뿐이다. (즉, 샘플이다.)</li>
    </ul>
  </li>
  <li>이제 논의를 확장하기 위해 몇 가지 가정을 해보자.
    <ul>
      <li>우리는 분포 \( p(t, x) \) 를 통해 생성된 \( N \) 개의 샘플로 구성된 데이터 집합 \( D \) 를 얻을 수 있음.</li>
      <li>또한 여러 개의 데이터 집합을 얻을 수 있으며 모든 샘플은 서로 독립적으로 생성된다고 가정할 수 있음. ( <em>i.i.d</em> )</li>
      <li>우리는 각각의 데이터 집합을 이용하여 예측 함수 \( y(x;D) \) 를 만들어 낼 수 있다.</li>
    </ul>
  </li>
  <li>이런 경우 서로 다른 데이터 집합은 서로 다른 손실(loss) 함수와 결과를 만들어 낼 것이다.</li>
  <li>아까 사용했던 식의 첫번 째 텀에 이 내용을 반영해 보자.</li>
</ul>

<script type="math/tex; mode=display">\{y(x;D)-h(x)\}^2 \qquad{(3.38)}</script>

<ul>
  <li>하나의 데이터 집합을 이용하여 손실 함수를 구할 수 있기 때문이다.</li>
  <li>이제 여러 개의 데이터 집합으로 구해진 함수의 평균값 \( E_D[y(x;D)] \) 를 이용해서 식을 전개해보자.</li>
</ul>

<script type="math/tex; mode=display">\{y(x;D)-E_D[y(x;D)]+E_D[y(x;D)]-h(x)\}^2\\=\{y(x;D)-E_D[y(x;D)]\}^2+\{E_D[y(x;D)-h(x)]\}^2\\\\+2\{y(x;D)-E_D[y(x;D)]\}\{E_D(x;D)-h(x)\} \qquad{(3.39)}</script>

<ul>
  <li>위의 식을 통해 우리는 데이터로 얻어진 \( y(x;D) \) 와 회귀식 \( h(x) \) 사이의 차이는 2개의 속성으로 나누어짐을 확인할 수 있다.</li>
  <li>첫번 째 텀은 바이어스(bias)라고 하고, 두번 째 텀은 분산(variance)이라고 한다.</li>
</ul>

<script type="math/tex; mode=display">E_D[\{y({\bf x};D)-h({\bf x})\}^2]\\
= \{E_D[y({\bf x};D)]-h({\bf x})\}^2+ E_D[\{y({\bf x};D)-E_D[y({\bf x};D)]\}^2] \qquad{(3.40)}</script>

<ul>
  <li>최종적으로 우리는 기대 손실(expected loss) 값을 다음과 같이 정의할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">{Expected}\;{loss} = (bias)^2 + variance + noise \qquad{(3.41)}</script>

<ul>
  <li>각각의 값들은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">(bias)^2=\int{\{E_D[y(x;D)]-h(x)\}^2p(x)\;dx} \qquad{(3.42)}</script>

<script type="math/tex; mode=display">variance = \int{E_D[\{y(x;D)-E_D[y(x;D)]\}^2]p(x)\;dx} \qquad{(3.43)}</script>

<script type="math/tex; mode=display">noise=\iint{\{h(x)-t\}^2p(x,t)\;dx\;dt} \qquad{(3.44)}</script>

<ul>
  <li>우리의 목표는 기대 손실 \( E[L] \) 값을 최소화하는 것이다. 그리고 이 값은 위의 3가지 요소로 나누어(decomposed) 고려할 수 있다.</li>
  <li>이 중 노이즈 값은 관찰 데이터 자체에 포함된 에러로서 우리가 제어하기 힘든 영역이다.</li>
  <li>따라서 우리는 바이어스(bias) 와 분산(variance) 값을 적절히 조절하여 기대 손실이 가장 작은 값을 만들어야 한다.
    <ul>
      <li>보통 이 두 요소는 서로 <em>trade-off</em>  관계를 가진다.</li>
      <li>따라서 유연한 모델은 낮은 바이어스, 높은 분산도를 가지게 되며,</li>
      <li>엄격한 모델은 높은 바이어스, 낮은 분산도를 가지게 된다.</li>
    </ul>
  </li>
</ul>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure3.5a.png" alt="Figure 3.5a" height="200px" />
  <img src="/kyo-ml-study/images/Figure3.5b.png" alt="Figure 3.5b" height="200px" />
</div>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure3.5c.png" alt="Figure 3.5c" height="200px" />
  <img src="/kyo-ml-study/images/Figure3.5d.png" alt="Figure 3.5d" height="200px" />
</div>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure3.5e.png" alt="Figure 3.5e" height="200px" />
  <img src="/kyo-ml-study/images/Figure3.5f.png" alt="Figure 3.5f" height="200px" />
</div>

<ul>
  <li>위의 그림은 바이어스와 분산 사이의 trade-off 관계를 잘 나타태는 그림이다.
    <ul>
      <li>하나의 샘플은 25개의 데이터로 구성된다. (일정 위치로부터 샘플 데이터를 추출하였다.)</li>
      <li>이러한 샘플 집합은 총 \( L=100 \) 개로 구성된다. 각각의 학습 결과는 왼쪽 그림의 붉은 색 선이 된다.</li>
      <li>오른쪽 그림의 붉은 색 그래프는 왼쪽 샘플 집합 결과의 평균 값이다.</li>
      <li>오른쪽 그림의 녹색 선은 우리가 예측해야 할 \( \sin2\pi \) 곡선이다.</li>
      <li>\( \lambda \) 값을 변화시켜 가면서 결과를 확인하였다.</li>
    </ul>
  </li>
  <li>\( \ln\lambda \) 값이 큰 경우 높은 바이어스와 낮은 분산 값을 가진다.
    <ul>
      <li>따라서 각각의 샘플 집합들 사이의 분산도가 작은 것을 알 수 있다.</li>
      <li>대신 실제 예측할 수 있는 값의 범위가 제한적이어서 최종 결과가 잘못 도출될 수도 있다.</li>
      <li>샘플 수를 충분히 확보하지 못하는 경우에는 가급적 이러한 안정된 모델을 선호하기도 한다.</li>
    </ul>
  </li>
  <li>\( \ln\lambda \) 값이 작은 경우에는 낮은 바이어스와 높은 분산 값을 가진다.
    <ul>
      <li>따라서 최종 결과로 사용된 예측 값(평균값)은 실제 결과와 매우 유사한 것을 확인할 수 있다.</li>
      <li>하지만 개별 샘플 집합의 결과를 보면 분산도가 매우 큰 것을 알 수 있다.</li>
      <li>따라서 샘플 수가 충분하지 못한 경우 실제 결과와 매우 큰 편차를 보이는 잘못된 결과를 얻을 수 있다.</li>
      <li>샘플 수가 충분하기만 하다면 이런 방식이 좋다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>위와 같은 <em>bias-variance</em> 의 <em>trade-off</em> 관계를 다음과 같은 식으로도 확인할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">\bar{y}(x) = \frac{1}{L}\sum_{l=1}^{L}y^{(l)}(x) \qquad{(3.45)}</script>

<ul>
  <li>squared-bias 와 variance 에 대한 합은 다음과 같이 주어진다.</li>
</ul>

<script type="math/tex; mode=display">(bias)^2  = \frac{1}{N}\sum_{n=1}^{N}\{\bar{y}(x_n)-h(x_n)\}^2 \qquad{(3.46)}</script>

<script type="math/tex; mode=display">variance = \frac{1}{N}\sum_{n=1}^{N}\frac{1}{L}\sum_{l=1}^{L}\{y^{(l)}(x_n)-\bar{y}(x_n)\}^2 \qquad{(3.47)}</script>

<ul>
  <li>이 식을 도식화한 결과를 보도록 하자.</li>
</ul>

<p><img src="/kyo-ml-study/images/Figure3.6.png" alt="figure3.6" class="center-block" height="200px" /></p>

<ul>
  <li>결국 분홍색 선이 가장 최소가 되는 지점의 \( \lambda \) 를 고르는 문제가 된다.</li>
  <li>
    <p>이 위치에서 테스트 에러도 최소가 되는 것을 확인할 수 있다.</p>
  </li>
  <li>이러한 관점은 빈도론자가 모델을 어떻게 바라보고 있는지에 대한 좋은 해석점이 된다.
    <ul>
      <li>다만 위의 수식에서는 여러 데이터 집합에 대한 앙상블 형태로 식이 전개되기 때문에 충분한 데이터가 확보되어야 한다.</li>
      <li>결국 데이터 자체가 모델의 중요한 요소가 된다.</li>
    </ul>
  </li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoongithub'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-ml-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
