<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>5. Duality</title>
  <link rel="stylesheet" href="/study/css/main.css">
  <link rel="stylesheet" href="/study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/study/docs/opt_study/intro_opt/5.html">
  <script src="/study/js/jquery-1.12.0.min.js"></script>    
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/study/"><strong>study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/study/categories/ml_study"><strong>Machine Learning</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/opt_study"><strong>Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/paper_study"><strong>Review Of Papers</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/piano"><strong>Piano</strong></a>
            </li>
            
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Introduction to Optimization</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      <li class=" " >
        <a href="/study/docs/opt_study/intro_opt/1">1. Optimization Basics</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/study/docs/opt_study/intro_opt/2">2. Convex Sets</a>
      </li>
      
      
      
      
      
      <li class=" " >
        <a href="/study/docs/opt_study/intro_opt/3">3. Convex Functions</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/study/docs/opt_study/intro_opt/4">4. Convex Optimizaion</a>
      </li>
      
      
      
      <li class="active " >
        <a href="/study/docs/opt_study/intro_opt/5">5. Duality</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/study/docs/opt_study/intro_opt/6">6. Gradient Descent</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/study/docs/opt_study/intro_opt/7">7. Projection method</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/study/docs/opt_study/intro_opt/8">8. Game Theory</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/study/blob/gh-pages/docs/opt_study/intro_opt/5.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>5. Duality</h1>
    </header>
    <hr class="title">
    <article>
      <h2 id="outline">Outline</h2>
<blockquote>
  <p>지금까지 Existence, Convex sets and functions, Convex optimization 을 배웠다.
Constrained Optimization 과 더불어 Optimization 의 꽃이라 할 수 있는 Duality 를 배워보자.</p>
</blockquote>

<h2 id="lagrangian">Lagrangian</h2>

<ul>
  <li>다음과 같은 constrained optimization 문제를 고려해보자.</li>
</ul>

<script type="math/tex; mode=display">\underset{x}{\text{min}} \; f_0(x)</script>

<script type="math/tex; mode=display">\text{s.t. } f_i(x) \le 0, \; \forall i=1,2,...,m</script>

<script type="math/tex; mode=display">h_i(x) = 0,  \; i = 1,2,...,p</script>

<ul>
  <li>where the domain is <script type="math/tex">D</script></li>
  <li>And its optimal point is <script type="math/tex">p^*</script></li>
</ul>

<p><strong>Lagrangian</strong></p>

<ul>
  <li>Lagrangian <script type="math/tex">L</script> : <script type="math/tex">\mathbf{R^n} \times \mathbf{R^m} \times \mathbf{R^p} \to D\times \mathbf{R^m} \times \mathbf{R^p}</script></li>
</ul>

<script type="math/tex; mode=display">L(x,\lambda,\nu) = f_0(x)+\sum_{i=1}^m \lambda_i f_i(x)+\sum_{i=1}^m \nu_i h_i(x) \\
=f_0(x)+\lambda^T f(x)+\nu^T h(x)</script>

<ul>
  <li>이때, <script type="math/tex">\lambda_i</script>, <script type="math/tex">\nu_i</script> 는 <script type="math/tex">f_i(x)</script>, <script type="math/tex">h_i(x)</script> 에 대한 Lagrangian multiplier 이라 한다.</li>
  <li>duality 를 적용하기 이전의 original problem 을 primal problem 이라 한다. (새로 최적화 할 dual problem 은 뒤에서 정의 할 것)</li>
</ul>

<p><strong>Dual function</strong></p>

<ul>
  <li>Dual function <script type="math/tex">g</script> : <script type="math/tex">\mathbf{R^m} \times \mathbf{R^p} \to \mathbf{R}</script></li>
</ul>

<script type="math/tex; mode=display">g(\lambda,\nu) = \inf_{x\in D}L(x,\lambda,\nu)\\
=\inf_{x\in D} f_0(x)+\sum_{i=1}^m \lambda_i f_i(x)+\sum_{i=1}^m \nu_i h_i(x) \\
=\inf_{x\in D} f_0(x)+\lambda^T f(x)+\nu^T h(x)</script>

<ul>
  <li>dual function 은 <script type="math/tex">\lambda,\nu</script> 에 대해 affine 하다.</li>
  <li>참고로, original problem 의 <strong>convex 여부에 상관 없이</strong> dual function <script type="math/tex">g</script>는 <strong>concave</strong> 하다.</li>
  <li>dual function 은 concave 한 pointwise infimum 함수이다. (pointwise supremum 이 convex 하므로)</li>
  <li>뒤에서 배울 duality gap 과 연결해서 생각해 보자.</li>
  <li>주어진 모든 constraint 를 만족하는 <script type="math/tex">\bar{x}</script> 와 original problem 의 optimal value <script type="math/tex">p^*</script> 를 가정해보자.</li>
  <li><strong><script type="math/tex">\lambda \ge 0</script> 을 만족한다면,</strong></li>
  <li><script type="math/tex">f(\bar{x}) \le 0</script> , <script type="math/tex">h(\bar{x}) =0</script> 이기 때문에,</li>
</ul>

<script type="math/tex; mode=display">\lambda^T f(\bar{x})+\nu^T h(\bar{x}) \le 0</script>

<ul>
  <li>위의 식을 이용하면</li>
</ul>

<script type="math/tex; mode=display">L(\bar{x},\lambda,\nu) = f_0(\bar{x})+\lambda^T f(\bar{x})+\nu^T h(\bar{x}) \le f(\bar{x})</script>

<ul>
  <li>모든 <script type="math/tex">\bar{x}</script> 에 대해 위의 식이 만족하기 때문에</li>
</ul>

<script type="math/tex; mode=display">g(\lambda,\nu) \le p^*</script>

<ul>
  <li>original problem 의 optimal value <script type="math/tex">p^*</script> 와 dual function 값 사이에 <strong>gap 이 존재!</strong></li>
  <li>(We should have <script type="math/tex">g(\lambda,\nu) > -\infty</script>)</li>
  <li>지금까지의 내용을 정리하자면,
    <ul>
      <li>Lagrangian and its dual function are related to a unconstrained optimization problem, since the constraints are embedded into the objective function with Lagrangian multiplier.</li>
      <li>제약조건이 있는 문제의 경우 대부분의 경우 optimize 하기가 어려워, duality 를 이용하여 제약조건이 없는 문제로 바꾼다.</li>
      <li>하지만 optimal solution 과 gap 이 존재. (related to duality gap)</li>
      <li>duality gap 이 없는 조건은 뒤에서 다룰 것이다.</li>
    </ul>
  </li>
</ul>

<h2 id="dual-function-examples">Dual Function: Examples</h2>

<p><strong>Quadratic Problem</strong></p>

<script type="math/tex; mode=display">\underset{x}{\text{min}} \; x^{T}x \\
\text{s.t. } Ax=b</script>

<ul>
  <li>The Lagrangian:</li>
</ul>

<script type="math/tex; mode=display">L(x,\nu)=x^{T}x+\nu^{T}(Ax-b)</script>

<ul>
  <li>Lagrangian 을 <script type="math/tex">x</script> 에 대해 미분</li>
</ul>

<script type="math/tex; mode=display">\nabla_x L(x,\nu)=2x+A^{T}\nu=0 \Rightarrow x=\frac{-1}{2}A^{T}\nu</script>

<ul>
  <li>dual function 을 구하기 위해 Lagrangian 을 최소로 만드는 <script type="math/tex">x</script> 대입
    <ul>
      <li><strong>이때 <script type="math/tex">x</script> 는 제약 조건을 고려하지 않고 구한다.</strong></li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">g(\nu)=\frac{-1}{4}\nu^{T}AA^{T}\nu - b^{T}\nu</script>

<ul>
  <li>참고로, 위의 식을 <script type="math/tex">\nu</script> 에 관한 식으로 나타내기 위해 <script type="math/tex">\nu^{T}b</script> 대신에 <script type="math/tex">b^{T}\nu</script> 로 표현</li>
  <li>Dual function <script type="math/tex">g</script> 는 <script type="math/tex">\nu</script> 에 대해서 concave 하다.</li>
  <li>이렇게 얻어진 Dual function 은 optimal solution 의 lower bound 이다.</li>
</ul>

<script type="math/tex; mode=display">p^{*} \ge \frac{-1}{4}\nu^{T}AA^{T}\nu - b^{T}\nu</script>

<p><strong>Linear Programming</strong></p>

<script type="math/tex; mode=display">\underset{x}{\text{min}} \; c^{T}x \\
\text{s.t. } Ax = b, \; x \ge 0</script>

<ul>
  <li>참고로 <script type="math/tex">x \ge 0</script> 은 <script type="math/tex">-x \le 0</script> 으로 생각하여 <script type="math/tex">\lambda</script> 대신에 <script type="math/tex">-\lambda</script> 를 쓰면 된다.</li>
  <li>The Lagrangian:</li>
</ul>

<script type="math/tex; mode=display">L(x,\lambda,\nu) = c^{T}x-\lambda^{T}x+\nu^{T}(Ax-b)\\
=-b^{T}v+(c-\lambda+A^{T}v)^{T}x</script>

<ul>
  <li><script type="math/tex">L</script> 은 <script type="math/tex">x</script> 에 관해 affine 하다.
    <ul>
      <li>따라서 <script type="math/tex">x</script> 관해 미분하지 않고 바로 dual function 을 구할 수 있다.</li>
    </ul>
  </li>
  <li>The dual function:</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
g(\lambda,\nu)=\underset{x}{\text{inf}} \; L(x,\lambda,\nu)=\begin{cases}
    -b^{T}\nu, & A^{T}v-\lambda+c = 0  \\
    -\infty,              & \text{otherwise}
\end{cases} %]]></script>

<ul>
  <li><script type="math/tex">c-\lambda+A^{T}v = 0</script> 일때 optimal solution 의 lower bound 가 존재한다.</li>
</ul>

<script type="math/tex; mode=display">p^{*} \ge -b^{T}\nu</script>

<ul>
  <li>중요한 사실은, Linear Programming 경우
    <ul>
      <li><script type="math/tex">p^{*} = -b^{T}\nu</script> 을 만족하는 <script type="math/tex">v^{*}</script> 이 항상 존재한다.</li>
      <li>duality gap 이 없다.</li>
    </ul>
  </li>
</ul>

<p>Lagrangian Dual and Conjugate Functions</p>

<p><strong>Conjugate function</strong></p>

<ul>
  <li>Conjugate function 이란,</li>
</ul>

<script type="math/tex; mode=display">f^{*}(y)=\underset{x}{\text{sup}} (y^{T}x-f(x))\\
=-\underset{x}{\text{inf}} -(y^{T}x-f(x))</script>

<ul>
  <li>Conjugate function <script type="math/tex">f^{*}(y)</script> 는 convex 하다.</li>
  <li>dual function 을 conjugate function을 이용하여 표현해보자.</li>
  <li>예를 들어,</li>
</ul>

<script type="math/tex; mode=display">\underset{x}{\text{min}} \; f(x)\\
\text{s.t. } Ax \le b, \; Cx = b</script>

<ul>
  <li>The dual function:</li>
</ul>

<script type="math/tex; mode=display">g(\lambda,\nu)=\underset{x}{\text{inf}} \ f(x)+\lambda^{T}(Ax-b)+\nu^{T}(Cx-d) \\
= -b^{T}\lambda-d^{T}\nu+\underset{x}{\text{inf}} \ f(x)+(A^{T}\lambda+C^{T}\nu) \\
= -b^{T}\lambda-d^{T}\nu-f^{*}(-A^{T}\lambda-C^{T}\nu)</script>

<h2 id="dual-problem">Dual Problem</h2>

<ul>
  <li>앞서서 duality 를 적용하기 이전의 original problem 을 primal problem 이라 정의하였다.</li>
  <li>dual problem 을 정의해보자.</li>
  <li>많은 경우에 optimal solution 과 dual function (lower bound) 간의 gap 이 있었다.</li>
  <li>따라서 problem 을 다음과 같이 정의한다.</li>
  <li>Lagrangian dual problam:</li>
</ul>

<script type="math/tex; mode=display">\underset{\lambda,\nu}{\text{max}} \ g(\lambda,\nu) \\
\text{s.t. } \lambda \ge 0</script>

<ul>
  <li>dual problem 에 대해 설명하자면,
    <ul>
      <li>optimal solution <script type="math/tex">p^{*}</script> 의 best lower bound 를 찾는 것으로 볼 수 있다.</li>
      <li>이때 dual optimal value 를 <script type="math/tex">d^{*}</script> 라 하면,
        <ul>
          <li>
            <script type="math/tex; mode=display">d^{*} \le p^{*}</script>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>예를 들어 Linear Programming 의 dual problem 을 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">\underset{x}{\text{min}} \ c^{T}x \\
\text{s.t. } Ax= b, \ x\ge 0</script>

<ul>
  <li>The dual function:</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
g(\lambda,\nu)=\underset{x}{\text{inf}} \; L(x,\lambda,\nu)=\begin{cases}
    -b^{T}\nu, & A^{T}v-\lambda+c = 0  \\
    -\infty,              & \text{otherwise}
\end{cases} %]]></script>

<ul>
  <li>The dual function:</li>
</ul>

<script type="math/tex; mode=display">\underset{\lambda,\nu}{\text{max}} \ -b^{T}\nu \\
\text{s.t. } A^{T}v-\lambda+c = 0, \; \lambda \ge 0</script>

<ul>
  <li>다르게 표현하면,</li>
</ul>

<script type="math/tex; mode=display">\underset{\lambda,\nu}{\text{max}} \ -b^{T}\nu \\
\text{s.t. } A^{T}v+c \ge 0</script>

<h2 id="weak-and-strong-duality">Weak and Strong Duality</h2>

<ul>
  <li>weak duality : <script type="math/tex">d^{*} \le p^{*}</script></li>
  <li>strong duality : <script type="math/tex">d^{*} = p^{*}</script></li>
  <li>duality gap : <script type="math/tex">d^{*} - p^{*}</script></li>
  <li><strong>Main isuues</strong>
    <ul>
      <li>primal 이랑 dual problem 의 general relation 이 있는지?</li>
      <li>어떤 조건에서 zero duality gap?</li>
      <li>어떤 조건에서 primal 이랑 dual problem 이 optimal soultions 을 가지는지?</li>
      <li>dual optimal solutions 이 primal problem 에게 어떤 유용한 정보를 제공하는지?</li>
    </ul>
  </li>
</ul>

<h2 id="geometric-interpretation-of-duality">Geometric interpretation of Duality</h2>

<ul>
  <li>이제부터 duality 를 geometric 관점에서 바라보자.</li>
</ul>

<script type="math/tex; mode=display">\underset{x}{\text{min}}f_{0}(x) \\
\text{s.t. } f_{1}(x) \le 0</script>

<ul>
  <li>예를 들어, 위와 같은 optimization problem 을 고려해 보자.</li>
</ul>

<script type="math/tex; mode=display">p^{*} = \underset{x}{\text{inf}}\{f_{0}(x) \; \vert \; (f_1(x),f_0(x))\in \mathbf{R} \times \mathbf{R}, f_1(x) \le 0\} \\
=\underset{x}{\text{inf}}\{(\lambda,1)^{T}(f_{1}(x),f_{0}(x)) \; \vert \; (f_1(x),f_0(x))\in \mathbf{R} \times \mathbf{R}, f_1(x) \le 0\} \\
=\underset{x}{\text{inf}}\{(\lambda,1)^{T}(f_{1}(x),f_{0}(x)) \; \vert \; (f_1(x),f_0(x))\in \mathbf{R} \times \mathbf{R}\} \\
=g(\lambda)</script>

<ul>
  <li>geometric 관점으로 생각해보면,</li>
  <li>dual function 은 supporting hyperplane 으로 생각될 수 있다.
    <ul>
      <li><script type="math/tex">y</script> 축이 <script type="math/tex">f_0(x)</script>, <script type="math/tex">x</script> 축이 <script type="math/tex">f_1(x)</script> 일때
        <ul>
          <li>기울기 : <script type="math/tex">-\lambda</script></li>
          <li><script type="math/tex">y</script> 절편 : <script type="math/tex">g(\lambda)</script></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/study/images/opt_study/intro_opt/5.1.png" alt="" class="center-block" /></p>

<p><img src="/study/images/opt_study/intro_opt/5.2.png" alt="" class="center-block" /></p>

<p><img src="/study/images/opt_study/intro_opt/5.3.png" alt="" class="center-block" /></p>

<ul>
  <li><strong>convex problem 에서도 duality gap 이 존재할 수 있다.</strong></li>
  <li>supproting hyperplane 이 <script type="math/tex">y</script> 절편에서 <script type="math/tex">(0,p^{*})</script> 을 지날때 duality gap 이 없는 strong duality 가 된다.</li>
</ul>

<p><img src="/study/images/opt_study/intro_opt/5.4.png" alt="" class="center-block" /></p>

<p><img src="/study/images/opt_study/intro_opt/5.5.png" alt="" class="center-block" /></p>

<h2 id="slaters-constraint-qualification">Slater’s Constraint Qualification</h2>

<ul>
  <li>zero duality gap 을 위한 strong duality condition 들을 배워보자.</li>
</ul>

<p><strong>Slater’s condition</strong></p>

<ul>
  <li><strong>convex optimization</strong> problem 에 대해서,
    <ul>
      <li>(Slater’s condition 은 convex 조건 필요)</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\underset{x}{\text{min}}f(x) \\
\text{s.t. } f_i(x) \le 0, \; \forall i=1,2,...,m \\
Ax=b</script>

<ul>
  <li>만약 problem 이 strictly feasible 하면.. e.g.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\exists x \in \text{int}(D): f_i(x) < 0, \; \forall i=1,2,...,m , \; Ax=b %]]></script>

<ul>
  <li>strong duality 를 만족한다. (증명은 hyperplane theorem 이용)</li>
</ul>

<p><img src="/study/images/opt_study/intro_opt/5.6.png" alt="" class="center-block" /></p>

<h2 id="saddle-point-interpretation">Saddle-Point Interpretation</h2>

<ul>
  <li>(Slater’s condition 과 다르게) <strong>convexity 만족할 필요 없다!</strong></li>
</ul>

<script type="math/tex; mode=display">\underset{x}{\text{inf}} \; f_0(x) \\
\text{s.t. } f_1(x) \le 0</script>

<ul>
  <li>다르게 나타내면</li>
</ul>

<script type="math/tex; mode=display">\underset{x}{\text{inf}} \; \underset{\lambda \ge 0}{\text{sup}} \; \left( f_0(x)+\lambda^{T}f_1(x) \right)</script>

<ul>
  <li>Let <script type="math/tex">L(x,\lambda) = f_0(x)+\lambda^{T}f_1(x)</script>.</li>
  <li><script type="math/tex">L(x,\lambda)</script> 은 <script type="math/tex">\lambda</script> 에 대해 affine 하기 때문에,</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\underset{\lambda \ge 0}{\text{sup}} \; \left( f_0(x)+\lambda^{T}f_1(x) \right) = \begin{cases}
    f_0(x), & f_1(x) \le 0 \\
    \infty,              & \text{otherwise}
\end{cases} %]]></script>

<ul>
  <li>duality gap 이용해 식을 표현해보면,</li>
</ul>

<script type="math/tex; mode=display">\underset{x}{\text{inf}} \; \underset{\lambda \ge 0}{\text{sup}} \; L(x,\lambda) \ge d^{*} =\underset{\lambda \ge 0}{\text{sup}} \; \underset{x}{\text{inf}} \; L(x,\lambda)</script>

<ul>
  <li>따라서 <script type="math/tex">\underset{x}{\text{inf}} \; \underset{\lambda \ge 0}{\text{sup}} \; L(x,\lambda) =\underset{\lambda \ge 0}{\text{sup}} \; \underset{x}{\text{inf}} \; L(x,\lambda)</script> 일때 strong duality 를 만족한다.</li>
</ul>

<p><strong>Saddle-point</strong></p>

<ul>
  <li>For the Lagrangian  <script type="math/tex">L(x,\lambda), (\bar{x},\bar{\lambda})</script> is the <strong>saddle-point</strong> of <script type="math/tex">L</script> if</li>
</ul>

<script type="math/tex; mode=display">L(\bar{x},\lambda)\le L(\bar{x},\bar{\lambda})\le L(x,\bar{\lambda}), \; \forall x, \lambda \ge 0</script>

<ul>
  <li>참고로 , <script type="math/tex">L(\bar{x},\lambda)</script> 이 optimal soultion 의 lower bound.</li>
  <li><script type="math/tex">(\bar{x},\bar{\lambda})</script> 에서 saddle point 임을 보이면 strong duality 를 만족한다.</li>
</ul>

<h2 id="kkt-condition-complementary-slackness">KKT Condition: Complementary Slackness</h2>

<p><strong>Complementary Slackness</strong></p>

<ul>
  <li>Duality gap 이 없다고 가정해보자.</li>
</ul>

<script type="math/tex; mode=display">g(\lambda^*,\nu^*)=\underset{x}{\text{inf}} \; \left( f_0(x)+\lambda^{T}f(x)+\nu^{T}h(x) \right) \\
=f_0(x^*)+\lambda^{T}f(x^*)+\nu^{T}h(x^*)) \\
\le f_0(x^*)</script>

<ul>
  <li>Duality gap 이 없기 위해서는
    <ul>
      <li>모든 <script type="math/tex">i</script> 에 대해서 <script type="math/tex">f_i(x^*) \le 0</script> 이고 <script type="math/tex">\lambda_i \ge 0</script> 이기 때문에,</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\lambda_i^* f_i(x^*) = 0, \; \forall i = 1,2,...,m</script>

<ul>
  <li>이 조건을 <strong>complementary slackness</strong> 라 한다.</li>
  <li>아래와 같이 나타낼 수도 있다.</li>
</ul>

<script type="math/tex; mode=display">\lambda_i^* > 0 \Rightarrow f_i(x^*)=0</script>

<script type="math/tex; mode=display">% <![CDATA[
f_i(x^*) < 0 \Rightarrow \lambda_i = 0 %]]></script>

<h2 id="kkt-condition">KKT Condition</h2>

<ul>
  <li><script type="math/tex">x^*</script> 와 <script type="math/tex">(\lambda^*,\nu^*)</script> 는 각각 primal, dual problem 의 optimal point 이므로</li>
</ul>

<script type="math/tex; mode=display">\nabla L_x(x^*,\lambda^*,\nu^*)=\nabla f_0(x^*)+\sum_{i=1}^m \lambda_i^*\nabla f_i(x^*)+\sum_{i=1}^p \nu_i^*\nabla h_i(x^*) = 0</script>

<ul>
  <li>참고로 위의 조건은 strong duality 의 first order neceesary condition
    <ul>
      <li>strong duality 가 아닌 걸 보일 때 사용가능.</li>
    </ul>
  </li>
  <li>Hence, we must have
    <ul>
      <li>Feasibility : <script type="math/tex">f_i(x^*) \le 0, \; \forall i</script></li>
      <li>Feasibility : <script type="math/tex">h_i(x^*) = 0, \; \forall i</script></li>
      <li>Feasibility : <script type="math/tex">\lambda_i^* \ge 0, \; \forall i</script></li>
      <li>Complementary slackness : <script type="math/tex">\lambda_i^* f_i(x^*) = 0, \; \forall i</script></li>
    </ul>
  </li>
  <li>(feasibility 와 complementary slackness 조건을 만족시키는) 이 조건을 <strong>KKT</strong> (KarushKuhn Tucker) condition 이라 한다.</li>
</ul>

<p><strong>Necessary condition for the nonlinear programming</strong></p>

<ul>
  <li><strong>Theorem</strong> : <script type="math/tex">x^*</script> 가 constrained optimization problem 의 local optimal point 일때, KKT condition 을 만족하는 <strong>unique</strong> 한 Lagrangian multipliers <script type="math/tex">(\lambda^*,\nu^*)</script> 가 존재한다.
    <ul>
      <li><strong>Constrained optimization problem</strong> 의 1st order necessary condition (일차 필요조건).</li>
      <li>달리 말하면 KKT 조건을 만족하는 <script type="math/tex">(\lambda^*,\nu^*)</script> 가 존재하지 않으면 local optimal point 가 아님.</li>
    </ul>
  </li>
</ul>

<p><strong>Sufficient condition for the convex optimization problem</strong></p>

<ul>
  <li><strong>Convex</strong> Optimization 에 대해서..</li>
  <li>만약 <script type="math/tex">x</script> 와 <script type="math/tex">(\lambda^*,\nu^*)</script> 가 KKT condition 을 만족한다면, <script type="math/tex">x</script> 는 primal problem 의 global minimizer 가 되고 <script type="math/tex">(\lambda^*,\nu^*)</script> 은 dual optimal 이 된다.</li>
  <li>또한 strong duality (zero duality gap) 을 만족하게 된다.</li>
  <li>중요한 것은..
    <ul>
      <li><strong>Convexity</strong> 와 <strong>Slater’s condition</strong> 이 만족되면 KKT condition 은 optimality 에 대한 필요충분 조건이 되고</li>
      <li>unconstrained optimization problem 의 sufficient condition 처럼
        <ul>
          <li>Convexity 는 local optimal point 를 global optimal point 로 만들어준다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>KKT condition 이 optimality 의 Sufficient condition (충분조건) 임을 증명해보자.</li>
  <li>Convex optimization problem 에 대해서,</li>
</ul>

<script type="math/tex; mode=display">L(x^*,\lambda^*,\nu^*) \le L(x,\lambda^*,\nu^*) = f_0(x)+\lambda^Tf(x)+\nu^Th(x)</script>

<ul>
  <li>For any feasible <script type="math/tex">x</script> and <script type="math/tex">\lambda^*</script>, due to the complementary slackness, we must have</li>
</ul>

<script type="math/tex; mode=display">f(x^*)=L(x^*,\lambda^*,\nu^*) \le L(x,\lambda^*,\nu^*) \le f_0(x)</script>

<ul>
  <li>Hence, <script type="math/tex">f(x^*) \le f(x)</script> for all feasible <script type="math/tex">x</script> (증명 끝)</li>
</ul>

<p><strong>KKT condition 장단점</strong></p>

<ul>
  <li>장점 : Unconstrained optimization problem 을 풀때, 연립 방정식 꼴로 풀 수 있다.</li>
  <li>단점 : 수많은 condition ..  그리고 Complementary slackness condition 경우의 수가 참 많다..</li>
</ul>

<h2 id="kkt-">KKT 예제</h2>

<ul>
  <li>transmitter 과 receiver 사이에 <script type="math/tex">n</script> 개의 channel 이 있다.</li>
  <li>다음과 같은 opimization problem 을 고려해보자.</li>
</ul>

<script type="math/tex; mode=display">\underset{p_i}{\text{max}} \; \sum_{i=1}^n \frac{1}{2}\log(1+\frac{P_i}{\sigma_i^2}) \\
\text{s.t. } P_i \ge 0, \; \sum_{i}P_i \le P, \; \forall i</script>

<ul>
  <li>이때 <script type="math/tex">P_i</script>, <script type="math/tex">\sigma_i</script> 는 각각 <script type="math/tex">i</script> 번째 channel 의 power, noise 이다.</li>
  <li>즉 각 channel 에 power 을 어떻게 분배해야 가장 큰 효율을 발휘할수 있을까? 에 관한 문제이다.</li>
  <li>위 식은 concave 함수를 maximize 하는 문제이다. (안의 식이 log에 의해 concave)</li>
  <li>Slater’s condition 만족한다.
    <ul>
      <li>모든 <script type="math/tex">i</script> 에 대해 <script type="math/tex">% <![CDATA[
P_i > 0, \; \sum_{i}P_i < P %]]></script> 을 만족하는 <script type="math/tex">P_i</script> 존재</li>
    </ul>
  </li>
  <li>KKT condition 을 적용하기 위해 우선 convex optimization problem 으로 바꾸자</li>
</ul>

<script type="math/tex; mode=display">\underset{p_i}{\text{min}} \; -\sum_{i=1}^n \frac{1}{2}\log(1+\frac{P_i}{\sigma_i^2}) \\
\text{s.t. } P_i \ge 0, \; \sum_{i}P_i \le P, \; \forall i</script>

<ul>
  <li>The Lagrangian :</li>
</ul>

<script type="math/tex; mode=display">L(P,\lambda_0,\lambda)=-\sum_{i=1}^n\frac{1}{2}\log(1+\frac{P_i}{\sigma_i^2})+\lambda_0(\sum_{i=1}^n P_i-P)-\sum_{i=1}^n \lambda_i P_i</script>

<script type="math/tex; mode=display">\nabla L_{P_i}(P,\lambda_0,\lambda)=-\frac{1}{1+\frac{P_i}{\sigma_i^2}}\frac{1}{\sigma_i^2}+\lambda_0-\lambda_i</script>

<script type="math/tex; mode=display">\Leftrightarrow P_i=\frac{1}{\lambda_0-\lambda_i}-\sigma_i^2</script>

<ul>
  <li>KKT condtion 이 만족 되기 전에는 first order necessary condition</li>
  <li>convexity 와 slater’s condition 이 만족 되어진 상태인 지금,</li>
  <li>KKT condition 이 만족되면 optimality 의 necessary and sufficient condition 이 된다.
    <ul>
      <li>Feasibility : <script type="math/tex">\sum_{i=1}^n P_i \le P, \; \forall i</script></li>
      <li>Feasibility : <script type="math/tex">P_i \ge 0, \; \forall i</script></li>
      <li>Feasibility : <script type="math/tex">\lambda_0 \ge 0, \; \forall i</script></li>
      <li>Feasibility : <script type="math/tex">\lambda_i \ge 0, \; \forall i</script></li>
      <li>Complementary slackness : <script type="math/tex">\lambda_0(\sum_{i=1}^n P_i-P)=0</script></li>
      <li>Complementary slackness : <script type="math/tex">\lambda_i P_i=0</script></li>
    </ul>
  </li>
  <li>앞서 말했듯이, 모든 <script type="math/tex">i</script> 에 대해 slater’s condition 을 만족하는 <script type="math/tex">P_i</script> 가 존재한다.
    <ul>
      <li>이때 <script type="math/tex">P_i > 0</script> 을 만족하는 <script type="math/tex">P_i</script> 도 존재하고,</li>
      <li>이 경우에는 <script type="math/tex">\lambda_i=0</script></li>
    </ul>
  </li>
  <li><script type="math/tex">\log</script> 의 경우 non-decreasing 함수 이므로, <script type="math/tex">\sum_{i=1}^n P_i=P</script> (equality) 를 만족할 때 가장 효율적.. 따라서,</li>
</ul>

<script type="math/tex; mode=display">P_i=\frac{1}{\lambda_0}-\sigma_i^2</script>

<script type="math/tex; mode=display">\sum_{i=1}^n P_i = \sum_{i=1}^n \frac{1}{\lambda_0}-\sigma_i^2 = P</script>

<ul>
  <li>위의 식을 만족하는 <script type="math/tex">\lambda_0</script> 이 있을까?
    <ul>
      <li><script type="math/tex">\lambda_0 \to 0</script> 일때 <script type="math/tex">\frac{1}{\lambda_0}-\sigma_i^2 \to \infty</script></li>
      <li><script type="math/tex">\lambda_0 \to \infty</script> 일때 <script type="math/tex">\frac{1}{\lambda_0}-\sigma_i^2 \to 0</script></li>
      <li>즉, <script type="math/tex">0</script> 과 <script type="math/tex">\infty</script> 사이에 만족하는 <script type="math/tex">\lambda_0</script> 존재</li>
    </ul>
  </li>
  <li><script type="math/tex">\lambda_0</script> 이 존재 할때를 생각해보면 결국 noise (<script type="math/tex">\sigma</script>) 가 적은 channel 을 더사용 하는게 효율적. (water filling effect)</li>
</ul>

    </article>
  </div>
</div>

    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
