<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>3. An Alternative View of EM</title>
  <link rel="stylesheet" href="/kyo-ml-study/css/main.css">
  <link rel="stylesheet" href="/kyo-ml-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-ml-study/docs/chapter09/3.html">
  <script src="/kyo-ml-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-ml-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-ml-study/"><strong>kyo-ml-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <!-- for drop-down navi -->
            <li class="dropdown">
              <a href="/kyo-ml-study" class="dropdown-toggle" data-toggle="dropdown"><strong>Chapter
                  <b class="caret"></b></strong>
              </a>
              <ul class="dropdown-menu">
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter01/0">1. Introduction</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter02/0">2. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter03/0">3. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter04/0">4. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter05/0">5. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter06/0">6. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter07/0">7. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter08/0">8. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter09/0">9. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter10/0">10. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter11/0">11. Neural Networks</a>
                </li>
                
                
              </ul>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter01/0"><strong>1</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter02/0"><strong>2</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter03/0"><strong>3</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter04/0"><strong>4</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter05/0"><strong>5</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">6</strong></a></li>
                      
          
          
            <li><a href="#"><strong class="text-danger">7</strong></a></li>
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter08/0"><strong>8</strong></a>
            </li>
            
                      
          
          
            
            <li  class="active" >
              <a href="/kyo-ml-study/docs/chapter09/0"><strong>9</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter10/0"><strong>10</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">11</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 9</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/0">0. Mixture Models and EM</a>
      </li>
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/1">1. K-means Clustering</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/2">2. Mixture of Gaussians</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-ml-study/docs/chapter09/3">3. An Alternative View of EM</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/4">4. The EM Algorithm in General</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-ml-study/blob/gh-pages/docs/chapter09/3.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>3. An Alternative View of EM</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>이번 절에서는 <em>EM</em> 알고리즘에서 사용되는 잠재 변수(latent variable)의 핵심적인 역할에 대해 살펴볼 것이다.
    <ul>
      <li>잠시 기억을 떠올려보면 입력 샘플 \( {\bf x}_n^T \) 에 대응되는 \( {\bf z}_n^T \) 가 존재하는 것으로 처리하였다.</li>
    </ul>
  </li>
  <li><em>EM</em> 알고리즘의 최종 목적은 데이터 \( {\bf X} \) 가 주어졌을 때 잠재 변수를 통해 <em>MLE</em> 를 구하는 것이다.
    <ul>
      <li>물론 잠재 변수 \( {\bf Z} \) 에 대해 사전에 알려져있는 정보는 없다.</li>
      <li>이후 잠재 변수와 입력 변수에 대한 조건부 분포 형태로 문제를 풀게 된다.</li>
    </ul>
  </li>
  <li><em>EM</em> 알고리즘에 대해 잠시 생각해보는 시간을 갖도록 하자.
    <ul>
      <li>우리는 지금까지 <em>SGD</em> (Stocastic gradient descent) 등의 함수 최적화 방식을 배웠었다.</li>
      <li>그럼에도 불구하고 <em>EM</em> 이라는 방식을 사용하는 이유는 뭘까?
        <ul>
          <li>현실적으로는 <em>SGD</em> 연산보다 <em>EM</em> 방식이 이해하기도 쉽고 구현하기도 쉽다. 그래서 많이 쓴다.</li>
          <li>물론 성능이 더 좋다고는 말할 수 없음.</li>
        </ul>
      </li>
      <li>그럼 다른 방법 대신 무조건 <em>EM</em> 을 쓰면 되는것 아닌가?</li>
      <li>모든 경우에 <em>EM</em> 을 적용할 수 있는 것은 아니고 일정한 조건이 필요함
        <ul>
          <li>조건에 부합되는 모델에서는 당연히 그냥 사용하면 되고,</li>
          <li>조건에 부합되지 않는 경우에는 모델 자체를 해당 조건으로 변환을 하여 조건을 만족하게 한 뒤에 식을 추스려야지만 <em>EM</em> 을 사용할 수 있다.</li>
        </ul>
      </li>
      <li>그럼 <em>EM</em> 을 사용할 수 있는 조건이란?
        <ul>
          <li>일단 잠재 변수가 존재해야 한다. (<em>EM</em> 자체가 잠재 변수가 있는 경우를 해결하기 위한 모델이다.)</li>
          <li>잠재 변수가 관찰되지 않았을 경우에는 모델의 파라미터 추정이 불가능한 상태를 가진다.</li>
          <li>하지만 잠재 변수가 관찰된 후에는 모델의 파라미터 추정이 가능해진다.</li>
          <li>혼합 분포가 <em>EM</em> 을 적용하기 좋은 예인데,
            <ul>
              <li>잠재 변수의 역할이 혼합 분포 내의 한 분포를 선택하는 것을 나타내는 랜덤 변수인 경우 쉽게 적용 가능하다.</li>
              <li>이것은 지난 절에서 살펴본 내용이다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>잠재 변수가 주어진 모델의 <em>MLE</em> 를 이해하여 보자.</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}|{\bf \theta}) = \ln \left\{ \sum_{\bf Z} p({\bf X}, {\bf Z}|{\bf \theta}) \right\} \qquad{(9.29)}</script>

<ul>
  <li>간단한 합의 공식(sum rule)을 이용한 식이다.
    <ul>
      <li>위의 식은 합의 공식으로 표현되어 있지만 \( {\bf Z} \) 가 연속 변수인 경우 적분으로 표현할 수 있다.</li>
    </ul>
  </li>
  <li>여기서 중요한 점은,
    <ul>
      <li>잠재 변수가 로그(log) 함수 안에서 합으로 표현되고 있다는 것이다.
        <ul>
          <li>결합 확률 분포 \( p({\bf X}, {\bf Z} | {\bf \theta}) \) 가 지수족 분포를 따른다고 할지라도,
            <ul>
              <li>이에 대한 주변 확률 분포 \( p({\bf X} | {\bf \theta}) \) 는 지수족 분포가 아닐 수 있다.</li>
              <li>가우시안 함수를 합한 식에 로그를 취한 함수는 간단한 이차 형식(<code class="highlighter-rouge">quadratic</code>)의 함수가 <strong>아니다</strong>.</li>
            </ul>
          </li>
          <li>이는 결국 복잡한 형태의 <em>MLE</em> 풀이를 가지게 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>앞서서 <em>complete</em> 와 <em>incomplete</em> 의 개념을 배웠었다.
    <ul>
      <li>Complete Data : \( { {\bf X}, {\bf Z} } \)
        <ul>
          <li>모든 관찰 데이터에 대해 잠재 변수 \( {\bf Z} \) 가 주어진 모델</li>
          <li>이 때의 로그 가능도 함수는 \( \ln p({\bf X}, {\bf Z} | {\bf \theta}) \) 가 된다.</li>
          <li>각각에 대해 가능도 함수를 구하게 되므로 어렵지 않다.</li>
        </ul>
      </li>
      <li>Incomplete Data : \( { {\bf X}} \)
        <ul>
          <li>모든 관찰 데이터에 대해서만 주어지고 잠재 변수는 주어지지 않는다.</li>
          <li>결국 가능한 모든 잠재 변수의 값에 대한 분포를 고려해야 한다.</li>
          <li>로그 가능도 함수는 \( \ln p({\bf X} | {\bf \theta}) =  \ln { \sum_Z p( {\bf X}, {\bf Z} | {\bf \theta} ) } \) 로 주어진다.</li>
          <li>로그 함수 내에 합(sum) 에 대한 수식이 존재하기 때문에 최대화 시키는 지점을 찾기가 어렵다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>이제 <em>EM</em> 을 이용하여 로그 가능도 함수를 계산하는 방법을 살펴보도록 하자.
    <ul>
      <li>앞서 이야기한대로 로그 가능도 함수가 계산 가능한 경우는 오로지 잠재 변수 \( {\bf Z} \) 가 관찰된 경우 밖에 없다.</li>
      <li>하지만 일반적으로 이러한 잠재 변수의 실제 값은 얻을 수 없다. (즉, 오로지 얻을 수 있는 데이터는 <em>incomplete</em>  데이터)</li>
      <li>따라서 이 경우 MLE를 풀기 위해서는 샘플을 통해 이 잠재 변수를 추정하여 값이 제공된다고 생각하는 수 밖에 없다.</li>
      <li>그래서 Expectation of Log-likelihood 가 등장한다.</li>
      <li>별거 없고, 로그 가능도 함수에서 잠재 변수의 값을 정확히 알기 어려우므로 그냥 평균(기대) 값을 식으로 표현한 것.
        <ul>
          <li>즉, 가능도 함수에 \( E[\ln p({\bf X}, {\bf Z} | {\bf \theta})] \) 를 사용하게 된다.</li>
          <li>물론 가능도 함수 대신 이 수식을 사용하는 것이 타당한지에 대한 의문이 들 수 있는데, 왜 가능한지는 다음 절에 나온다.</li>
          <li>여기서는 아무런 설명 없이 바로 그냥 식이 제공되기 때문에 의아해할 수도 있지만 우선은 그런가보다 하고 진행하도록 하자.</li>
        </ul>
      </li>
      <li>예전에도 이와 비슷한 상황을 좀 보긴 했는데 손실 함수를 최소화하는 방식도 이와 유사한 방식이다.
        <ul>
          <li>물론 기억은 잘 나지 않을 것이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이렇게 정의한 가능도 함수의 기대값을 좀 전개해보자.</li>
</ul>

<script type="math/tex; mode=display">E_Z[\ln p({\bf X}, {\bf Z}|\theta)] = \sum_{\bf Z} p({\bf Z}|{\bf X}, {\bf \theta})\ln p({\bf X}, {\bf Z}|{\bf \theta})</script>

<ul>
  <li>
    <p>참고로 함수에 대한 기대값을 구하는 식은 다음과 같다.
<script type="math/tex">E[f]=\sum_x p(x)f(x)</script></p>
  </li>
  <li>이제 기대값을 최대로 만드는 파라미터 값을 추론할 것이다.</li>
  <li>이 때 <em>EM</em> 을 사용하게 되는데 앞서 살펴본 방식과 동일한 형태로 위의 값을 최대화하는 과정으로 만들어 낼 수 있기 때문</li>
</ul>

<p><em>E-Step</em></p>

<ul>
  <li>우선 주어진 파라미터는 고정되어 있다고 하고 이를 \( {\bf \theta}^{old} \) 라고 하자.</li>
  <li>여기서 잠재 변수 \( {\bf Z} \) 의 확률 값 \( p({\bf Z}|{\bf X}, {\bf \theta}^{old}) \) 를 얻어야 한다.</li>
  <li>이렇게 얻어진 확률 값을 이용하여 <em>complete</em>-data 가능도 함수의 값을 구할 것이다.
    <ul>
      <li>즉, 가능한 \( {\bf Z} \) 의 확률 값을 추론한 뒤 이를 이용하여 각각의 파라미터 값을 추론하는 MLE 과정을 진행할 것이다.</li>
    </ul>
  </li>
  <li>잠재 변수의 기대값이 예측되었을 때, 이제 일반 파라미터 \( \theta \) 를 가지고 <em>complete</em>-data 가능도 함수의 기대값을 서술해보자.</li>
</ul>

<script type="math/tex; mode=display">Q({\bf \theta}, {\bf \theta}^{old}) = \sum_{\bf Z} p({\bf Z}|{\bf X}, {\bf \theta}^{old}) \ln p({\bf X}, {\bf Z}|{\bf \theta}) \qquad{(9.30)}</script>

<ul>
  <li>수식이 크게 차이가 있는 것은 아니지만, \( {\bf Z} \) 를 추론할 때에는 사전에 결정된 파라미터를 사용한다는 것을 알 수 있다.</li>
  <li>
    <p>이후에는 그냥 \( \theta \) 에 대한 함수가 된다.</p>
  </li>
  <li>따라서 이 함수 값을 최대화하는 방법은 이제 별로 어렵지 않다.</li>
</ul>

<p><em>M-Step</em></p>

<ul>
  <li>여기서는 예측된 \( {\bf Z} \) 의 기대값을 이용해서 이를 최대화하는 파라미터를 구하는 단계가 된다.</li>
</ul>

<script type="math/tex; mode=display">{\bf \theta}^{new} = \arg\max_{\theta} Q({\bf \theta}, {\bf \theta}^{old}) \qquad{(9.31)}</script>

<ul>
  <li>사실 기존에 정리한 EM 방식과 동일하다. 정리해보자.</li>
</ul>

<hr />

<p><strong>EM 알고리즘 일반화</strong></p>

<ul>
  <li>관찰데이터 \( {\bf X} \) 와 잠재 변수 \( {\bf Z} \) 가 파라미터 \( {\bf \theta} \) 에 의해 주어졌을 때,</li>
  <li>결합 분포는 \( p({\bf X}, {\bf Z}|{\bf \theta}) \) 와 같이 표현 가능하다.</li>
  <li>
    <p>이 때 \( p({\bf X}|{\bf \theta}) \) 값을 가장 크게 만드는 파라미터 \( {\bf \theta} \) 값을 얻고 싶다. (MLE를 이용)</p>

    <ul>
      <li><strong>Init Step</strong> : (임의의) 파라미터 \( {\bf \theta}^{old} \) 의 값을 설정한다.</li>
      <li><strong>E-Step</strong> : \( p({\bf Z}|{\bf X}, {\bf \theta}^{old}) \) 값을 추론한다.</li>
      <li><strong>M-Step</strong> : \( {\bf \theta}^{new} \) 값을 추론한다. 이 때,
        <ul>
          <li>\( {\bf \theta}^{new} = \arg\max_{\bf \theta} Q({\bf \theta}, {\bf \theta}^{old}) \qquad{(9.32)} \)</li>
          <li>\( Q({\bf \theta}, {\bf \theta}^{old}) = \sum_{\bf Z}p({\bf Z}|{\bf X}, {\bf \theta}^{old})\ln p({\bf X}, {\bf Z}|{\bf \theta}) \qquad{(9.33)} \)</li>
        </ul>
      </li>
      <li>새롭게 구해진 파라미터의 값들이 수렴 상태인지 확인한다.
        <ul>
          <li>수렴되지 않았다면 아래 작업 후 Step-2 로 돌아간다.</li>
          <li>수렴되었을 경우 종료한다.
            <ul>
              <li>\( {\bf \theta}^{old} \leftarrow {\bf \theta}^{new} \qquad{(9.34)} \)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>참고로 위의 수식에서도 \( Singularity \) 문제는 여전히 발생할 수 있는데, 다음 식을 도입하여 이를 제거할 수 있다. (MAP 방식)</li>
</ul>

<script type="math/tex; mode=display">Q({\bf \theta}, {\bf \theta}^{old})+\ln p({\bf \theta})</script>

<p><strong>Missing Variables</strong></p>

<ul>
  <li>지금까지 이산 잠재 변수가 존재할 때의 <em>MLE</em> 처리를 수행하는 <em>EM</em> 알고리즘을 살펴보았음.</li>
  <li>사실 이 방식은 입력 데이터에서 관찰되지 못한 데이터(un-observed variables)가 존재하는 경우에도 적용 가능한 방법임
    <ul>
      <li>가능한 모든 변수에 대한 결합 분포를 만든 다음 주변화 작업을 통해 해당 변수를 남기게 하면 된다.</li>
    </ul>
  </li>
</ul>

<h2 id="gaussian-mixtures-revisited">9.3.1. 가우시안 혼합분포 다시 보기 (Gaussian mixtures revisited)</h2>

<ul>
  <li>잠재 변수를 이용하여 <em>GMM</em> 을 다시 해석해보도록 한다.
    <ul>
      <li>물론 자세한 내용은 이미 앞절에서 다루었으므로 여기서는 일반화 식으로의 전개만을 다루도록 한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/kyo-ml-study/images/Figure9.9.png" alt="figure9.9" class="center-block" height="120px" /></p>

<ul>
  <li>위의 그림은 <em>complete-data</em> 에 대해 <em>GMM</em> 모델을 표기한 것이다.
    <ul>
      <li>잠재 변수도 관찰되었으므로 음영으로 처리되어 있다.</li>
    </ul>
  </li>
  <li>이제 <em>complete</em> 데이터에 대한 가능도 함수와 로그 가능도 함수를 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf X}, {\bf Z}|{\bf \mu}, \Sigma, {\bf \pi}) = \prod_{n=1}^N\prod_{k=1}^K \pi_k^{z_{nk}} N({\bf x}_n|{\bf \mu}_k, \Sigma_k)^{z_{nk}} \qquad{(9.35)}</script>

<script type="math/tex; mode=display">\ln p({\bf X}, {\bf Z}|{\bf \mu}, \Sigma, {\bf \pi}) = \sum_{n=1}^N\sum_{k=1}^K z_{nk}\{\ln\pi_k + \ln N({\bf x}_n|{\bf \mu}_k, \Sigma_k)\} \qquad{(9.36)}</script>

<ul>
  <li>이 식은 <em>incomplete</em> 데이터에 대한 로그 가능도 함수보다 쉽게 계산되는 꼴이다.
    <ul>
      <li><em>incomplete</em> 와 다르게 \( \ln(\cdot) \) 함수가 가우시안 분포에 바로 적용 가능함.</li>
      <li>이것으로부터 파라미터의 MLE 값은 쉽게 계산 가능한데, 각각의 파라미터에 대해 <code class="highlighter-rouge">convex</code> 형태의 식이기 때문이다.</li>
      <li>물론 \( {\bf z}_k \) 은 \( {\bf z}_k \in {0,1} \) 이므로 식 전개에 영향을 주지 못하고, 하나의 가우시안 함수에 대해 처리하는 것처럼 동작하게 된다.</li>
      <li>따라서 각각의 \( k \) 에 대해 MLE 를 구하게 되면 다음과 같은 결과를 얻을 수 있음을 알 수 있다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\pi_k = \frac{1}{N}\sum_{n=1}^N z_{nk} \qquad{(9.37)}</script>

<ul>
  <li>이제 <em>incomlete</em> 데이터를 살펴보도록 하자. 이에 대한 로그 가능도 함수는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}|{\bf \mu}, \Sigma, {\bf \pi}) = \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)\right\}</script>

<ul>
  <li>앞서 살펴본 바와 같이 <em>complete</em> 데이터는 는 닫힌 형태(closed form)의 식이므로 MLE 처리가 가능.</li>
  <li>하지만 <em>incomplete</em> 데이터는 로그 식 내에 합의 식이 존재하므로 이게 안된다.</li>
  <li>
    <p>따라서 <em>EM</em> 에서 보았던 로그 가능 함수의 평균 값을 정의하여 최대화하는 방식으로 전개해야 한다.</p>
  </li>
  <li>기존에 살펴보았듯이 결국 잠재 변수의 사후 분포를 통해 \( {\bf Z} \) 를 먼저 추론해야 하는 것.</li>
  <li>우선 \( {\bf Z} \) 에 대한 사후 분포를 구하는 식을 기술해 보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf Z}|{\bf X}, {\bf \mu}, \Sigma, {\bf \pi}) \propto \prod_{n=1}^{N}\prod_{k=1}^{K}[\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)]^{z_{nk}} \qquad{(9.38)}</script>

<ul>
  <li>물론 위의 식은 다음 식으로 부터 얻어진 것이다.
    <ul>
      <li>\( p({\bf z})=\prod_{k=1}^{K} \pi^{z_k} \)</li>
      <li>\( p({\bf x}|{\bf z}) = \prod_{k=1}^{K} N({\bf x}|{\bf \mu}_k, \Sigma_k)^{z_k} \)</li>
    </ul>
  </li>
  <li>우선 결론부터 보고 나서 식을 확인하도록 하자.</li>
  <li>\( E \) 단계에서 필요한 \( {\bf Z} \) 의 사후 확률 분포를 구하고 나면 로그 가능도 함수의 평균 값을 고려해야 한다.</li>
</ul>

<script type="math/tex; mode=display">E_{\bf Z}[\ln p({\bf X}, {\bf Z}|{\bf \mu}, \Sigma, {\bf \pi})] = \sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk})\{\ln\pi_k+\ln N({\bf x}_n|{\bf \mu_k}, \Sigma_k)\}</script>

<ul>
  <li>위의 식은 앞서 <em>complete</em> 데이터라고 가정한 뒤 얻어진 가능도 함수의 식과 거의 동일하다.
    <ul>
      <li>단지 \( z_{nk} \) 식이 \( \gamma(z_{nk}) \) 로 변경되었을 뿐이다.</li>
    </ul>
  </li>
  <li>
    <p>교재에는 식이 어떻게 전개되었는지 설명이 없다.</p>
  </li>
  <li>식 전개를 먼저 좀 보자. 우선 수식 전개에 필요한 식 몇개를 기술한 다음 실제 식을 전개한다.</li>
</ul>

<script type="math/tex; mode=display">E[X+Y] = E[X]+E[Y]</script>

<script type="math/tex; mode=display">p({\bf x}_n|{\bf z}_n) = \prod_{k=1}^K N({\bf x}_n|{\bf \mu}_k, \Sigma_k)^{z_nk}</script>

<ul>
  <li>이제 식을 전개한다.</li>
</ul>

<script type="math/tex; mode=display">E_{\bf Z}[\ln p({\bf X}, {\bf Z}|\theta)] = E_{\bf Z}[\sum_n\ln p({\bf x}_n, {\bf z}_n|\theta)] = \sum_n E_{\bf Z}[\ln p({\bf x}_n, {\bf z}_n|\theta)]\\\\
= \sum_{n=1}^N E_{\bf Z}[\ln (p({\bf z}_n) p({\bf x}_n|{\bf z}_n, \theta))] = \sum_{n=1}^N E_{\bf Z}[\ln[\prod_{k=1}^K (\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k))^{z_{nk}}]]\\\\
=\sum_{n=1}^N \sum_{k=1}^K E_{z}[(z_{nk})\ln(\pi_k N({\bf x}|{\bf \mu}_k, \Sigma_k))] = \sum_{n=1}^N \sum_{k=1}^K E_{z}[z_{nk}] \ln(\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k))\\\\
=\sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk})\{\ln\pi_k+\ln N({\bf x}_n|{\bf \mu_k}, \Sigma_k)\}</script>

<ul>
  <li>\( E[z_{nk}]=\gamma(z_{nk}) \) 는 다음으로 확인 가능하다.</li>
</ul>

<script type="math/tex; mode=display">E[z_{nk}] = \frac{\sum_{ {\bf z}_n} z_{nk}\prod_{k'}[\pi_{k'} N({\bf x}_n|{\bf \mu}_{k'}, \Sigma_{k'})]^{z_{nk'}}}{\sum_{ {\bf z}_n} \prod_j [\pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)]^{z_{nj}}}\\\\
= \frac{\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)}{\sum_{j=1}^K \pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)} = \gamma(z_{nk})</script>

<ul>
  <li>이제 얻어진 식으로부터 MLE 를 수행해서 파라미터를 얻는 식을 만들어낸다.
    <ul>
      <li>뭐, 당연하게도 앞에서 구한 결과와 동일한 식을 얻게 된다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_k = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{nk}){\bf x}_n</script>

<script type="math/tex; mode=display">\Sigma_k = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk})({\bf x}_n-{\bf \mu}_k)({\bf x}_n-{\bf \mu}_k)^T</script>

<script type="math/tex; mode=display">\pi_k = \frac{N_k}{N}</script>

<h2 id="k-means---relation-to-k-means">9.3.2. <em>K</em>-means 와의 관계 (Relation to <em>K</em>-means)</h2>

<ul>
  <li>가우시안 분포를 위한 <em>EM</em> 알고리즘과 <em>K</em>-means 알고리즘은 매우 유사하다.</li>
  <li>앞서 살펴보았듯이 <em>K</em>-means 는 경성(hard) 클러스터 방식
    <ul>
      <li>하나의 샘플이 오로지 하나의 클러스터에만 속하는 구조</li>
    </ul>
  </li>
  <li><em>EM</em> 은 사후 분포에 대해서 연성(soft) 클러스터 방식을 제공하였다.
    <ul>
      <li>즉, 각각의 클러스터에 대해 샘플이 속할 확률 값을 표현</li>
    </ul>
  </li>
  <li>또한 <em>K</em>-means 는 각 클러스터에 대해 분산도를 고려하지 않았다.
    <ul>
      <li>오로지 평균과의 거리로만 고려된다.</li>
      <li>유클리디안 거리 측정 방식이기 때문</li>
    </ul>
  </li>
  <li>사실 <em>K</em>-means 알고리즘은 \( GMM \) 알고리즘의 특별한 경우를 나타낸다.
    <ul>
      <li>만약 각각의 가우시안 컴포넌트의 분산 값이 서로 동일하고 독립적이라고 하자. ( \( e{\bf I} \) )</li>
      <li>이 경우 \( {\bf x} \) 에 대한 확률 분포는 다음과 같게 된다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf \mu}_k, \Sigma_k) = \frac{1}{(2\pi e)^{M/2}}\exp\left\{-\frac{1}{2e}\|{\bf x}-{\bf \mu}_k\|^2\right\}</script>

<ul>
  <li>여기서 \( K \) 개의 가우시안 분포를 고려한다면,</li>
  <li>responsibility 함수는 다음과 같게 된다.</li>
</ul>

<script type="math/tex; mode=display">\gamma(z_{nk}) = \frac{\pi_k \exp\{-\|{\bf x}_n-{\bf \mu}_k\|^2/2e\}}{\sum_j \pi_j \exp\{-\|{\bf x}_n-{\bf \mu}_j\|^2/2e\}}</script>

<ul>
  <li>이제 분산 \( e\rightarrow0 \) 이라고 하자.
    <ul>
      <li>결국 평균만 의미가 있고, 분산 값은 의미가 없어지게 된다.</li>
    </ul>
  </li>
  <li>
    <p>responsibility 함수는 결국 binary indicator 가 된다.</p>
  </li>
  <li>이제 로그 가능도 함수의 기대값을 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">E_{\bf Z}[\ln p({\bf X}, {\bf Z}|{\bf \mu}, \Sigma, {\bf \pi})] \rightarrow -\frac{1}{2}\sum_{n=1}^N\sum_{k=1}^K r_{nk}\|{\bf x}_n-{\bf \mu}_k\|^2+const</script>

<ul>
  <li>이 식은 기존의 <em>K</em>-means 에서 정의한 <em>distortion measure</em> (식 9-1)와 계수를 빼고 동일하다.</li>
</ul>

<h2 id="mixtures-of-bernoulli-distributions">9.3.3. 베르누의 혼합 분포 (Mixtures of Bernoulli distributions)</h2>

<ul>
  <li>지금까지 살펴본 분포에서는 \( {\bf x} \) 가 모두 연속 변수였다.</li>
  <li>이제 이산 분포인 베르누이 분포에 대한 혼합 분포를 살펴보도록 하자.</li>
  <li>
    <p>사실 이게 \( HMM \) 알고리즘을 배우는데 초석이 되는 부분이다.</p>
  </li>
  <li>이제 \( D \) 차원의 이항 변수가 있다고 하자. ( \( i=1,…,D \) )</li>
  <li>이제 이를 베르누이 분포로 나타내면 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf \mu}) = \prod_{i=1}^D \mu_i^{x_i}(1-\mu_i)^{(1-x_i)}</script>

<ul>
  <li>
    <p>단, \( {\bf x}=(x_1,…,x_D)^T \) 이고 \( {\bf \mu} = (\mu_1,…,\mu_D)^T \) 이다.</p>
  </li>
  <li>
    <p>이 때의 평균과 공분산을 살펴보자.</p>
  </li>
</ul>

<script type="math/tex; mode=display">E[{\bf x}] = {\bf \mu}</script>

<script type="math/tex; mode=display">cov[{\bf x}] = diag(\mu_i(1-\mu_i))</script>

<ul>
  <li>이제 여기서 \( K \) 개의 베르누이 분포가 사용된다고 생각해보자.</li>
  <li>이러면 혼한 분포는 다음과 같은 형태가 될 것이다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf \mu}, {\bf \pi}) = \sum_{k=1}^K \pi_k p({\bf x}|{\bf \mu}_k)</script>

<ul>
  <li>이 때 \( {\bf \mu} = ({\bf \mu}_1,…,{\bf \mu}_K) \) 이고 \( {\bf \pi} = (\pi_1,…,\pi_K) \) 이다.</li>
  <li>물론 각 \( k \) 에 대한 \( {\bf x} \) 에 대한 분포도 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf \mu}_k) = \prod_{i=1}^D \mu_{ki}^{x_i}(1-{\bf \mu}_{ki})^{(1-x_i)}</script>

<ul>
  <li>이 때의 평균과 분산은 다음과 같아진다.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf x}] = \sum_{k=1}^K \pi_k{\bf \mu}_k</script>

<script type="math/tex; mode=display">cov[{\bf x}] = \sum_{k=1}^K \pi_k \{\Sigma_k+{\bf \mu}_k{\bf \mu}_k^T\}-E[{\bf x}]E[{\bf x}]^T</script>

<script type="math/tex; mode=display">\Sigma_k = diag\{\mu_{ki}(1-\mu_{ki})\}</script>

<ul>
  <li>단일 베르누이 분포에서는 분산이 \( diag \) 였지만 혼합 분포는 더 이상 \( diag \) 가 아니다.</li>
  <li>계속해서 로그 가능도 함수를 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}|{\bf \mu}, {\bf \pi}) = \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k p({\bf x}_n|{\bf \mu}_k)\right\}</script>

<ul>
  <li>
    <p>역시나 로그 안에 합(sum)에 관한 식이 들어있다. 이런 경우 닫힌 형태(closed form)가 아님을 이미 확인했다.</p>
  </li>
  <li>
    <p>따라서 이 경우에도 <em>EM</em> 알고리즘을 도입해야 한다.</p>
  </li>
</ul>

<hr />

<ul>
  <li>마찬가지로 이제 잠재 변수 \( {\bf z} \) 를 도입해야 한다.
    <ul>
      <li>가우시안 혼합 분포에서 사용한 것과 마찬가지로 \( K \) 개의 상태를 나타내는 1-to-K 형태의 이산 변수이다.</li>
      <li>\( {\bf z}=({\bf z}_1,…,{\bf z}_K)^T \)</li>
    </ul>
  </li>
  <li>잠재 변수가 주어졌을 때의 조건부 분포는 간단하게 기술할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf z}, {\bf \mu})=\prod_{k=1}^K p({\bf x}|{\bf \mu}_k)^{z_k}</script>

<ul>
  <li>잠재 변수의 사전 확률 분포는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf z}|{\bf \pi}) = \prod_{k=1}^K \pi_k^{z_k}</script>

<ul>
  <li>이제 <em>EM</em> 알고리즘을 사용하기 위해 로그 가능도 함수 식을 만들어야 한다.</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}, {\bf Z}|{\bf \mu}, {\bf \pi})=\sum_{n=1}^N\sum_{k=1}^K z_{nk}\left\{ \ln\pi_k + \sum_{i=1}^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})]\right\}</script>

<ul>
  <li>이것에 대한 기대값을 계산한다.</li>
</ul>

<script type="math/tex; mode=display">E_{\bf Z}[\ln p({\bf X}, {\bf Z}|{\bf \mu}, {\bf \pi})] = \sum_{n=1}^N\sum_{k=1}^K \gamma(z_{nk})\left\{\ln\pi_k + \sum_{i=1}^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})]\right\}</script>

<ul>
  <li>\( \gamma(z_{nk})=E[z_{nk}] \) 이다. E-Step 에서 계산된다.</li>
</ul>

<script type="math/tex; mode=display">\gamma(z_{nk})=E[z_{nk}] = \frac{\sum_{ {\bf z}_n} z_{nk}\prod_{k'}[\pi_{k'} p({\bf x}_n|{\bf \mu}_{k'})]^{z_{nk'}}}{\sum_{ {\bf z}_n} \prod_j [\pi_{j} p({\bf x}_n|{\bf \mu}_j)]^{z_{nj}}} = \frac{\pi_k p({\bf x}_n|{\bf \mu}_k)}{\sum_{j=1}^K \pi_j p({\bf x}_n|{\bf \mu}_j)}</script>

<ul>
  <li>로그 가능도 함수의 기대값 수식 내에서 responsibility 함수가 사용되는 영역은 다음밖에 없다.</li>
</ul>

<script type="math/tex; mode=display">N_k = \sum_{n=1}^N \gamma(z_{nk})</script>

<script type="math/tex; mode=display">\bar{\bf x}_k = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk}){\bf x}_n</script>

<ul>
  <li>M-Step 에서는 파라미터에 대한 추정을 한다.
    <ul>
      <li>증명은 생략한다. 간단하게 기술해보자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_k = \bar{\bf x}_k</script>

<script type="math/tex; mode=display">\pi_k = \frac{N_k}{N}</script>

<ul>
  <li>교재에는 나오지 않지만 다항 분포(multinomial distribution)에 대해서도 동일한 방식으로 식 전개가 가능하다.</li>
</ul>

<hr />

<p><strong>베르누이 혼합 분포의 예제</strong></p>

<p><img src="/kyo-ml-study/images/Figure9.10a.png" alt="figure9.10a" class="center-block" height="50px" /></p>

<ul>
  <li>위의 예제는 숫자 인식을 위한 데이터로 5개의 데이터가 주어졌다.</li>
  <li>쉽게 알 수 있듯 2,3,4 와 같은 값을 결정해야 한다.</li>
  <li><em>EM</em> 알고리즘을 이용하여 이를 분류해보자. \( K=3 \) 을 가정하고 전개하자.</li>
  <li>총 600개의 데이터를 이용한다. (물론 숫자는 2,3,4 뿐이다.)</li>
  <li>초기 \( \pi_k=1/K \) 로 시작한다.</li>
  <li>이미지 벡터는 0과 1로 값으로 gray-scale 을 표현한다.
    <ul>
      <li>이 둘을 구분짓는 확률 값은 0.5로 정의한다.</li>
    </ul>
  </li>
  <li><em>EM</em> 을 수행하기 위해서는 해당 필드에 대한 확률의 기대(평균) 값이 필요하다.
    <ul>
      <li>(0.25, 0.75) 사이의 범위에서 랜덤하게 하나를 선택하여 시작하도록 하자.</li>
    </ul>
  </li>
  <li>공액 사전 분포로는 베타 분포(beta)를 사용하였다.</li>
</ul>

<p><img src="/kyo-ml-study/images/Figure9.10b.png" alt="figure9.10b" class="center-block" height="50px" /></p>

<ul>
  <li>위의 그림은 3개의 클러스터로 나누어진 후에 각각의 평균값으로 그림을 표현한 것이다.
    <ul>
      <li>대충 들어맞는 느낌이 있다.</li>
    </ul>
  </li>
</ul>

<p><img src="/kyo-ml-study/images/Figure9.10c.png" alt="figure9.10c" class="center-block" height="50px" /></p>

<ul>
  <li>위의 경우는 \( K=1 \) 인 경우이다.
    <ul>
      <li>이 때에는 전체 샘플에 대한 평균 값으로 처리되게 된다.</li>
      <li>따라서 위의 이미지는 샘플 이미지의 평균 이미지</li>
    </ul>
  </li>
</ul>

<h2 id="em-em-for-bayesian-linear-regression">9.3.4. 베이지안 선형 회귀 EM (EM for Bayesian linear regression)</h2>

<ul>
  <li>이번 절은 꽤나 독특한데 앞서 살펴보았던 GMM 처럼 혼합 모델이 아닌 모델에 대한 EM 적용을 다루고 있다.
    <ul>
      <li>결국 이러한 기법이 결국 다음 절의 EM 알고리즘의 일반화 과정까지 이어지게 된다.</li>
    </ul>
  </li>
  <li>마지막 EM 에제로 앞장에서 다루었던 베이지안 선형 회귀(Bayesian linear regression) 를 다루어보자.
    <ul>
      <li>생각이 나지 않는다면 3.5.2 절을 참고하도록 하자.</li>
    </ul>
  </li>
  <li>여기에는 하이퍼-파라미터(초모수) \( \alpha \) 와 \( \beta \) 가 사용된다.
    <ul>
      <li>우리는 EM 알고리즘을 이용하여 이 초모수를 예측하게 될 것이다.</li>
    </ul>
  </li>
  <li>잠시 기억을 떠올려보자.
    <ul>
      <li>보통 초모수의 값은 사용자가 임의대로 지정하게 되는데,</li>
      <li>주어진 샘플을 이용하여 가장 적절한 초모수 값을 선택하는 방법도 가능하다.
        <ul>
          <li>즉, 예측 모델(predictive model)의 형태로, 주어진 데이터를 통해 초모수를 지정하고 새로운 데이터를 추정하는 문제.</li>
          <li>3.5.2절이 이와 관련된 내용이었다.</li>
        </ul>
      </li>
      <li>이를 위해 evidence 함수 \( p({\bf t}|\alpha, \beta) \) 를 최대화하는 \( \alpha \) 와 \( \beta \) 를 수식으로 찾아내는 과정이 있었다.</li>
      <li>이 때 실제 파라미터 \( {\bf w} \) 가 없는 이유는 주변화(marginalize) 되었기 때문.</li>
      <li>이거 어디서 많이 보던 스타일이었다. (즉, EM 알고리즘과 동일한 형태이다.)</li>
      <li>이제부터 최적의 \( \alpha \) 와 \( \beta \) 를 구하기 위한 EM 기법을 살펴보도록 하자.
        <ul>
          <li>이제 우리가 모수로 사용했던 \( {\bf w} \) 가 잠재 변수 \( {\bf Z} \) 역할을 수행하고 초모수 \( \alpha \) , \( \beta \) 가 파라미터로 사용된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>E-Step 에서는 \( {\bf w} \) 에 대한 사후 분포를 구하게 되고, (이 때 \( \alpha \) 와 \( \beta \) 는 고정)</li>
  <li>
    <p>M-Step에서는 로그 가능도 함수의 기대값을 최대화하는 \( \alpha \) 와 \( \beta \) 값을 구하게 된다.</p>
  </li>
  <li>이제 가장 먼저 로그 가능도 함수를 설계해야 한다. ( \( E[\ln p({\bf t}, {\bf w}| \alpha, \beta)] \) )</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf t}, {\bf w}| \alpha, \beta) = \ln p({\bf t}|{\bf w}, \beta)+\ln p({\bf w}|\alpha)</script>

<ul>
  <li>이 때 \( p({\bf t}|{\bf w}, \beta) \) 와 \( p({\bf w}|\alpha) \) 는 (식3.10)과 (식3.52) 에 기술되어 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf t}|{\bf X}, {\bf w}, \beta) = \prod_{n=1}^N N(t_n|{\bf w}^T\phi({\bf x}_n), \beta^{-1})</script>

<script type="math/tex; mode=display">p({\bf w}|\alpha) = N({\bf w}|0, \alpha^{-1}{\bf I})</script>

<ul>
  <li>이제 최종적으로 로그 가능도 함수의 기대값을 기술하자.</li>
</ul>

<script type="math/tex; mode=display">E[\ln p({\bf t}, {\bf w}|\alpha, \beta)] = \frac{M}{2}\ln\left(\frac{\alpha}{2\pi}\right) - \frac{\alpha}{2}E[{\bf w}^T{\bf w}]+\frac{N}{2}\ln\left(\frac{\beta}{2\pi}\right) - \frac{\beta}{2}\sum_{n=1}^N E[(t_n-{\bf w}^T\phi_n)^2]</script>

<ul>
  <li>이를 미분하여 식을 만들면 된다. M-Step 에서 초모수의 값을 보정하게 된다.</li>
</ul>

<script type="math/tex; mode=display">\alpha = \frac{M}{E[{\bf w}^T{\bf w}]}=\frac{M}{m_N^T m_N+Tr(S_N)}</script>

<ul>
  <li>이건 3장에서 얻어진 식과는 조금 다르다.
    <ul>
      <li>하지만 이 식도 결국 \( M\times M \) 행렬의 역행렬 연산과 관련이 있기 때문에 반복에 따른 비용은 비슷하다.</li>
    </ul>
  </li>
  <li>수식을 편하게 전개하기 위해 3.5.2절에서 했던 것처럼 quantity \( \gamma \) 를 정의한다.</li>
</ul>

<script type="math/tex; mode=display">\gamma = M-\alpha\sum_{i=1}^M\frac{1}{\lambda_i+\alpha} = M - \alpha Tr(S_N)</script>

<ul>
  <li>사실 이 식은 (식3.92)에서 사용했던 방식으로 해도 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">a{\bf m}_N^T{\bf m}_N = \gamma = M- \alpha Tr(S_N)</script>

<ul>
  <li>이후 \( \beta \) 를 전개하면 3.5.2절에서 보았던 것과 동일한 결과를 얻는다.</li>
</ul>

<hr />

<p><strong>EM을 이용하여 베이지언 선형 회귀의 초모수 값 결정하기</strong></p>

<ul>
  <li><strong>E-Step</strong>
    <ul>
      <li>responsibilities 계산 : 잠재변수 \( {\bf w} \) 의 값을 결정한다.</li>
      <li>\( p({\bf w} | {\bf t}, \alpha, \beta) = N({\bf w} | {\bf m}, S) \)</li>
      <li>\( m=\beta S\Phi^T{\bf t} \)</li>
      <li>\( S^{-1}=\alpha{\bf I}+\beta\Phi^T\Phi \)</li>
    </ul>
  </li>
  <li><strong>M-Step</strong>
    <ul>
      <li>\( \alpha^{-1} = \frac{1}{M}({\bf m}^T{\bf m}+Tr(S)) \)</li>
      <li>\( \beta^{-1} = \frac{1}{N}\sum_{n=1}^N {t_n-{\bf m}^T\phi({\bf x}_n)}^2 \)</li>
    </ul>
  </li>
  <li>RVM(Relevance vector machine) 회귀는 교재에 간단히 언급되나 생략함.</li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoongithub'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-ml-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
