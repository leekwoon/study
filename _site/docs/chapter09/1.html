<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>1. K-means Clustering</title>
  <link rel="stylesheet" href="/kyo-ml-study/css/main.css">
  <link rel="stylesheet" href="/kyo-ml-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-ml-study/docs/chapter09/1.html">
  <script src="/kyo-ml-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-ml-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-ml-study/"><strong>kyo-ml-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <!-- for drop-down navi -->
            <li class="dropdown">
              <a href="/kyo-ml-study" class="dropdown-toggle" data-toggle="dropdown"><strong>Chapter
                  <b class="caret"></b></strong>
              </a>
              <ul class="dropdown-menu">
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter01/0">1. Introduction</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter02/0">2. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter03/0">3. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter04/0">4. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter05/0">5. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter06/0">6. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter07/0">7. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter08/0">8. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter09/0">9. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter10/0">10. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter11/0">11. Neural Networks</a>
                </li>
                
                
              </ul>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter01/0"><strong>1</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter02/0"><strong>2</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter03/0"><strong>3</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter04/0"><strong>4</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter05/0"><strong>5</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">6</strong></a></li>
                      
          
          
            <li><a href="#"><strong class="text-danger">7</strong></a></li>
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter08/0"><strong>8</strong></a>
            </li>
            
                      
          
          
            
            <li  class="active" >
              <a href="/kyo-ml-study/docs/chapter09/0"><strong>9</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter10/0"><strong>10</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">11</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 9</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/0">0. Mixture Models and EM</a>
      </li>
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-ml-study/docs/chapter09/1">1. K-means Clustering</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/2">2. Mixture of Gaussians</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/3">3. An Alternative View of EM</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/4">4. The EM Algorithm in General</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-ml-study/blob/gh-pages/docs/chapter09/1.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>1. K-means Clustering</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>우리는 이제부터 다차원 입력 데이터에 대해 해당 데이터가 어떤 그룹에 속하게 될지를 결정짓는 클러스터링 문제를 다루게 될 것이다.</li>
  <li>이 때의 데이터 집합 \( ({\bf x}_1,…,{\bf x}_N) \) 을 고려하자.
    <ul>
      <li>이 데이터는 총 \( N \) 개의 데이터로 \( D \) 개의 차원을 가지며 \( K \) 개의 클러스터로 나눌 예정이다.</li>
      <li>최종 목표는 모든 샘플 데이터를 \( K \) 개의 클러스터에 각각 배정하는 것이다. (물론 \( K \) 값은 미리 지정한다.)</li>
    </ul>
  </li>
  <li>간단하게 표기법을 보자면 \( D \) 개의 차원을 가지는 벡터를 \( {\bf u} \) 처럼 표기하고 (볼드체 주의),
    <ul>
      <li>이 벡터가 \( k \) 번 째 클러스터에 속하는 경우 \( {\bf u}_k \) 와 같이 표기한다.</li>
      <li>이제 이 \( {\bf u}_k \) 벡터를 \( k \) 번째 클러스터의 정 중앙에 높여있는 벡터라고 생각해보자. (무게중심)</li>
      <li>\( {\bf u}_k \) 를 중심으로 해당 클러스터에 속하는 점들과의 거리의 총 합은, 클러스터에 속하지 않는 점들과의 거리의 총 합보다 작다.</li>
    </ul>
  </li>
  <li>
    <p>우리의 최종 목표는 주어진 데이터 집합으로부터 이러한 중심점 \( {\bf u}_k \) 의 값들을 결정하는 것.</p>
  </li>
  <li>위와 같은 문제는 데이터 집합을 특정 클러스터에 할당하는 과정으로 표기하는 것이 좋다.</li>
  <li>그러기 위해 우선 변수 \( r_{nk}\in {0,1} \) 을 정의한다.
    <ul>
      <li>이 때의 \( k \) 값은 당연히 \( k=1,…K \) 이다.</li>
      <li>이는 어떤 \( n \) 번째 샘플 \( {\bf x}_n \) 가 \( k \) 번째 클러스터에 속하는 경우 \( r_{nk}=1 \) 이고 아닌 경우 0이 된다.</li>
      <li>이건 이전에 봤던 <em>1-to-K</em> 코딩 방식이다.</li>
    </ul>
  </li>
  <li>이제 목적 함수를 정의한다. 이를 왜곡 측정( <em>distortion measure</em> ) 함수라고 한다.</li>
</ul>

<script type="math/tex; mode=display">J=\sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\|{\bf x}_n-{\bf \mu}_k\|^2 \qquad{(9.1)}</script>

<ul>
  <li>이는 같은 클러스터에 속하는 각각의 점들로부터 그 클러스터의 평균과의 거리의 합을 제곱한 함수이다.</li>
  <li>우리는 여기서 \( {r_{nk} } \) 와 \( {\bf \mu}_k \) 를 구해야 한다
    <ul>
      <li>이 때 함수 \( J \) 값이 최소가 될 때 각각의 값을 구해야 한다.</li>
    </ul>
  </li>
  <li>이를 위해 우리는 반복적인(iterative) 절차를 통해 이를 해결하는 방법을 살펴볼 것이다.
    <ul>
      <li>\( r_{nk} \) 와 \( {\bf \mu}_k \) 를 구하기 위해 크게 2개의 단계로 나누게 된다.</li>
      <li>먼저 \( {\bf \mu}_k \) 의 임의의 초기값을 설정한다.</li>
      <li>첫번째 단계에서는 이 \( {\bf \mu}_k \) 값을 고정한 채로 \( J \) 를 최소화하는 \( r_{nk} \) 값을 구한다.</li>
      <li>두번째 단계에서는 새롭게 얻어진 \( r_{nk} \) 를 고정하고 다시 \( {\bf \mu}_k \) 를 구한다.</li>
      <li>두 값이 적당한 범위 내로 수렴할 때까지, 혹은 적당한 반복 횟수에 도달할 때까지 계속 반복한다.</li>
      <li>각각의 두 단계를 \( E \) (expectation) 단계와 \( M \) (maximazation) 단계로 부르고 합쳐 \( EM \) 알고리즘이라고 한다.</li>
    </ul>
  </li>
  <li>이제 이러한 \( E \) 단계, \( M \) 단계를 이용한 K-means 알고리즘을 확인해보도록 하자.</li>
  <li>\( r_{nk} \) 를 구하는 과정을 먼저 살펴본다.
    <ul>
      <li>\( J \) 가 \( r_{nk} \) 에 대해 선형 함수이므로 닫힌 형태(closed form)라고 볼 수 있다.</li>
      <li>샘플은 모두 서로 독립적이므로 \( r_{nk} \) 도 각각의 \( n \) 에 대해 따로 최적화하면 된다.
        <ul>
          <li>즉, 다른 샘플과의 연관을 고려할 필요없이 현재 샘플에 대해 가장 타장한 \( r_{nk} \) 값을 선택하면 된다.</li>
        </ul>
      </li>
      <li>이 과정이 크게 어려운 것은 없다. 그냥 각 클러스터 중심과 샘플의 거리를 측정해서 가장 가까운 클러스터를 선택하면 된다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
r_{nk}=\left\{\begin{array}{ll} 1 & if\;k=argmin_j\|{\bf x}_n-{\bf \mu}_j\|^2 \\0 & otherwise\end{array}\right. \qquad{(9.2)} %]]></script>

<ul>
  <li>이제 \( {\bf \mu}_k \) 를 구해보도록 하자. 물론 \( r_{nk} \) 는 고정한다.
    <ul>
      <li>목적함수 \( J \) 는 \( {\bf \mu}_k \) 에 대해서는 이차형식(quadratic)이다.</li>
      <li>따라서 미분을 통해 최소값이 되는 지점을 얻을 수 있다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">2\sum_{n=1}^{N}r_{nk}({\bf x}_n-{\bf \mu}_k)=0 \qquad{(9.3)}</script>

<ul>
  <li>이를 전개하면 다음을 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_k = \frac{\sum_n r_{nk}{\bf x}_n}{\sum_n r_{nk} } \qquad{(9.4)}</script>

<ul>
  <li>결국 \( k \) 클러스터에 속한 점들의 평균을 구하게 된다. (k-means 라는 이름이 이런 이유이다.)</li>
  <li>두 단계를 거치는 동안 데이터는 각각의 클러스터에 다시 할당이 되고,</li>
  <li>이렇게 재할당된 데이터를 이용하여 평균 값을 다시 계산하는 과정을 반복하게 된다.</li>
</ul>

<hr />

<ul>
  <li>그림을 통해 <em>K</em>-means 가 수렴해가는 과정을 살펴보도록 하자.</li>
</ul>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure9.1a.png" alt="Figure 9.1a" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.1b.png" alt="Figure 9.1b" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.1c.png" alt="Figure 9.1c" height="180px" />
</div>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure9.1d.png" alt="Figure 9.1d" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.1e.png" alt="Figure 9.1e" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.1f.png" alt="Figure 9.1f" height="180px" />
</div>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure9.1g.png" alt="Figure 9.1g" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.1h.png" alt="Figure 9.1h" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.1i.png" alt="Figure 9.1i" height="180px" />
</div>

<ul>
  <li>그림을 보면 쉽게 이해가 된다.
    <ul>
      <li>우선 2개의 클러스터를 구성한다고 결정한 상태이다.</li>
      <li>임의의 2점을 평균 값으로 하여 \( EM \) 과정을 반복하게 된다.</li>
      <li>그림 순서대로 평균 값을 계산한 뒤, 각각의 샘플에 대해 어느 클러스터에 속할지 다시 할당하고,</li>
      <li>이렇게 할당된 데이터를 기준으로 다시 평균 값을 계산한다.</li>
      <li>이를 수렴 조건이 만족할 때까지 반복한다.</li>
    </ul>
  </li>
</ul>

<p><img src="/kyo-ml-study/images/Figure9.2.png" alt="figure9.2" class="center-block" height="200px" /></p>

<ul>
  <li>
    <p>위 그림은 \( EM \) 과정을 반복할 때마다 함수 \( J \) 이 값이 줄어드는 것을 나타내는 그림이다.</p>
  </li>
  <li>
    <p>사실 평균 \( {\bf \mu}_k \) 의 초기 값을 어디에 위치가는가에 따라 성능의 차이가 발생한다.</p>
    <ul>
      <li>실제 반복 횟수의 차이가 크게 발생할 수 있기 때문이다.</li>
      <li>초기 평균값을 정하는 적당한 방법 중 하나는 샘플 내에서 임의의 점들을 추출하여 평균을 구한 뒤 이를 초기 평균값으로 사용하는 것이다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>지금까지 언급한 방식대로 <em>K</em>-means를 구현한다면 성능이 크게 느릴 수 있는데 ,
    <ul>
      <li>\( E \) 단계에서 평균과 모든 점들에 대해 비교를 하는 과정이 포함되어 있기 때문이다.</li>
    </ul>
  </li>
  <li>학습 속도를 높이기 위한 방법으로 여러 가지를 고려해볼 수 있다.
    <ul>
      <li>데이터를 트리 자료구조로 저장하여 사용하는 방법이 있다. (KD-트리 같은 방식)</li>
      <li>삼각 부등식(triangle inequality) 방식을 사용하여 거리 계산을 하지 않는 방식도 있다고 한다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>지금까지는 배치(batch) 방식의 업데이트 기법을 살펴보았다.</li>
  <li>온라인 업데이트 (online-update) 방식의 업데이트 기법도 있는데 (<em>MacQueen, 1967</em>)
    <ul>
      <li>정의한 \( J \) 함수에 대해 \( u_{k} \) 로 미분을 하면 회귀 함수가 얻어진다.</li>
      <li>이에 대한 근(root) 값을 계산하는데 \( Robbins-Monro \) 알고리즘을 사용하는 방식이다.</li>
      <li>이 식은 다음과 같은 식을 유도한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_k^{new} = {\bf \mu}_k^{old}+\eta_n({\bf x}_n-{\bf \mu}_k^{old}) \qquad{(9.5)}</script>

<ul>
  <li>이 때 사용되는 학습 속도 비율(<em>learning rate</em>) \( \eta_n \) 는 식을 단조 감소하도록 만드는 값으로 사용해야 한다.</li>
</ul>

<hr />

<ul>
  <li><em>K</em>-means 알고리즘은 샘플과 평균간의 유클리디안 거리 측정 방식을 사용하기 때문에 데이터 타입에 제약이 존재한다.
    <ul>
      <li>유클리디안 거리를 사용할 수 있는 자료 타입이어야 한다.</li>
      <li>예를 들어 특정 카테고리를 가지는 명목형 타입인 경우 거리를 측정할 수 없다.</li>
    </ul>
  </li>
  <li>또한 이상치(outlier)에 영향을 많이 받는 알고리즘이다.
    <ul>
      <li>평균을 사용하기 때문인데, 원래 평균값이 이상치에 취약하다.</li>
    </ul>
  </li>
  <li>따라서 이러한 제약을 없앨 수 있도록 <em>K</em>-means 알고리즘을 일반화한 알고리즘이 <em>K</em>-medoids 알고리즘이다.</li>
</ul>

<p><strong>K-medoids 알고리즘</strong></p>

<ul>
  <li>유클리디안 거리 측정 함수 대신 \( \nu({\bf x}, {\bf x}’) \)  를 정의하여 사용한다.
    <ul>
      <li>이는 \( {\bf x} \) 와 \( {\bf x}’ \) 사이의 거리 개념을 나타내는 것이다.</li>
      <li>즉, 거리를 구하는 식에 반드시 유클리디안 방식을 사용하지 않아도 된다.</li>
    </ul>
  </li>
  <li>목적 함수는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">\tilde{J}=\sum_{n=1}^N\sum_{k=1}^K r_{nk}\nu({\bf x}, {\bf \mu}_k) \qquad{(9.6)}</script>

<ul>
  <li>\( E \) 스텝은 앞서 살펴본 방식과 동일한 방법으로 진행되며 따라서 이 때 필요한 연산량도 \( O(NK) \) 가된다.</li>
  <li>\( M \) 스텝은 단연히 달라지게 되는데,
    <ul>
      <li>두 점 사이의 상이함을 측정하는 단계이므로 (dissmilarity meature) \( \nu(\cdot) \) 의 비용이 이전보다 더 클수 있다.</li>
      <li>그래서 연산량을 줄이기 위해 클러스터의 중심점을 클러스터에 속하는 데이터 중 하나로만 선정하기도 한다.
        <ul>
          <li>이러면 각 점들의 거리 계산을 중간에 좀 저장해 두고 다시 사용할 수 있다.</li>
        </ul>
      </li>
      <li>이 경우 \( \nu(\cdot) \) 를 계산하는 비용은 \( O(N_k^2) \) 이 필요하다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li><em>K</em>-means 알고리즘은 계산 중에 하나의 점이 반드시 하나의 클러스터에 속하는 것으로 간주되어 계산되게 된다.
    <ul>
      <li>하지만 한 점이 여러 개의 클러스터의 중심으로부터 모두 비슷한 거리에 위치할 수도 있다.
        <ul>
          <li>이런 경우에는 이 점을 어떤 클러스터로 속하게 할지 애매하다.</li>
        </ul>
      </li>
      <li>확률적인 접근 방식을 이용하여 특정 클러스터에 대한 할당을 특정 클러스터에 포함될 확률값으로 계산할 수도 있다.
        <ul>
          <li>이러한 방식을 soft 대입 방식이라고 한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="image-segmentation-and-compression">9.1.1. 이미지 분할 및 압축 (Image segmentation and compression)</h2>
<ul>
  <li><em>K</em>-means 알고리즘을 살펴보기 위해서 이미지를 분할하고 압축하는 예제를 살펴보도록 한다.</li>
</ul>

<p><strong>이미지 분할</strong></p>

<ul>
  <li>목표 : 이미지 영역을 동질의 시각적 영역 단위로 나누는 작업
    <ul>
      <li>말이 좀 어렵긴한데 예제를 보면 쉽게 알 수 있다. 그냥 비슷한 색상들을 대표 색상으로 그룹화하는 작업.</li>
    </ul>
  </li>
  <li>이미지의 각 픽셀(pixel)을 하나의 데이터 포인트로 간주한다.</li>
  <li>픽셀(pixel)
    <ul>
      <li>3차원 데이터 (RGB format : Red, Green, Blue)</li>
      <li>이 때 각각의 색상의 범위를 정규화하여 [0, 1] 사이의 값만 가지도록 한다.
        <ul>
          <li>보통 한 색상을 0~255로 표현하는 경우도 많다. (R, G, B 각각에 대해)</li>
        </ul>
      </li>
      <li><em>K</em>-means 알고리즘을 적용하여 각 픽셀 데이터를 \( {R,G,B} \) 로 표현되는 중심 벡터 \( \mu_k \) 로 표현한다.</li>
    </ul>
  </li>
</ul>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure9.3a.png" alt="Figure 9.3a" width="120px" />
  <img src="/kyo-ml-study/images/Figure9.3b.png" alt="Figure 9.3b" width="120px" />
  <img src="/kyo-ml-study/images/Figure9.3c.png" alt="Figure 9.3c" width="120px" />
  <img src="/kyo-ml-study/images/Figure9.3d.png" alt="Figure 9.3d" width="120px" />
</div>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure9.3e.png" alt="Figure 9.3e" width="120px" />
  <img src="/kyo-ml-study/images/Figure9.3f.png" alt="Figure 9.3f" width="120px" />
  <img src="/kyo-ml-study/images/Figure9.3g.png" alt="Figure 9.3g" width="120px" />
  <img src="/kyo-ml-study/images/Figure9.3h.png" alt="Figure 9.3h" width="120px" />
</div>

<ul>
  <li>위의 그림은 이미지에 <em>K</em>-means를 적용한 결과이다.</li>
  <li>각각의 점들을 얻어진 중심 픽셀의 대표 색으로 변경한 뒤에 다시 그림을 그렸을 때의 결과이다.</li>
</ul>

<p><strong>이미지 압축</strong></p>

<ul>
  <li>분할 문제 뿐만 아니라 압축 문제에서도 <em>K</em>-means를 적용해 볼 수 있다.
    <ul>
      <li>물론 이 때 다루는 압축 방식은 손실 압축(lossy data compression) 방식을 의미한다.</li>
    </ul>
  </li>
  <li>압축 과정은 다음과 같다.
    <ul>
      <li>\( N \) 개의 픽셀이 주어졌을 때 이를 \( K \) 개의 클러스터에 각각 할당한 뒤에</li>
      <li>실제 각 점들을 다음과 같이 새로 저장한다.
        <ul>
          <li>각 점들이 할당된 클러스터의 식별자 \( K \) 를 만들고 (총 \( K \) 개)</li>
          <li>각 점들에 대해 픽셀 색상이 아닌 이에 대응하는 식별자 값을 저장한다.
            <ul>
              <li>물론 이 때 식별자는 매우 작은 숫자이므로 좀 더 적은 저장 크기를 가지게 된다.</li>
            </ul>
          </li>
          <li>다음으로 \( K \) 에 대한 중심값 \( \mu_k \) 의 값을 저장.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이러한 방식을 벡터 양자화(vector quantization) 라고 한다.
    <ul>
      <li>그리고 이 때 사용된 \( \mu_k \) 를 코드북 벡터(code-book vector)라고 부른다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>사실 데이터 압축 문제를 데이터 분할 문제로도 다룰 수 있는데,
    <ul>
      <li>이미지를 분할하면 실제 이미지 압축 효과를 얻을 수 있기 때문</li>
    </ul>
  </li>
  <li>원본 이미지
    <ul>
      <li>N 개의 픽셀</li>
      <li>하나의 픽셀은 \( {R,G,B} \) 이며 한 색당 8bit 로 저장되므로 한 필셀은 24bit</li>
      <li>따라서 전체 픽셀 저장에 필요한 크기는 \( 24N \) bit</li>
    </ul>
  </li>
  <li><em>K</em>-means가 적용된 이미지
    <ul>
      <li>하나의 픽셀을 \( \mu_k \) 의 id로 표현하게 된다.</li>
      <li>이런 경우 하나의 픽셀은 \( log_2K \) bit 가 필요. 총 \( N \) 개의 픽셀은 \( Nlog_2K \) bit 가 필요.</li>
      <li>코드북은 24bit 가 \( K \) 개 필요. 즉, 24K bit</li>
      <li>총 필요한 저장 크기는 \( 24K + Nlog_2K \) bit</li>
    </ul>
  </li>
  <li>
    <p>실제 \( K « N \) 이므로 저장 크기가 많이 줄어들 수 있다.</p>
  </li>
  <li>위의 그림에 대한 압축 비율을 살펴보도록 하자.
    <ul>
      <li>원본 이미지 : \( 240\times180 \) 픽셀이므로 \( 24\times43,200=1,036,800 \) bit 가 필요</li>
      <li>\( K=2 \) 인 경우 : \( 43,248 \) bit ( 원본의 4.2% )</li>
      <li>\( K=3 \) 인 경우 : \( 86,472 \) bit ( 원본의 8.3% )</li>
      <li>\( K=10 \) 인 경우 : \( 173,040 \) bit ( 원본의 16.7% )</li>
    </ul>
  </li>
  <li>물론 압축 정도와 이미지 품질간의 trade-off 가 존재한다.</li>
  <li>좀 더 좋은 이미지 압축 방식을 고려한다면, 인접 픽셀 블럭 (예를 들어 \( 5\times5 \) 크기의 픽셀 블럭) 단위의 압축 등을 고려할 수 있다.</li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoongithub'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-ml-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
