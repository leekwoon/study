<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2. Mixture of Gaussians</title>
  <link rel="stylesheet" href="/kyo-ml-study/css/main.css">
  <link rel="stylesheet" href="/kyo-ml-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-ml-study/docs/chapter09/2.html">
  <script src="/kyo-ml-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-ml-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-ml-study/"><strong>kyo-ml-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <!-- for drop-down navi -->
            <li class="dropdown">
              <a href="/kyo-ml-study" class="dropdown-toggle" data-toggle="dropdown"><strong>Chapter
                  <b class="caret"></b></strong>
              </a>
              <ul class="dropdown-menu">
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter01/0">1. Introduction</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter02/0">2. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter03/0">3. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter04/0">4. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter05/0">5. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter06/0">6. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter07/0">7. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter08/0">8. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter09/0">9. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter10/0">10. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter11/0">11. Neural Networks</a>
                </li>
                
                
              </ul>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter01/0"><strong>1</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter02/0"><strong>2</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter03/0"><strong>3</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter04/0"><strong>4</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter05/0"><strong>5</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">6</strong></a></li>
                      
          
          
            <li><a href="#"><strong class="text-danger">7</strong></a></li>
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter08/0"><strong>8</strong></a>
            </li>
            
                      
          
          
            
            <li  class="active" >
              <a href="/kyo-ml-study/docs/chapter09/0"><strong>9</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter10/0"><strong>10</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">11</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 9</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/0">0. Mixture Models and EM</a>
      </li>
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/1">1. K-means Clustering</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-ml-study/docs/chapter09/2">2. Mixture of Gaussians</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/3">3. An Alternative View of EM</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter09/4">4. The EM Algorithm in General</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-ml-study/blob/gh-pages/docs/chapter09/2.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>2. Mixture of Gaussians</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>앞서 2.3.9 절에서 이미 <em>GMM</em> 즉, 가우시안 혼합 모델을 살펴보았다.
    <ul>
      <li>잠시 언급해보자면 여러 개의 가우시안 분포를 선형 결합한 형태였다.</li>
    </ul>
  </li>
  <li>이번 절에서는 가우시안 혼합 분포를 이산 잠재 변수(discrete latent variable)를 도입하여 풀어갈 예정이다.
    <ul>
      <li>이러한 방법은 혼합 분포를 좀 더 명확하게 이해하는데 도움을 준다.</li>
      <li>그리고 <em>EM</em> 알고리즘을 왜 사용해야 하는지도 이해하게 될 것이다.</li>
    </ul>
  </li>
  <li>이제 예전에 살펴보았던 식을 상기해보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}) = \sum_{k=1}^K \pi_k N({\bf x}|{\bf \mu}_k, \Sigma_k) \qquad{(9.7)}</script>

<ul>
  <li>이제 \( K \) 차원을 가지는 이진 랜덤 변수 (binary random variable) \( z \) 변수를 도입한다.
    <ul>
      <li>이전에도 보았던 <em>1-to-K</em> 코딩 스킴 형태의 변수다.</li>
      <li>마찬가지로 잠재(latent) 변수의 형태로 식을 전개할 것이다.</li>
      <li>\( K \) 개의 요소 중 1개의 값만 1이고 나머지는 0인 값이다.</li>
      <li>따라서 \( z_k \in {0,1} \) 이고 \( \sum_k z_k=1 \) 이다.</li>
    </ul>
  </li>
  <li>이제 잠재변수와의 결합 분포를 고려해보자.
    <ul>
      <li>식에 의해 \( p({\bf x}, {\bf z}) = p({\bf z})p({\bf x}|{\bf z}) \) 이다.</li>
    </ul>
  </li>
</ul>

<p><img src="/kyo-ml-study/images/Figure9.4.png" alt="figure9.4" class="center-block" height="120px" /></p>

<ul>
  <li>이제 \( z_k \) 의 주변 분포를 혼합계수(mixing coefficients) \( \pi_k \) 를 사용하여 정의할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p(z_k=1) = \pi_k</script>

<ul>
  <li>파라미터 \( \pi_k \) 는 다음과 같은 성질을 만족해야 한다.</li>
</ul>

<script type="math/tex; mode=display">0 \le \pi_k \le 1 \qquad{(9.8)}</script>

<script type="math/tex; mode=display">\sum_{k=1}^K \pi_k = 1 \qquad{(9.9)}</script>

<ul>
  <li>이제 \( {\bf z} \) 가 <em>1-of-K</em> 코딩 스킴을 사용하기 때문에 이에 대한 확률 식을 다음과 같이 사용할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf z})=\prod_{k=1}^K \pi_k^{z_k} \qquad{(9.10)}</script>

<ul>
  <li>이와 유사하게 이제 \( x \) 에 대한 조건부 확률을 표현해보자. \( x \) 는 \( z_k \) 가 주어졌을 때 가우시안 분포를 따른다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|z_k=1) = N({\bf x}|{\bf \mu_k}, \Sigma_k)</script>

<ul>
  <li>어려운 내용은 아니다. 특정 클러스터에 해당하는 가우시안 분포를 조건부 분포로 표현한 것이다.</li>
  <li>이를 일반화시키면 다음과 같아진다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf z}) = \prod_{k=1}^{K} N({\bf x}|{\bf \mu}_k, \Sigma_k)^{z_k} \qquad{(9.11)}</script>

<ul>
  <li>이제 \( {\bf x} \) 에 대한 주변 확률을 계산할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}) = \sum_z p({\bf z})p({\bf x}|{\bf z}) = \sum_{k=1}^{K} \pi_k N({\bf x}|{\bf \mu}_k, \Sigma_k) \qquad{(9.12)}</script>

<ul>
  <li>결국 예전에 봤던 식이 나오게 된다.</li>
</ul>

<hr />

<ul>
  <li>명시적으로 잠재 변수 \( {\bf z} \) 를 도입하며 \( {\bf x} \) 에 대한 주변 확률 분포를 구하는 식을 살펴보았다.</li>
  <li>사실 이전에 보아왔던 것과 동일한 결과를 가진다는 것 외에는 별로 얻은 소득은 없어 보인다.</li>
  <li>하지만 이제 우리는 명시적으로 잠재 변수를 도입하여 결합 확률 분포 \( p({\bf x}, {\bf z}) \) 를 사용하였다는게 중요하다.</li>
  <li>
    <p>주변 확률 \( p({\bf x}) \) 대신에 조건부 확률을 통해 작업을 진행할 수 있다. 이로써 \( EM \) 알고리즘도 함께 사용가능하게 되는 것이다.</p>
  </li>
  <li>이제 \( {\bf x} \) 가 주어졌을 때의 조건부 확률을 정의해보자.</li>
</ul>

<script type="math/tex; mode=display">\gamma(z_k) \equiv p(z_k=1|{\bf x}) = \frac{p(z_k=1)p({\bf x}|z_k=1)}{\sum_j^K p(z_j=1)p({\bf x}|z_j=1)}=\frac{\pi_k N({\bf x}|{\bf \mu}_k, \Sigma_k)}{\sum_{j=1}^K \pi_j N({\bf x}|{\bf \mu}_j, \sigma_j)} \qquad{(9.13)}</script>

<ul>
  <li>우리는 \( \pi_k \) 가 \( z_k=1 \) 일 때의 사전 확률 값이라는 것을 알고 있다.</li>
  <li>이제 \( \gamma(z_k) \) 는 관찰 데이터 \( {\bf x} \) 가 주어졌을 때의 사후 확률 값이 된다.</li>
  <li>이후에 살펴볼 것이지만 \( \gamma(z_k) \) 를 성분 \( k \) 에 대한 <em>responsibility</em> 라고도 한다.</li>
</ul>

<hr />

<ul>
  <li>가우시안 혼합 모델을 이용하여 샘플을 생성하는 방법에 대해 살펴보도록 하자. (실제적인 내용은 11장에 나온다.)
    <ul>
      <li>이 작업을 수행하기 위해서는 우선 \( p({\bf z}) \) 분포로부터 \( {\bf z} \) 값을 생성해야 한다.</li>
      <li>이 때의 값을 \( \widehat{\bf z} \) 라고 하자.</li>
      <li>이제 \( \widehat{\bf z} \) 가 주어진 상태에서의 \( {\bf x} \) 값을 조건부 분포 \( p({\bf x}|\widehat{\bf z}) \) 에서 샘플을 생성한다.</li>
      <li>이러한 샘플링 방식을 ancestral sampling 이라고 한다. 앞 장에서 이미 다루었던 내용이다.</li>
    </ul>
  </li>
  <li>우리는 \( p({\bf x}, {\bf z}) \) 로부터 샘플을 그려낼 수 있다.</li>
</ul>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure9.5a.png" alt="Figure 9.5a" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.5b.png" alt="Figure 9.5b" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.5c.png" alt="Figure 9.5c" height="180px" />
</div>

<ul>
  <li>위 그림은 3개의 가우시안 혼합 분포로부터 500개의 샘플을 생성한 후의 그림이다.</li>
  <li>\( (a) \) 는 결합 분포 \( p({\bf z})p({\bf x}|{\bf z}) \) 로부터 각각의 \( z_k \) 에 대해 생성된 샘플을 다른 색으로 표현하고 있다.</li>
  <li>\( (b) \) 는 생성된 데이터의 타겟 레이블을 삭제한 후의 결과이고,</li>
  <li>\( (c) \) 는 \( \gamma(z_k) \) 에 따라 색을 달리하여 표현한 것이다.
    <ul>
      <li>하지만 위의 그림을 자세히보면 \( (a) \) 와는 색상이 좀 다른 것을 알 수 있는데,</li>
      <li>실제 결과를 경성 혼합(soft mixture) 방식으로 표현했기 때문이다.</li>
      <li>즉, \( \gamma(z_k) \) 의 값에 따라 각각의 색을 적당히 혼합했다.
        <ul>
          <li>예를 들어 \( \gamma(z_1)=1 \) 인 경우 완전히 붉은색으로 표기되지만,</li>
          <li>\( \gamma(z_2)=0.5 \) , \( \gamma(z_3)=0.5 \) 와 같은 결과를 얻은 경우 녹색과 파란색을 반반씩 섞게 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>참고로 \( (a) \) 와 같은 결과를 <em>complete</em> 하다고 표현하고 (b)와 같은 경우 <em>incomplete</em> 하다고 표현한다.</li>
</ul>

<h2 id="maximum-likelihood">9.2.1. 최대 가능도 (Maximum likelihood)</h2>

<ul>
  <li>관찰 데이터가 \( { {\bf x}_1,…,{\bf x}_N} \) 으로 주어졌다고 가정하자.</li>
  <li>이를 이용하여 가장 적합한 혼합 가우시안 분포를 결정하는 문제를 확인해 볼 차례이다.</li>
  <li>이 때의 입력 데이터는 \( N\times D \) 의 크기를 가지며, ( \( N \) 은 샘플의 개수, \( D \) 는 한 샘플의 차원)</li>
  <li>따라서 입력 데이터를 행렬 \( X \) 로 표현할 수 있다 . 이런 경우에 한 줄의 해당하는 샘플은 \( {\bf x}_n^T \) 가 된다.</li>
  <li>이제 이 샘플에 대응하는 잠재 변수 \( Z \) 를 고려해보자.
    <ul>
      <li>이 변수는 \( N\times K \) 크기의 행렬이 될 것이다. 이 때 한 샘플에 대응하는 잠재 변수는 \( {\bf z}_n^T \) 가 된다.</li>
    </ul>
  </li>
  <li>모든 데이터가 독립이라고 가정하면, 우리는 이를 그래프 모델로 쉽게 표현 가능하다.</li>
</ul>

<p><img src="/kyo-ml-study/images/Figure9.6.png" alt="figure9.6" class="center-block" height="120px" /></p>

<ul>
  <li>이제 로그 가능도 함수(likelihood function)를 기술해보자.</li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}|{\bf \pi}, {\bf \mu}, \Sigma) = \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)\right\} \qquad{(9.14)}</script>

<ul>
  <li>MLE를 통해 이 함수를 최대화하는 파라미터를 찾는 과정을 살펴보기 전에 중요한 2가지 이슈에 대해 논의해보자.
    <ul>
      <li>특이점(singularities) 문제 (problem of singularities)</li>
      <li>혼합의 식별 문제 (Identifiability of mixtures)</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>특이점 문제 (Problem of singularities)</strong></p>

<ul>
  <li>공분산이 \( \Sigma_k=\sigma_k^2{\bf I} \) 인 간단한 모델을 생각해보자.</li>
  <li>이 때 \( j \) 번째 분포만을 고려한다면 이 분포의 평균은 \( \mu_j \) 일 것이다.</li>
  <li>그런데 이 때 오로지 단 1개의 샘플만이 이 컴포넌트에 속하게 된다고 생각해보자.
    <ul>
      <li>이런 경우 \( \mu_j={\bf x}_n \) 이 된다.</li>
    </ul>
  </li>
  <li>이러면 가능도 함수의 결과는 다음과 같이 된다.</li>
</ul>

<script type="math/tex; mode=display">N({\bf x}_n|{\bf x}_n, \sigma_j^2{\bf I}) = \frac{1}{(2\pi)^{1/2}}\frac{1}{\sigma_j} \qquad{(9.15)}</script>

<ul>
  <li>이 때 \( j \) 번째 컴포넌트가 \( \sigma_j\rightarrow0 \) 인 성질을 가진다고 해보자.
    <ul>
      <li>즉, \( j \) 번째 컴포넌트의 확률 분포는 거의 하나의 값( \( \mu_j \) )으로 수렴되는 상태. (평균 값에서 높은 피크를 가진다)</li>
    </ul>
  </li>
  <li>이런 경우 가능도 함수의 결과 값도 무한대 값으로 수렴하게 된다.
    <ul>
      <li>가능도 함수는 각각의 분포의 곱으로 표현되기 때문에 가능도 함수 또한 무한대가 된다.</li>
    </ul>
  </li>
  <li>사실 이러한 형태의 가능도 함수는 우량조건(well-posed) 문제가 아님을 알 수 있는데,
    <ul>
      <li>이러한 특이점 문제가 언제나 발생을 하고,</li>
      <li>가우시안 컴포넌트 중 하나가 특정 데이터 점으로 쏠리기만 해도 이러한 문제는 발생할 수 밖에 없다.</li>
      <li>참고로 우량 조건의 문제란 명확한 해를 구할 수 있는 문제를 의미한다.</li>
    </ul>
  </li>
  <li>그런데 이 문제는 1개의 가우시안 모델을 사용하는 가능도 함수의 경우에도 동일하게 발생할 수 있는 문제가 아닌가?
    <ul>
      <li>결론부터 말하자면 하나의 가우시안 분포만을 사용하는 가능도 함수의 경우에는 이런 문제가 발생하지 않는데,</li>
      <li>하나의 위치로 거의 수렴하는 단일 가우시안 모델은 (즉, \( \sigma\rightarrow0 \) 인 모델) 평균 값 외에는 밀도 값이 0이 된다.</li>
      <li>따라서 가능도 함수에서 어떤 한 샘플이 평균 값과 같더라도 다른 샘플들이 0 값을 가지게 되므로 최종 곱은 0이 된다.
        <ul>
          <li>가능도 함수는 각 샘플에 대한 확률 분포의 곱으로 표기됨을 잊지 말자.</li>
          <li>그리고 꽤 많은 수의 0의 곱과 무한대의 곱의 경우 0으로 수렴하는 속도가 더 빠르기 때문에 0이라고 생각할 수 있다.</li>
          <li>물론 샘플이 한개밖에 없다면 발생할수 있겠지만 보통 하나의 가우시안 모델에서 샘플 한개만 주어진다고는 생각하기 어렵다.</li>
          <li>반면 혼합 분포에서는 \( N \) 개의 샘플이 주어진다고 해도 한개의 샘플만 하나의 분포에 속하는 일이 발생 가능하다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/kyo-ml-study/images/Figure9.7.png" alt="figure9.7" class="center-block" height="200px" /></p>

<ul>
  <li>위의 그림을 보면 현재 2개의 가우시안 분포가 혼합된 형태이므로, 각각의 샘플들의 가능도 함수 값은 임의의 실수값을 가지게 된다.</li>
  <li>하지만 마지막에 피크를 보면 해당 분포는 한 점을 중심으로 높은 피크를 그리는 형태이다.</li>
  <li>
    <p>최종적으로 모든 샘플의 가능도 함수 값을 곱해야 하는데 마지막 값으로 인해 발산한다.</p>
  </li>
  <li>사실 MLE 방식은 오버 피팅 문제가 존재하는데 이러한 특이점 문제도 오버 피팅 문제의 또 다른 예이다.
    <ul>
      <li>따라서 베이지안 방식에서도 이러한 문제는 발생하지 않는다.</li>
    </ul>
  </li>
  <li>이런 문제를 피하는 방법은 여러 가지가 있는데 휴리스틱한 방법으로는,
    <ul>
      <li>발현된 샘플이 평균 값과 일치하는 경우에 평균 값을 재설정하거나 공분산 값을 조절한다.</li>
    </ul>
  </li>
</ul>

<p><strong>식별 문제 (Problem of Identifiability)</strong></p>

<ul>
  <li>보통의 경우 \( p({\bf x}|\theta) \) 를 구하는데 있어서 \( \theta \neq \theta’ \) 인 경우 서로 다른 분포를 가지게 된다.</li>
  <li>하지만 <em>K</em> 개의 컴포넌트를 사용하는 혼합 분포의 경우 MLE 결과를 동일하게 만들어내는 솔루션을 \( K! \) 개 만들 수 있다.
    <ul>
      <li>이유는 파라미터를 \( K \) 컴포넌트에 할당할 수 있는 방법이 총 \( K! \) 개 이기 때문.</li>
      <li>따라서 임의의 한 샘플에 대해 동일한 혼합 분포 결과를 만들어낼 수 있는 샘플 수는 \( K!-1 \) 가 된다.</li>
      <li>이러한 문제를 식별성 문제라고 한다.</li>
    </ul>
  </li>
  <li>이후 연속 잠재 변수를 다룰 때 좀 더 살펴보기로 하자.</li>
</ul>

<h2 id="em--em-for-gaussian-mixtures">9.2.2. 가우시안 혼합 분포를 EM으로 처리하기 (EM for Gaussian mixtures)</h2>

<ul>
  <li>잠재 변수를 포함한 모델에서 MLE를 수행하기 위한 좋은 수단은 EM 알고리즘이다.</li>
  <li>우리는 이후 일반화한 EM 알고리즘에 대해 살펴보고 이를 변분 방식의 프레임워크로 확장할 것이다.</li>
  <li>그 시작으로 우선은 GMM 모델을 이용하여 EM 알고리즘을 적용하는 문제를 다룬다.
    <ul>
      <li>하지만 EM 알고리즘은 이렇게 제한적인 환경에서만 사용하는 모델은 아니고 다른 형태의 모델에서도 얼마든지 응용 가능하다.</li>
    </ul>
  </li>
  <li>가장 먼저 GMM 의 가능도 함수를 이용하여 식을 전개해 보도록 한다.
    <ul>
      <li>로그 가능도 함수 \( \ln p({\bf X} | \pi, {\bf \mu}, \Sigma) \) 를 미분하여 이 값이 0이 되는 때의 평균 값을 추정하도록 한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">0=-\sum_{n=1}^N \frac{\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)}{\sum_j \pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)}\Sigma_K^{-1}({\bf x}_n-{\bf \mu}_k) \qquad{(9.16)}</script>

<script type="math/tex; mode=display">\gamma(z_{nk})=\frac{\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)}{\sum_j \pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)}</script>

<ul>
  <li>양변에 \( \Sigma_k \) 를 곱하고 전개하면 (물론 이 때 \( \Sigma_k \) 는 nonsingular 즉, 역행렬이 존재한다고 가정한다.)</li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_k = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk}){\bf x}_n \qquad{(9.17)}</script>

<script type="math/tex; mode=display">N_k = \sum_{n=1}^N \gamma(z_{nk}) \qquad{(9.18)}</script>

<ul>
  <li>우리는 \( N_k \) 를 클러스터 \( k \) 에 대한 effective number 라고 고려할 수 있다.</li>
  <li>수식을 자세히 살펴보면 사실 \( k \) 번째 가우시안 분포의 평균 값인 \( {\bf \mu}_k \) 는 전체 샘플 데이터에 대한 평균 값을 취하고 있다는 것을 알 수 있다.
    <ul>
      <li>그럼에도 불구하고 하나의 클러스터에 국한된 평균 값으로 고려할 수 있는 이유는,</li>
      <li>사후 분포 \( \gamma(z_{nk}) \) 에 의해 데이터 하나가 \( k \) 번째 가우시안 분포에만 영향을 미치는 정도를 계산할 수 있기 때문이다.</li>
    </ul>
  </li>
  <li>마찬가지로 로그 가능도 함수를 \( \Sigma_k \) 에 대해 미분하여 이 값을 0으로 놓고 풀면,</li>
</ul>

<script type="math/tex; mode=display">\Sigma_k = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk})({\bf x}_n-{\bf \mu}_k)({\bf x}_n-{\bf \mu}_k)^T \qquad{(9.19)}</script>

<ul>
  <li>위의 결과를 살펴보면 기존의 1개의 가우시안 모델을 사용했을 때의 결과와 비슷한 형태에, 추가로 각 컴포넌트 별 가중치가 곱해진 형태로 식이 얻어진다.</li>
  <li>이제 로그 가능도 함수를 혼합 계수 \( \pi_k \) 에 대해 미분하여 풀면 되는데, 혼합 계수는 추가적인 제약이 존재한다.
    <ul>
      <li>따라서 로그 가능도 함수에 이러한 제약을 추가하여 라그랑지안 승수 방식으로 처리한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}|{\bf \pi}, {\bf \mu}, \Sigma)+\lambda\left(\sum_{k=1}^K \pi_k-1\right) \qquad{(9.20)}</script>

<script type="math/tex; mode=display">0 = \sum_{n=1}^{N} \frac{N({\bf x}_n|{\bf \mu}_k, \Sigma_k)}{\sum_j \pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)}+\lambda \qquad{(9.21)}</script>

<ul>
  <li>양변에 \( \pi_k \) 를 곱하면 \( \gamma(z_{nk}) \) 가 나오게 된다. 이를 전개하면</li>
</ul>

<script type="math/tex; mode=display">\pi_k = \frac{N_k}{N} \qquad{(9.22)}</script>

<ul>
  <li>결국 \( k \) 번째 혼합 계수는 responsibility 를 평균한 값이 된다.</li>
  <li>이렇게 얻어진 평균과 분산, 혼합 계수는 자명한 (closed-form) 해를 가지지 못하는데 모두 \( \gamma(z_{nk}) \) 에 영향을 받기 때문.
    <ul>
      <li>따라서 이를 풀기 위해서는 다른 방식의 접근이 필요하게 된다. (iterative 방식의 도입)</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>EM 알고리즘의 적용과정</strong></p>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure9.8a.png" alt="Figure 9.8a" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.8b.png" alt="Figure 9.8b" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.8c.png" alt="Figure 9.8c" height="180px" />
</div>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure9.8d.png" alt="Figure 9.8d" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.8e.png" alt="Figure 9.8e" height="180px" />
  <img src="/kyo-ml-study/images/Figure9.8f.png" alt="Figure 9.8f" height="180px" />
</div>

<ul>
  <li>EM 알고리즘은 반복적인(iterative) 방법을 이용하여 값을 추정하게 되는데 GMM 에서는 이를 다음과 같이 처리한다.
    <ul>
      <li>Init-Step
        <ul>
          <li>그림 \( (a) \) 와 같이 임의의 값으로 평균과, 분산을 초기화한다.</li>
        </ul>
      </li>
      <li>E-Step
        <ul>
          <li>현재 파라미터의 값을 사용하여 사후 확률 \( \gamma(z_{nk}) \) (responsibilities) 를 구한다.</li>
          <li>그림 \( (b) \) 에 해당.</li>
        </ul>
      </li>
      <li>M-Step
        <ul>
          <li>E-Step 에서 계산한 사후 확률 값을 이용하여 MLE를 통한 평균, 분산, 혼합 계수의 값을 다시 추정한다.</li>
          <li>그림 \( (c) \) 에 해당</li>
        </ul>
      </li>
      <li>iterative
        <ul>
          <li>수렴 조건이 만족할 때까지 E-Step 과 M-Step 을 계속 반복</li>
          <li>그림 \( (d), (e), (f) \) 에 해당.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>보통 EM 알고리즘이 <em>K</em>-means 알고리즘에 비해 반복 횟수가 많다.</li>
  <li>따라서 보통의 경우 EM 알고리즘을 수행하기 전에 <em>K</em>-means 알고리즘을 돌려 적절한 초기값을 설정한 뒤에 다시 수행하는 경우가 많다.</li>
  <li>앞서 설명한 대로 혼합 모델에서는 특이점 문제가 발생하지 않도록 주의를 기울여야 한다.
    <ul>
      <li>가우시안 분포에 한 점만 쏠리지 않도록 해야 한다.</li>
    </ul>
  </li>
  <li>EM 알고리즘은 전역 최적해(global solution)를 보장하지 않는다. 따라서 지역 최적해(local solution) 만을 얻을 수 있다.</li>
</ul>

<hr class="gray" />

<p><strong>가우시안 혼합 모델의 EM 적용 정리</strong></p>

<ul>
  <li>
    <p>가우시안 혼합 모델(GMM)이 주어진 경우 다음과 같은 순서로 수행.</p>
  </li>
  <li><strong>초기화 단계</strong>
    <ul>
      <li>평균 \( {\bf \mu}_k \) , 공분산 \( \Sigma_k \), 그리고 혼합 계수 \( \pi_k \) 를 적당한 값으로 초기화한다.</li>
    </ul>
  </li>
  <li><strong>E 단계</strong>
    <ul>
      <li>주어진 파라미터 값들을 이용하여 responsibility 값을 계산한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\gamma(z_{nk})=\frac{\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)}{\sum_j \pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)} \qquad{(9.23)}</script>

<ul>
  <li><strong>M 단계</strong>
    <ul>
      <li>주어진 responsibility 값을 이용하여 파라미터를 추정한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\bf \mu}_k^{new} = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk}){\bf x}_n \qquad{(9.24)}</script>

<script type="math/tex; mode=display">\Sigma_k^{new} = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk})({\bf x}_n-{\bf \mu}_k)({\bf x}_n-{\bf \mu}_k)^T \qquad{(9.25)}</script>

<script type="math/tex; mode=display">\pi_k^{new} = \frac{N_k}{N} \qquad{(9.26)}</script>

<script type="math/tex; mode=display">N_k = \sum_{n=1}^N \gamma(z_{nk}) \qquad{(9.27)}</script>

<ul>
  <li><strong>가능도 함수 평가</strong>
    <ul>
      <li>파라미터 혹은 가능도 함수의 값이 수렴했는지 살펴본다. 수렴하지 않은 경우 E 단계로 돌아가서 반복.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\ln p({\bf X}|{\bf \mu}, \Sigma, {\bf \pi}) = \sum_{n=1}^N \left\{\sum_{k=1}^K \pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)\right\} \qquad{(9.28)}</script>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoongithub'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-ml-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
