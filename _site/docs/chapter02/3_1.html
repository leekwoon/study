<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>3. The Gaussian Distribution [I]</title>
  <link rel="stylesheet" href="/kyo-ml-study/css/main.css">
  <link rel="stylesheet" href="/kyo-ml-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-ml-study/docs/chapter02/3_1.html">
  <script src="/kyo-ml-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-ml-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-ml-study/"><strong>kyo-ml-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <!-- for drop-down navi -->
            <li class="dropdown">
              <a href="/kyo-ml-study" class="dropdown-toggle" data-toggle="dropdown"><strong>Chapter
                  <b class="caret"></b></strong>
              </a>
              <ul class="dropdown-menu">
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter01/0">1. Introduction</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter02/0">2. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter03/0">3. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter04/0">4. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter05/0">5. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter06/0">6. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter07/0">7. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter08/0">8. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter09/0">9. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter10/0">10. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter11/0">11. Neural Networks</a>
                </li>
                
                
              </ul>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter01/0"><strong>1</strong></a>
            </li>
            
                      
          
          
            
            <li  class="active" >
              <a href="/kyo-ml-study/docs/chapter02/0"><strong>2</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter03/0"><strong>3</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter04/0"><strong>4</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter05/0"><strong>5</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">6</strong></a></li>
                      
          
          
            <li><a href="#"><strong class="text-danger">7</strong></a></li>
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter08/0"><strong>8</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter09/0"><strong>9</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter10/0"><strong>10</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">11</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 2</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/0">0. Probability Distributions</a>
      </li>
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/1">1. Binary Variables</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/2">2. Multinomial Variables</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/kyo-ml-study/docs/chapter02/3_1">3. The Gaussian Distribution [I]</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/3_2">3. The Gaussian Distribution [II]</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/3_3">3. The Gaussian Distribution [III]</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/4">4. The Exponential Family</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/5">5. Nonparametric Methods</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-ml-study/blob/gh-pages/docs/chapter02/3_1.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>3. The Gaussian Distribution [I]</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>가우시안 분포는 보통 정규분포(standard distribution)로 알려져있다.</li>
  <li>왜냐하면 연속 확률 분포 중 가장 널리 알려진 분포이기 때문이다.</li>
  <li>단일 변수 \( x \) 에 대해 가우시안 분포는 다음과 같이 기술된다.</li>
</ul>

<script type="math/tex; mode=display">N(x|\mu, \sigma^2) = \dfrac{1}{(2\pi\sigma^2)^{1/2}}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\} \qquad{(2.42)}</script>

<ul>
  <li>여기서 \( \mu \) 는 평균, \( \sigma^2 \) 은 분산이다.</li>
  <li>입력 변수가 \( D \) 차원의 벡터인 경우를 다변량 가우시안 분포라 하며 다음과 같은 식으로 기술한다.</li>
</ul>

<script type="math/tex; mode=display">N({\bf x}|{\pmb \mu}, {\bf \Sigma}) = \dfrac{1}{(2\pi)^{D/2}|{\bf \Sigma}|^{1/2}}\exp\left\{-\frac{1}{2}({\bf x}-{\pmb \mu})^T{\bf \Sigma}^{-1}({\bf x}-{\pmb \mu})\right\} \qquad{(2.43)}</script>

<ul>
  <li>여기서 \( {\pmb \mu} \) 는 \( D \) 차원의 평균 벡터이고, \( {\bf \Sigma} \) 는 \( D\times D \) 크기를 가지는 공분산 행렬이다.</li>
  <li>
    <p>참고로 \( |{\bf \Sigma}| \) 는 \( {\bf \Sigma} \) 의 행렬식(determinant)가 된다.</p>
  </li>
  <li>앞서도 살펴보았지만 연속형 확률 변수에서 가우시안 분포는 변수의 엔트로피를 최대화하는 분포이다.</li>
  <li>
    <p>가우시안 분포가 가지는 또다른 중요한 성질은 중심 극한 정리(<em>central limit theorem</em>)이다.</p>
  </li>
  <li><strong>중심 극한 정리</strong>
    <ul>
      <li>표본 평균(sample mean)들이 이루는 분포는 샘플 크기가 큰 경우 모집단의 원래 분포와 상관없이 가우시안 분포를 따름.</li>
      <li>동일한 확률 분포를 따르는 \( N \) 개의 독립 확률 변수의 평균 값은 \( N \) 이 충분이 크다면 가우시안 분포를 따름.</li>
      <li>\( N \) 개의 확률 변수가 어떤 확률 분포를 따르든지 상관없이 \( N \) 이 충분이 크다면 그 합은 가우시안 분포를 따름.</li>
      <li>문장을 잘 읽어볼 것. 표본의 값을 랜덤 변수로 놓는 것이 아니라 표본의 평균 값을 랜덤 변수로 놓는 것이다.</li>
    </ul>
  </li>
</ul>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure2.6a.png" alt="Figure 2.6a" height="150px" />
  <img src="/kyo-ml-study/images/Figure2.6b.png" alt="Figure 2.6b" height="150px" />
  <img src="/kyo-ml-study/images/Figure2.6c.png" alt="Figure 2.6c" height="150px" />
</div>

<ul>
  <li>위의 그림은 \( [0,1] \) 범위에서 균등분포(uniform distribution)를 가지는 분포에서 얻은 결과의 평균 값의 히스토그램을 그린 것이다.</li>
  <li>
    <p>\( N \) 이 커질수록 정규 분포의 모양을 만들는 것을 확인할 수 있다.</p>
  </li>
  <li>마찬가지로 이항 분포(binomial distribution) 에서 관찰값 \( N \) 개의 합 \( m \) 이 분포를 이루는데,
    <ul>
      <li>이 때 관찰값 \( N \) 이 \( N\to\infty \) 가 되면, 이 분포는 결국 가우시안 분포가 된다.</li>
    </ul>
  </li>
  <li>가우시안 분포는 매우 중요하고 분석해볼만한 중요한 요소들을 포함하고 있다.</li>
  <li>우리는 이번 장에서 이를 꼼꼼하게 살펴볼 것이다. (1장에 비해 기술적인 내용이 더 나온다.)</li>
  <li>이후 장에서 이를 응용한 내용들이 많이 등장하니까 여기서 확실하게 익히고 가야 한다.</li>
</ul>

<hr />

<ul>
  <li>가우시안 분포의 기하학적인 형태를 살펴보자.</li>
  <li>\( x \) 에 대한 가우시안의 함수적 종속성은 \( exp \) 지수부에 등장하는 이차형식(<code class="highlighter-rouge">quadratic</code>)에 있음.
    <ul>
      <li>이번 기회에 아래 식을 자세히 좀 보도록 하자.</li>
      <li>이 식이 중요한 이유는, 이후에 자주 등장할 가우시안 분포 수식의 구조 때문이다.</li>
      <li>이후로 계속 가우시안 분포에 로그를 취하는 형태가 등장하게 될 터인데 지수부의 식이 로그로 인해 자주 사용되게 됨.
        <ul>
          <li>\( \ln e^{(a)} \equiv a \)</li>
        </ul>
      </li>
      <li>자주 등장할 터이니 여기서 외우려고 노력 할 필요는 없다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\Delta^2 = ({\bf x}-{\pmb \mu})^T{\bf \Sigma}^{-1}({\bf x}-{\pmb \mu}) \qquad{(2.44)}</script>

<ul>
  <li>여기서 \( \Delta \) 를 \( \mu \) 에서 \( x \) 까지의 마할라노비스 거리(<em>Mahalanobis distance</em>) 라고 부른다</li>
  <li>만약 공분산이 단위 행렬(identity matrix) \( I \) 인 경우 이 값은 유클리디안 거리(Euclidean distance)와 동일해진다.</li>
  <li>결국 이 값은 평균과의 거리를 측정할 때 분산도를 고려한다는 의미가 된다.</li>
  <li>\( x \) 공간에 놓인 가우시안 함수의 표면은 모두 동일한 상수 값을 가지게 되는데, (컨투어를 그리면 함수의 결과값이 동일함)</li>
  <li>이는 위의 이차식의 결과 값이 마찬가지로 동일한 상수 값이 되기 때문이다.</li>
  <li>무엇보다도 중요한 점은 공분산 행렬 \( {\bf \Sigma} \) 가 대칭행렬(<em>symmetric matrix</em>)이라는 점이다.</li>
  <li>이로 인해 지수(exponent) 연산에 의해서 비대칭적인 요소가 모두 사라진다. (종모양의 좌우 대칭 함수가 된다.)</li>
  <li>공분산이 대칭행렬이므로 이 행렬에 대해 아주 잘 알려진 고유벡터(eigen-vector) 식을 적용할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">{\bf \Sigma}{\bf u}_i = \lambda_i {\bf u}_i \qquad{(2.45)}</script>

<ul>
  <li>교재에서는 직접적으로 언급하지 않고 있지만, 고유(eigen) 벡터에 관련된 식을 좀 이해하고 있어야 한다.</li>
  <li>따라서 다른 교재를 좀 참고하기 바란다. 참고로 간단히만 언급하면,
    <ul>
      <li>고유식은 오로지 정방 행렬 ( \( N\times N \) ) 에만 적용가능하다.</li>
      <li>\( N\times N \) 크기의 정방 행렬에서는 최대 \( N \) 개의 고유벡터, 고유값을 얻을 수 있다.</li>
      <li>\( N\times N \) 크기의 대칭 행렬의 경우 \( N \) 개의 고유벡터를 얻을 수 있다.</li>
    </ul>
  </li>
  <li>따라서 위의 식은 공분산 행렬의 고유벡터, 고유값을 정의한 식이 된다.</li>
  <li>공분산은 대칭 행렬이며 실수 값(real number)만 가진다.
    <ul>
      <li>굳이 실수임을 강조하는 이유는 허수 값을 가지는 행렬에 대한 고유식도 존재하기 때문이다.</li>
      <li>그러나 여기서 허수를 고려한 고유벡터까지 고려할 필요는 없다.</li>
    </ul>
  </li>
  <li>공분산 행렬에 대한 모든 고유 벡터는 단위 직교(orthonormal)한다. (공분산 행렬이 대칭 행렬이므로)
    <ul>
      <li>따라서 위에서 언급한 식 중 \( {\bf u}_i \) 는 단위 직교 벡터를 의미한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\bf u}_i^T{\bf u}_j=I_{ij} \qquad{(2.46)}</script>

<script type="math/tex; mode=display">% <![CDATA[
I_{ij}=\left\{\begin{array}{lr}1 & if\;i=j\\0 & otherwise\end{array}\right. \qquad{(2.47)} %]]></script>

<ul>
  <li>이제 공분산 행렬을 고유벡터로 표현해보자.
    <ul>
      <li>‘고유’ 라는 표현에서도 알 수 있듯이 선형 변환 후에도 변하지 않는 성질과 직교 벡터의 성질로 인해,</li>
      <li>공분산 행렬을 고유 벡터의 선형 결합 형태로 표현 가능하다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">{\bf \Sigma}=\sum_{i=1}^{D}{\lambda_i}{\bf u}_i{\bf u}_i^T \qquad{(2.48)}</script>

<ul>
  <li>역행렬도 쉽게 구할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">{\bf \Sigma}^{-1}=\sum_{i=1}^{D}\dfrac{1}{\lambda_i}{\bf u}_i{\bf u}_i^T \qquad{(2.49)}</script>

<ul>
  <li>이제 맨 위의 <code class="highlighter-rouge">quadratic</code> 식에 이 값을 대입하면 다음과 같은 식이 된다.</li>
</ul>

<script type="math/tex; mode=display">\Delta^2 = \sum_{i=1}^{D}\frac{y_i^2}{\lambda_i} \qquad{(2.50)}</script>

<script type="math/tex; mode=display">y_i={\bf u}_i^T({\bf x}-{\pmb \mu}) \qquad{(2.51)}</script>

<ul>
  <li>이것을 벡터식으로 확장할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">{\bf y} = {\bf U}({\bf x}-{\pmb \mu}) \qquad{(2.52)}</script>

<ul>
  <li>이제 \( {\bf y} \) 를 어떻게 해석해야 할까?
    <ul>
      <li>위의 식으로 \( {\bf y} \) 를 \( {\bf x} \) 에 대해 새로운 좌표로 변환한 좌표로 볼 수있다.</li>
      <li>아마 고등학교 즈음해서 벡터에 대한 변환식을 배웠던 기억이 있을 것이다.</li>
      <li>결국 이 식은 가우시안 함수의 원점을 \( {\pmb \mu} \) 로 옮기고 고유 벡터를 축으로 회전 변환되는 식이 된다.</li>
      <li>\( {\bf U} \) 는 직교 행렬이고 \( {\bf U}^T{\bf U}=I \), \( {\bf U}{\bf U}^T=I \) 를 만족한다. ( \( I \) 는 단위행렬)</li>
    </ul>
  </li>
  <li>사실 이러한 변환을 주축 변환이라고 한다. 선형대수 교재에 많이 나온다.</li>
</ul>

<p><img src="/kyo-ml-study/images/Figure2.7.png" alt="figure2.7" class="center-block" height="200px" /></p>

<ul>
  <li>그림 설명
    <ul>
      <li>그림에서 사용되는 데이터는 2차원 데이터로 \( (x_1, x_2) \) 이다.</li>
      <li>붉은 선은 타원 표면(surface)으로 동일한 확률 값을 가지는 위치가 된다. (이를 컨투어라고 한다.)</li>
      <li>\( {\bf u} \) 는 고유 벡터로 정의되며 이 축으로 타원이 형성된다.</li>
      <li>이 때 고유벡터를 축으로 하는 타원에 대한 공분산은 고유값 \( \lambda \) 의 영향을 받는다.</li>
      <li>그림을 보면 쉽게 이해될 것이다.</li>
    </ul>
  </li>
  <li>그림에서 붉은 선처럼 동일한 상수 확률값을 가지는 위치에서 \( \Delta^2=\sum_D\frac{y_i^2}{\lambda_i} \) 식도 동일한 상수 값을 가지게 된다.</li>
  <li>만약 고유값 \( \lambda_i \) 가 모두 양수라면 컨투어는 타원(ellipsoid) 형태가 된다.
    <ul>
      <li>위의 그림이 이를 잘 나타내고 있다. (즉, 가우시안에서는 고유값이 모두 양수임)</li>
      <li>이 때에는 타원의 중심이 \( {\pmb \mu} \) 가 되고 고유값 \( \lambda_i^{1/2} \) 스케일로 타원이 형성되게 된다.</li>
    </ul>
  </li>
  <li>가우시안 분포의 경우 이에 대한 정의가 명확한데,
    <ul>
      <li>공분산에 대한 고유값은 모두 양수여야만 한다. (0보다 모두 커야 한다.)</li>
      <li>만약 모두 양수가 아닌 경우를 정규화되지 못했다고 하며 이런 경우를 12장에서 좀 다루고 있다. (0이 되는 값)</li>
      <li>모든 고유값이 양수이면 이런 행렬을 양의 정부호 행렬(positive definite matrix)이라고 한다.
        <ul>
          <li>선형 대수에서는 이런 성질을 주요하게 다루고 있는데, 양의 정부호를 만족하는 경우 할 수 있는 것들이 많기 때문이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이번 절에서는 여기까지만 들어가도록 하자.
    <ul>
      <li>이와 관련된 내용은 선형 대수 책에서 더 자세하게 설명하고 있다.</li>
      <li>다만 여기서는 일반적인 대칭 행렬에 대한 고유식 속성을 공분산에 적용한 것 뿐이다.</li>
      <li>궁금하신 분들은 선형 대수 책을 좀 더 찾아보도록 하자.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>이제 가우시안 분포를 \( {\bf y} \) 좌표로 전환하는 것에 대해 좀 살펴보자.</li>
  <li>\( {\bf x} \) 좌표계에서 \( {\bf y} \) 좌표계로 전환하는 것은 야코비안(Jacobian) 행렬 \( {\bf J} \) 를 이용한다.
    <ul>
      <li>이 식을 야코비안으로 발음할지 쟈코비안으로 발음할지 애매하기는 하다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">J_{ij}=\dfrac{\partial x_i}{\partial y_i}=U_{ji} \qquad{(2.53)}</script>

<ul>
  <li>야코비안이 좌표 축 변환을 만들어 낼 때 어떻게 변화하는지 좀 알아야 하는데, 여기서는 간단하게 다음의 성질만을 기술해본다.
    <ul>
      <li>아주 간단하게만 이야기하자면 공간의 선형 변환시 발생되는 부피의 변화율을확률  식에 반영하자는 것.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\int_{\bf x} f({\bf x})d{\bf x} = \int_{\bf y} f({\bf y})|{\bf J}|d{\bf y}</script>

<ul>
  <li>식을 전개하기 앞서 필요한 식들을 좀 정리하자. 이미 앞서서 \( {\bf y} = {\bf U}({\bf x}-{\pmb \mu}) \) 는 확인을 했다.</li>
</ul>

<script type="math/tex; mode=display">{\vert}{\bf J}{\vert}^2 = {\vert}{\bf U}^T{\vert}^2 = {\vert}{\bf U}^T{\vert}\;{\vert}{\bf U}{\vert} = {\vert}{\bf U}^T{\bf U}{\vert} = {\vert}{\bf I}{\vert} = 1 \qquad{(2.54)}</script>

<ul>
  <li>\( U \) 는 직교 행렬이므로 식을 전개하면 결국 \( |J|=1 \) 을 얻게 된다.</li>
  <li>이제 \( \left|\Sigma\right| \) 값을 구하면,</li>
</ul>

<script type="math/tex; mode=display">\left|\Sigma\right|^{\frac{1}{2}}=\prod_{j=1}^{D}\lambda_j^{\frac{1}{2}} \qquad{(2.55)}</script>

<ul>
  <li>이제 식을 \( x \) 축에서 \( y \) 축으로 전환하여 본다. \( {\bf y} = {\bf U}({\bf x}-{\pmb \mu}) \) 을 대입하고 기타 식들을 추가하면,</li>
</ul>

<script type="math/tex; mode=display">p({\bf y}) = p(x)|{\bf J}| = \prod_{j=1}^{D}\dfrac{1}{(2\pi\lambda_j)^{1/2}}\exp\left\{-\dfrac{y_j^2}{2\lambda_j}\right\} \qquad{(2.56)}</script>

<ul>
  <li>식을 잘 살펴보면 서로 독립적인 \( D \) 개의 정규 분포의 확률 값이 단순 곱으로 이루어져 있다는 것을 알 수 있다.</li>
  <li>고유 벡터를 이용해서 축을 변환시켜 얻은 식은 결국 차원간 서로 독립적인 정규 분포를 만들어낸다.</li>
  <li>이 식을 적분해보자.</li>
</ul>

<script type="math/tex; mode=display">\int p({\bf y})d{\bf y} = \prod_{j=1}^{D} \int_{-\infty}^{\infty}\dfrac{1}{(2\pi\lambda_j)^{1/2}}\exp \left\{-\dfrac{y_j^2}{2\lambda_j}\right\}dy_i=1 \qquad{(2.57)}</script>

<ul>
  <li>역시나 확률 값이므로 각각의 차원에 대해 전구간 적분하면 크기가 1이고, 이를 \( D \) 차원만큼 곱해도 여전히 결과는 1이다.
    <ul>
      <li>이건 확률식이라 당연하다.</li>
      <li>실제 얻어진 결과는 위의 그림에서 \( {\bf y} \) 축을 대상으로 회전 변환한 식이라 상상해보자.</li>
    </ul>
  </li>
  <li>이제 가우시안 분포의 적률(moment)을 좀 살펴보도록 하자.
    <ul>
      <li>참고로 적률(moment)은 고전 통계학에서 사용되었던 파라미터이다.</li>
    </ul>
  </li>
  <li>\( {\bf x} \) 축에 대해 평균값을 살펴볼 예정인데 우선 식 전개를 편하게 하기 위해 \( {\bf z} = ({\bf x}-{\pmb \mu}) \) 를 놓고 식을 전개한다.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf x}] = \dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\int\exp\left\{-\frac{1}{2}({\bf x}-{\pmb \mu})^T{\bf \Sigma}^{-1}({\bf x}-{\pmb \mu})\right\}{\bf x}d{\bf x}\\
= \dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\int\exp\left\{-\frac{1}{2}({\bf z})^T{\bf \Sigma}^{-1}({\bf z})\right\}({\bf z}+{\pmb \mu})d{\bf z} \qquad{(2.58)}</script>

<ul>
  <li>위의 식은 \( {\bf z} \) 에 의해 좌우 대칭인 함수가 만들어진다.</li>
  <li>여기에 \( ({\bf z}+{\pmb \mu}) \) 식이 추가되어 있으므로 \( {\pmb \mu} \) 만큼 평행이동한 함수이다.</li>
  <li>따라서 중심이 \( {\pmb \mu} \) 이고 좌우 대칭인 정규 함수가 만들어진다. 따라서 평균은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf x}]={\pmb \mu} \qquad{(2.59)}</script>

<ul>
  <li>이제 2차 적률(second order moments)에 대해 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf x}{\bf x}^T]=\dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\int\exp\left\{-\frac{1}{2}({\bf x}-{\pmb \mu})^T{\bf \Sigma}^{-1}({\bf x}-{\pmb \mu})\right\}{\bf x}{\bf x}^Td{\bf x}\\
= \dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\int\exp\left\{-\frac{1}{2}({\bf z})^T{\bf \Sigma}^{-1}({\bf z})\right\}({\bf z}+{\pmb \mu})({\bf z}+{\pmb \mu})^Td{\bf z}</script>

<ul>
  <li>여기서 \( ({\bf z}+{\pmb \mu})({\bf z}+{\pmb \mu})^T \) 를 전개할 수 있다.</li>
  <li>이를 전개한 수식에서 \( {\pmb \mu}{\bf z}^T \) 와 \( {\bf z}{\pmb \mu}^T \) 는 서로 대칭 관계이므로 제거된다.</li>
  <li>\( {\bf u}{\bf u}^T \) 는 수식에서 상수의 역할이므로 적분 바깥 쪽으로 나오게 된다.</li>
  <li>결국 우리가 집중해야 할 요소는 \( {\bf z}{\bf z}^T \) 이다.</li>
  <li>
    <p>참고로 \( {\bf z} \) 는 다음과 같이 고유벡터로 표현 가능하다.
<script type="math/tex">{\bf z}=\sum_{j=1}^{D}y_j{\bf u}_j \qquad{(2.60)}</script></p>
  </li>
  <li>여기서 \( y_j={\bf u}_j^T{\bf z} \) 이다.</li>
  <li>따라서 식을 다음과 같이 전개 가능하다.</li>
</ul>

<script type="math/tex; mode=display">\dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\int\exp\left\{-\frac{1}{2}({\bf z})^T{\bf \Sigma}^{-1}({\bf z})\right\}{\bf z}{\bf z}^Td{\bf z}</script>

<script type="math/tex; mode=display">=\dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\sum_{i=1}^{D}\sum_{j=1}^{D}{\bf u}_i{\bf u}_j^T\int \exp \left\{-\sum_{k=1}^{D}\frac{y_k^2}{2\lambda_k}\right\}y_iy_jd{\bf y}\\
=\sum_{i=1}^{D}{\bf u}_i{\bf u}_j^T\lambda_i=\Sigma \qquad{(2.61)}</script>

<ul>
  <li>원 식에 대입하면 다음과 같은 결과를 얻는다.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf x}{\bf x}^T]={\pmb \mu}{\pmb \mu}^T + \Sigma \qquad{(2.62)}</script>

<ul>
  <li>뭐, 원래 알고 있던 2차 적률 값이다. (평균의 제곱과 분산의 합)</li>
  <li>이제 공분산(covariance) 값도 한번 구해보자.</li>
  <li>\( E[{\bf x}]={\pmb \mu} \) 이므로 어차피 동일한 결과를 얻게 된다.</li>
</ul>

<script type="math/tex; mode=display">cov[{\bf x}]=E[({\bf x}-E[{\bf x}])({\bf x}-E[{\bf x}])^T] \qquad{(2.63)}</script>

<script type="math/tex; mode=display">cov[{\bf x}]=\Sigma \qquad{(2.64)}</script>

<hr />

<ul>
  <li>
    <p>가우시안 모델은 정말 널리 사용되는 모델이지만 몇 가지 제약들을 가지고 있다.</p>
  </li>
  <li>
    <p><strong>제약(1) : 모수(parameter)의 개수</strong></p>
    <ul>
      <li>\( D \) 개의 차원을 가진 데이터에서 총 \( D(D+3)/2 \) 개의 독립적인 파라미터를 가지게 된다.
        <ul>
          <li>\( \mu \) 파라미터 \( D \) 개, 공분산은 \( D(D+1)/2 \) 개를 가지게 된다. (공분산은 대칭 행렬임)</li>
        </ul>
      </li>
      <li>따라서 \( D \) 가 증가하게 되면 차수에 대해 제곱에 비례하여 모수의 개수가 증가하게 된다.</li>
      <li>이런 경우 여러가지 계산이 어려워지고 공분산의 역행렬 등을 구하기가 어려워 진다.</li>
      <li>가능한 대안점들
        <ul>
          <li>공분산을 대각행렬(diagonal matrix)로만 제한한다.
            <ul>
              <li>\( \Sigma=diag(\sigma^2) \) 형태로만 사용함.</li>
              <li>이런 경우 총 \( 2D \) 개의 독립적인 파라미터를 가지게 된다.</li>
              <li>컨투어는 각 축들에 평행한 타원으로만 주어지게 된다.</li>
            </ul>
          </li>
          <li>공분산을 단위행렬(I)에 비례하도록 제한
            <ul>
              <li>\( \Sigma=\sigma^2\cdot I \) 형태로만 사용함.</li>
              <li>이런 경우 총 \( D+1 \) 개의 독립적인 파라미터를 가지게 된다.</li>
              <li>이런 형태의 공분산을 등방성 공분산(isotropic covariance)이라고 부름</li>
              <li>밀도가 구(concentric circle)의 형태를 취하게 된다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure2.8a.png" alt="Figure 2.8a" height="150px" />
  <img src="/kyo-ml-study/images/Figure2.8b.png" alt="Figure 2.8b" height="150px" />
  <img src="/kyo-ml-study/images/Figure2.8b.png" alt="Figure 2.8c" height="150px" />
</div>

<ul>
  <li>(a) 는 일반적인 2차원 가우시안 분포</li>
  <li>(b) 는 공분산이 대각 행렬인 2차원 가우시안 분포</li>
  <li>(c) 는 공분산이 등방성 공분산인 2 차원 가우시안 분포</li>
  <li>모수 개수의 자유도를 제한하는 접근 형태는 공분산을 빠르게 계산할 수 있기 때문에 좋은 성능을 가짐</li>
  <li>
    <p>하지만 확률 밀도의 형태를 제한하기 때문에 데이터 사이의 연관 관계를 파악할 수 있는 능력을 제한하게 된다.</p>
  </li>
  <li><strong>제약(2) : 분포의 모양이 단봉(unimodal)의 형태만 올 수 있음</strong>
    <ul>
      <li>봉우리가 하나만 생기는 형태의 분포만을 근사할 수 있음.</li>
      <li>따라서 다봉(multimodal) 형태를 취하는 확률 분포를 근사할 수 없다.</li>
      <li>가능한 대안점들
        <ul>
          <li>이후에 살펴볼 Latent, Hidden, Un-observed 변수들을 사용해서 이런 문제를 해결할 수 있다. (이후에 등장할 것임)</li>
          <li>가우시안 분포 여러 개를 혼합(mixture)하여 만드는 모델도 이후에 언급할 것이다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>계층 모델(hierarchical model)을 이용하여 좀 더 좋음 모델을 제시할 수 있다. 예를 들어,
    <ul>
      <li>MRF (Markov Random Field) :
        <ul>
          <li>이미지 처리를 위한 확률 모델로 많이 사용됨</li>
          <li>픽셀의 공간적 구성을 반영한 구조를 도입하여 쉽게 랜더링할 수 있는 장점이 있다.</li>
        </ul>
      </li>
      <li>Linear Dynmaic System
        <ul>
          <li>시계열 데이터 모델링</li>
          <li>매우 큰 관측 모델과 잠재 변수(latent variable)의 사용</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>8장에서 이와 관련된 내용을 좀 다룰 것이다.</li>
</ul>

<h2 id="conditional-gaussian-distributions">2.3.1. 조건부 가우시안 분포 (Conditional Gaussian distributions)</h2>
<ul>
  <li>다차원 가우시안 확률 분포의 중요한 특징을 살펴보자.</li>
  <li>스포일러를 조금 하자면,
    <ul>
      <li>두개의 확률 변수의 결합 확률 분포가 가우시안이면, 조건부 확률 분포도 가우시안 분포가 된다.</li>
      <li>이 때 두 개의 주변(marginal) 확률 분포도 가우시안 분포가 된다.</li>
    </ul>
  </li>
  <li>조건부 분포의 경우를 먼저 살펴보자.
    <ul>
      <li>즉, 가우시안 분포를 따르는 두 개의 변수에 대해 조건부 분포가 어떤 분포를 따르게 되는지를 증명하여 볼 것이다.</li>
    </ul>
  </li>
  <li>\( D \) 차원을 가진 어떤 입력 벡터 \( {\bf x} \) 가 있고, 이 데이터는 \( N({\bf x}\;|\; {\pmb \mu}, \Sigma) \) 분포를 따른다고 하자.</li>
  <li>이 때 입력 벡터 \( {\bf x} \) 를 두개의 집합으로 나누면 다음과 같이 표기 가능하다.</li>
</ul>

<script type="math/tex; mode=display">{\bf x}=\dbinom{ {\bf x}_a }{ {\bf x}_b} \qquad{(2.65) }</script>

<ul>
  <li>이러면 \( a \) 는 \( M \) 개의 차원을 가지게 되고, \( b \) 는 \( D-M \) 개의 차원을 가지게 된다.</li>
  <li>이 때 대응되는 평균은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">\mu=\dbinom{ {\pmb \mu}_a }{ {\pmb \mu}_b } \qquad{(2.66)}</script>

<ul>
  <li>공분산도 표시해보자.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\Sigma=\left(\begin{array}{cc}\Sigma_{aa} & \Sigma_{ab}\\ \Sigma_{ba} & \Sigma_{bb}\end{array}\right) \qquad{(2.67)} %]]></script>

<ul>
  <li>위의 식이 처음에는 이해가 안될수도 있는데(나만 그런건가?), 잘 생각해보면 잘 들어 맞는다.</li>
  <li>공분산은 대칭행렬이므로 \( \Sigma^T=\Sigma \) 임을 잊지 말자. 따라서 \( \Sigma_{ba}=\Sigma_{ab}^T \) 이다.</li>
  <li>많은 경우 공분산을 그대로 사용하기 보다 공분산의 역행렬을 사용하기도 한다.
    <ul>
      <li>수식 전개를 할 때 훨씬 편리하기 때문</li>
      <li>따라서 여기서도 간단하게 공분한의 역행렬, 즉 정확도 행렬(precision matrix)을 정의하도록 한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\Lambda=\Sigma^{-1} \qquad{(2.68)}</script>

<ul>
  <li>이제 공분산 행렬의 관계식을 정확도 행렬을 통해 다시 정리해보자.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\Lambda = \left(\begin{array}{cc}\Lambda_{aa} & \Lambda_{ab}\\ \Lambda_{ba} & \Lambda_{bb}\end{array}\right) \qquad{(2.69)} %]]></script>

<ul>
  <li>마찬가지로 \( \Lambda_{ab}^T=\Lambda_{ba} \) 가 성립한다.</li>
  <li>단, \( \Lambda_{aa} \) 가 단순하게 \( \Sigma_{aa} \) 의 역행렬을 구하면 얻어지는 것은 아니다.
    <ul>
      <li>공분산의 부분 행렬의 역행렬이 부분 행렬의 정확도 행렬이라고는 할 수 없음. 혼동하지 말것.</li>
    </ul>
  </li>
  <li>
    <p>우리가 찾고자하는 식은 \( p({\bf x}_a | {\bf x}_b) \) 에 대한 확률 분포이다.</p>
  </li>
  <li>
    <p>여기서 현재의 결합 확률 분포는 \( p({\bf x})=p({\bf x}_a, {\bf x}_b) \) 로 나타낼 수 있다.</p>

    <ul>
      <li>만약 여기서 \( {\bf x}_b \) 가 고정된 관찰값이라고 가정하면 이 때의 \( {\bf x}_a \) 에 대한 확률 분포를 예측해볼 수 있다.
        <ul>
          <li>조건부 분포를 구하기 위해서는 당연한 가정이다.</li>
        </ul>
      </li>
      <li>좀 더 효율적으로 계산 결과를 얻어내기 위해 가우시안 분포에서 지수의 이차형식(quadratic) 부분을 살펴보도록 한다.</li>
      <li>앞서 언급했던 아래 식을 의미한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\Delta^2 = ({\bf x}-{\pmb \mu})^T{\bf \Sigma}^{-1}({\bf x}-{\pmb \mu})</script>

<ul>
  <li>식을 전개해보자.</li>
</ul>

<script type="math/tex; mode=display">-\dfrac{1}{2}({\bf x}-{\pmb \mu})^T\Sigma^{-1}({\bf x}-{\pmb \mu})=\\
-\dfrac{1}{2}({\bf x}_a-{\pmb \mu}_a)^T\Lambda_{aa}({\bf x}_a-{\pmb \mu}_a) - \dfrac{1}{2}({\bf x}_a-{\pmb \mu}_a)^T\Lambda_{ab}({\bf x}_b-{\pmb \mu}_b)\\
-\dfrac{1}{2}({\bf x}_b-{\pmb \mu}_b)^T\Lambda_{ba}({\bf x}_a-{\pmb \mu}_a) - \dfrac{1}{2}({\bf x}_b-{\pmb \mu}_b)^T\Lambda_{bb}({\bf x}_b-{\pmb \mu}_b) \qquad{(2.70)}</script>

<ul>
  <li>앞서 \( {\bf x}_b \) 는 특정한 값으로 고정되어 있다고 생각한다고 이야기 했다.</li>
  <li>따라서 위의 식은 그냥 \( {\bf x}_a \) 에 대한 식으로 고려할 수 있는데 마찬가지로 이차형식(<code class="highlighter-rouge">quadratic</code>)이 된다.</li>
  <li>이 식을 통해 결국 \( p({\bf x}_a|{\bf x}_b) \) 는 가우시안 분포를 따르게 될 것임을 짐작할 수 있다.
    <ul>
      <li>일반적인 가우시안 분포가 \( \exp(\cdot) \) 의 지수부에 이차형식(<code class="highlighter-rouge">quadratic</code>)의 식을 가지고 있으며, 별도의 상수부는 \( \exp(\cdot) \) 외부로 나올 수 있다.</li>
      <li>따라서 위의 식은 다시 정리를 해봤자 가우시안 분포의 형태가 나오게 될 것이다.</li>
    </ul>
  </li>
  <li>실제 이것이 가우시안 분포를 따른다고 하면 평균과 공분산을 구할 수 있어야 할 것이다.
    <ul>
      <li>이걸 한번 구해 보도록 하자.</li>
    </ul>
  </li>
  <li>좀 더 직관적으로 문제를 해결하기 위해서 일반적인 가우시안 분포에서의 이차형식(<code class="highlighter-rouge">quadratic</code>) 영역의 구조를 좀 살펴보자.
    <ul>
      <li>혹시나 해서 적어놓는데 이차형식(<code class="highlighter-rouge">quadratic</code>)의 정의를 잘 모르겠으면 선형 대수를 참고하도록 하자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">-\dfrac{1}{2}({\bf x}-{\pmb \mu})^T\Sigma^{-1}({\bf x}-{\pmb \mu})=-\dfrac{1}{2}{\bf x}^T\Sigma^{-1}{\bf x} + {\bf x}^T\Sigma^{-1}{\pmb \mu}+const \qquad{(2.71)}</script>

<ul>
  <li>위와 같은 전개 방식을 <em>completing the square</em> 라고 부른다.
    <ul>
      <li>가우시안 분포와 관련된 일반적인 작업 중 하나이다.</li>
      <li>\( \exp(\cdot) \) 내의 이차형식(quadratic)을 이용해서 평균(mean)과 공분산(covariance)을 구할 수 있다.</li>
    </ul>
  </li>
  <li>전개 식을 살펴보자. <em>const</em>  영역은 사실 \( {\bf x} \) 와는 독립적인 영역이다.</li>
  <li>공분산 \( \Sigma \) 는 대칭 행렬임을 잘 알고 있다.</li>
  <li>자, 이제 일반적인 가우시안 분포에서 이차 형식(<code class="highlighter-rouge">quadratic</code>) 내의 값들로부터 어떻게 평균과 분산이 연관되어 있는지 확인해본다.
    <ul>
      <li>위의 식에서 이차 형식을 일반화시켜 전개한 부분이 우변이다.</li>
      <li>이걸 두번 미분한 값은 (즉, 2차식의 계수에 주목해야 한다.) 바로 \( \Sigma^{-1} \) 이 된다.
        <ul>
          <li>이를 통해 공분산의 역행렬을 구할 수 있다. (즉, 두번 미분해서 얻은 행렬이 공분산이 되어야 한다.)</li>
        </ul>
      </li>
      <li>다음으로 1차식의 계수는 \( \Sigma^{-1}{\pmb \mu} \) 가 된다.
        <ul>
          <li>따라서 공분산을 구한 다음 1차 계수를 이용해서 평균의 벡터를 구할 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이제 조건부 분포에서의 평균과 분산을 구해보도록 하자.</li>
  <li>\( {\bf x}_a \) 와 \( {\bf x}_b \) 로 표현된 이차 형식 영역을 \( {\bf x}_a \) 에 대해 두번 미분한다고 할 때 의미가 있는 영역은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">-\dfrac{1}{2}{\bf x}_a^T\Lambda_{aa}{\bf x}_a \qquad{(2.72)}</script>

<ul>
  <li>\( {\bf x}_b \) 와 관련된 영역은 모두 상수 취급하면 된다.</li>
  <li>자, 이제 두번 미분한 값이 공분산의 역행렬이 되어야 하므로 조건부 분포 \( p({\bf x}_a|{\bf x}_b) \) 에 대한 공분산은 다음과 같이 정의할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">\Sigma\_{a|b} = \Lambda_{aa}^{-1} \qquad{(2.73)}</script>

<ul>
  <li>이제 당연히 \( {\bf x}_a \) 의 1차식과 관련된 텀들을 모아 이 계수를 확인해야 한다. (평균을 구하기 위해)</li>
</ul>

<script type="math/tex; mode=display">{\bf x}_a^T\{\Lambda_{aa}{\pmb \mu}_a - \Lambda({\bf x}_b-{\pmb \mu}_b)\} \qquad{(2.74)}</script>

<ul>
  <li>
    <p>\( \Lambda_{ba}^T=\Lambda_{ab} \) 임을 이미 알고 있다.</p>
  </li>
  <li>위의 식은 결국 \( \Sigma_{a|b}^{-1} \; {\pmb \mu}_{a|b} \) 와 같아져야 한다.</li>
  <li>식을 전개하면 다음과 같게 된다.</li>
</ul>

<script type="math/tex; mode=display">\Sigma_{a|b}^{-1}\; {\pmb \mu}_{a|b} = \{\Lambda_{aa}{\pmb \mu}_a - \Lambda({\bf x}_b-{\pmb \mu}_b)\}</script>

<script type="math/tex; mode=display">{\pmb \mu}_{a|b} = \Sigma_{a|b}\{\Lambda_{aa}{\pmb \mu}_a - \Lambda_{ab}({\bf x}_b-{\pmb \mu}_b)\}\\
= {\pmb \mu}_a - \Lambda_{aa}^{-1}\Lambda_{ab}({\bf x}_b-{\pmb \mu}_b) \qquad{(2.75)}</script>

<ul>
  <li>다 괜찮은데 한 가지 걸리는 부분은 현재 평균과 공분산이 정확도(precision) 행렬로 표현이 되었다는 것이다.</li>
  <li>최종 출력시에는 공분산으로 표현을 해 놓는 것도 좋을 듯 하다.</li>
  <li>이를 위해 역행렬을 구하는 다음 식을 고려해보자.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\left(\begin{array}{cc}A & B \\ C & D \end{array}\right)^{-1} = \left(\begin{array}{cc} M & -MBD^{-1} \\ -D^{-1}CM & D^{-1}CMBD^{-1} \end{array}\right) \qquad{(2.76)} %]]></script>

<ul>
  <li>이 중 \( M^{-1} \) 은 <em>Schur completment</em> 라고 불리는 행렬을 의미한다.</li>
</ul>

<script type="math/tex; mode=display">M = (A-BD^{-1}C)^{-1} \qquad{(2.77)}</script>

<ul>
  <li>이 식을 아래 행렬에 대입해서 풀 수 있다.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{pmatrix} \Sigma_{aa} & \Sigma_{ab} \\ \Sigma_{ba} & \Sigma_{bb} \end{pmatrix}^{-1} = \begin{pmatrix} \Lambda_{aa} & \Lambda_{ab} \\ \Lambda_{ba} & \Lambda_{bb} \end{pmatrix} \qquad{(2.78)} %]]></script>

<ul>
  <li>전개하면 다음과 같은 식을 얻게 된다.</li>
</ul>

<script type="math/tex; mode=display">\Lambda_{aa} = (\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1} \qquad{(2.79)}</script>

<script type="math/tex; mode=display">\Lambda_{ab} = -(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\Sigma_{ab}\Sigma_{bb}^{-1} \qquad{(2.80)}</script>

<ul>
  <li>최종적으로 \( p({\bf x}_a|{\bf x}_b) \) 에 대한 평균과 분산을 정리해보자.</li>
</ul>

<script type="math/tex; mode=display">{\pmb \mu}_{a|b} = {\pmb \mu}_a + \Sigma_{aa}\Sigma_{bb}^{-1}({\bf x}_b-{\pmb \mu}_b) \qquad{(2.81)}</script>

<script type="math/tex; mode=display">\Sigma_{a|b} = \Sigma_{aa} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba} \qquad{(2.82)}</script>

<ul>
  <li>아오, 기나긴 전개 과정이 끝났다.</li>
  <li>위의 식을 좀 살펴보면,
    <ul>
      <li>평균의 경우 \( {\bf x}_b \) 에 대한 선형 함수의 꼴로 나오게 되며,</li>
      <li>공분산의 경우 \( {\bf x}_b \) 에 대해 독립적인 식이 된다.</li>
    </ul>
  </li>
  <li>조건부 분포 \( p({\bf x}_a|{\bf x}_b) \) 에 대한 평균과 공분산 값은 \( {\bf x}_a \) 와 \( {\bf x}_b \) 에 대한 부분 공분산 행렬이 아닌 부분 정확도 행렬로 표현하는게 더 간결하다는 것도 알 수 있다.</li>
</ul>

<p><strong>결론</strong></p>

<ul>
  <li>\( p({\bf x}_a, {\bf x}_b) \) 가 가우시안 분포를 따르는 경우, \( p({\bf x}_a|{\bf x}_b) \) 도 가우시안 분포를 따르게 된다.
    <ul>
      <li>이거 하나 확인하자고 꽤 많은 양의 수식을 전개해 보았다.</li>
    </ul>
  </li>
</ul>

<h2 id="marginal-gaussian-distributions">2.3.2. 주변 확률 가우시안 분포 (Marginal Gaussian distributions)</h2>

<ul>
  <li>지금까지 결합확률분포인 \( p({\bf x}_a, {\bf x}_b) \) 의 확률 분포가 가우시안 분포이면 \( p({\bf x}_a|{\bf x}_b) \) 분포도 가우시안 분포임을 확인하였다.</li>
  <li>이제 이 식을 이용하여 주변 확률 분포에 대해서도 살펴볼 예정이다.</li>
  <li><strong>주변확률 분포</strong>
    <ul>
      <li>결합 확률 분포에서 한 쪽의 변수가 사라지거나 무시되는 것.</li>
      <li>\( p(x, y) \) 에서 \( x \) 에 대한 주변 확률 분포는 \( p(x) \) 가 되며, \( y \) 에 대한 주변 확률 분포는 마찬가지로 \( p(y) \) 가 된다.</li>
      <li>이 때 한쪽의 변수는 합산하여 사라지게 되는데 이산 변수는 모든 확률 값의 합으로, 연속 변수의 경우 적분을 통해 진행된다.</li>
    </ul>
  </li>
  <li>주변(marginal) 확률 분포에 대해 살펴보도록 하자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}_a) = \int p({\bf x}_a, {\bf x}_b)d{\bf x}_b \qquad{(2.83)}</script>

<ul>
  <li>주변 확률 분포 또한 가우시안 분포가 되는지를 확인하는 과정이 필요.</li>
  <li>마찬가지로 우리의 전략은 이차형식(<code class="highlighter-rouge">quadratic</code>) 내에서 평균과 분산을 얻어낼 수 있는지 여부를 확인하는 것.</li>
  <li>가우시안 분포 내 이차 형식을 다시 한번 보자.</li>
</ul>

<script type="math/tex; mode=display">-\dfrac{1}{2}({\bf x}-{\pmb \mu})^T\Sigma^{-1}({\bf x}-{\pmb \mu})=-\dfrac{1}{2}{\bf x}^T\Sigma^{-1}{\bf x} + {\bf x}^T\Sigma^{-1}{\pmb \mu}+const</script>

<ul>
  <li>여기서 우리는 \( {\bf x}_b \) 에 대한 적분을 수행해야 한다.</li>
  <li>따라서 이번에는 \( {\bf x}_b \) 텀에 대해 각각의 계수를 모아본다.</li>
  <li>따라서 마찬가지로 <em>completing the square</em> 를 사용하게 된다.</li>
</ul>

<script type="math/tex; mode=display">-\dfrac{1}{2}{\bf x}_b^{T}\Lambda_{bb}{\bf x}_b + {\bf x}_b^T{\bf m} = -\dfrac{1}{2}({\bf x}_b-{\bf m})^T\Lambda_{bb}({\bf x}_b-\Lambda_{bb}^{-1}{\bf m}) + \dfrac{1}{2}{\bf m}^T\Lambda_{bb}^{-1}{\bf m} \qquad{(2.84)}</script>

<ul>
  <li>위의 식은 \( {\bf x}_b \) 에 대해 완전 제곱식을 이용해서 전개한 식이다.
    <ul>
      <li>완전 제곱식이란 말을 너무 오랜만에 들어서 기억이 안 날수도 있는데,</li>
      <li>이차식을 어떻게든 제곱의 형태 즉, 이차 형식으로 만들어내고 이를 만들기 위해 추가한 텀을 다시 빼는 텀으로 넣어 식을 보정하는 것.</li>
      <li>위의 식을 보면 \( {\bf x}_b \) 에 대한 이차식을 만들어내 내고 이 때 필요한 \( \frac{1}{2}{\bf m}^T\Lambda_{bb}^{-1}{\bf m} \) 가 보정된 형태.</li>
    </ul>
  </li>
  <li>여기서 \( {\bf x}_b \) 의 1차 계수인 \( {\bf m} \) 은 다음과 같이 정의된다.</li>
</ul>

<script type="math/tex; mode=display">{\bf m} = \Lambda_{bb}{\pmb \mu}_b - \Lambda_{ba}({\bf x}_a-{\pmb \mu}_a) \qquad{(2.85)}</script>

<ul>
  <li>자, 이제 이 식을 가지고 실제 적분을 수행해보자. (적분은 \( {\bf x}_b \) 에 대한 적분이다.)
    <ul>
      <li>첫번째 텀 \( -\frac{1}{2}({\bf x}_b-{\bf m})^T\Lambda_{bb}({\bf x}_b-\Lambda_{bb}^{-1}{\bf m}) \) 은 \( {\bf x}_b \) 에 대한 식으로 생각할 수 있다.</li>
      <li>두번째 텀 \( \frac{1}{2}{\bf m}^T\Lambda_{bb}^{-1}{\bf m} \) 은 \( {\bf x}_b \) 에는 종속적이지 않으나 \( {\bf x}_a \) 에는 종속적이다.</li>
    </ul>
  </li>
  <li>이제 첫번 째 텀을 적분하는 식을 기술해보자.</li>
</ul>

<script type="math/tex; mode=display">\int\exp\left\{-\dfrac{1}{2}({\bf x}_b-\Lambda_{bb}^{-1}{\bf m})^T\Lambda_{bb}({\bf x}_b-\Lambda_{bb}^{-1}{\bf m})\right\}d{\bf x}_b \qquad{(2.86)}</script>

<ul>
  <li>결국 위의 적분식은 Unnormalize 가우시안 적분이 되며 실제 값은 정규화 계수의 역수가 된다.
    <ul>
      <li>가우시안 분포의 경우 적분의 합이 1이 되어야 하므로, 적분의 결과에 역수를 취한 값이 정규화 계수가 된다.</li>
      <li>위의 식 자체가 하나의 정규식으로 완전한 꼴을 가지고 있으므로 독립적인 값으로 생각하고 계산해도 문제 없음.</li>
      <li>이런 정규화 계수는 평균에 독립적이고 단지 공분산에 대한 행렬식에 의존하게 된다. (가우시안 분포의 정의에 의해)</li>
      <li>사실 첫번째 텀은 그냥 적분이 가능하다고 생각하기만 하면 큰 무리 없음.</li>
    </ul>
  </li>
  <li>이제 두번째 텀에 관심을 가져보자.두번 째 텀은 \( {\bf x}_a \) 에 종속적인 텀이다.
    <ul>
      <li>따라서 \( {\bf x}_b \) 에 대한 적분식에서는 관여하지 않고 적분식 밖으로 나오게 된다.</li>
      <li>하지만 이후에는 \( {\bf x}_a \) 에 대한 식으로 사용되게 된다.</li>
      <li>왜냐하면 적분 이후에는 \( p({\bf x}_a) \) 식의 꼴이 되고 \( {\bf x}_a \) 에 대한 확률 함수로 고려할 수 있기 때문이다.</li>
    </ul>
  </li>
  <li>이제 이 식을 \( {\bf x}_a \) 에 대해 정리해보도록 하자.</li>
</ul>

<script type="math/tex; mode=display">\dfrac{1}{2}[\Lambda_{bb}{\pmb \mu}_b-\Lambda_{ba}({\bf x}_a-{\pmb \mu}_a)]^T\Lambda_{bb}^{-1}[\Lambda_{bb}{\pmb \mu}_b-\Lambda_{ba}({\bf x}_a-{\pmb \mu}_a)]\\
- \dfrac{1}{2}{\bf x}_a^T\Lambda_{aa}{\bf x}_a + {\bf x}_a^T(\Lambda_{aa}{\pmb \mu}_a+\Lambda_{ab}{\pmb \mu}_b) + const\\
= - \dfrac{1}{2}{\bf x}_a^T(\Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba}){\bf x}_a + {\bf x}_a^T(\Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba}){\pmb \mu}_a+const \qquad{(2.87)}</script>

<ul>
  <li>최종적으로 위의 결과는 지부수 \( \exp(\cdot) \) 에 놓이게 된다.</li>
  <li><strong>const</strong> 영역은 일단 \( {\bf x}_a \) 와 무관한 식이므로 잠시 머릿속에서 비워 둔다.</li>
  <li>근데 이런 형태의 꼴은 어디선가 봤던 식이다. (식 2.71)</li>
</ul>

<script type="math/tex; mode=display">-\dfrac{1}{2}({\bf x}-{\pmb \mu})^T\Sigma^{-1}({\bf x}-{\pmb \mu})=-\dfrac{1}{2}{\bf x}^T\Sigma^{-1}{\bf x} + {\bf x}^T\Sigma^{-1}{\pmb \mu}+const</script>

<ul>
  <li>그렇다. 이 식도 사실 이차 형식(<code class="highlighter-rouge">quadratic</code>)의 풀이 식과 동일한 형태의 모양을 취하고 있다.</li>
  <li>결국은 \( \exp(quadratic) \) 와 같은 형태가 되므로 정규식이 된다.</li>
  <li>조건부 분포를 구할 때 사용한 방식을 그대로 적용하면 여기서 평균과 분산을 구할 수 있다.
    <ul>
      <li>공분산은 이차형식(quadratic) 내 2차식의 계수로, 평균은 1차식의 계수로 얻을 수 있음을 확인했다.</li>
    </ul>
  </li>
  <li>이를 이용해서 분산과 평균을 각각 구해보자.</li>
</ul>

<script type="math/tex; mode=display">\Sigma_a = (\Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})^{-1} \qquad{(2.88)}</script>

<script type="math/tex; mode=display">\Sigma_a(\Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba}){\pmb \mu}_a = {\pmb \mu}_a \qquad{(2.89)}</script>

<ul>
  <li>마찬가지로 정확도 행렬보다는 공분산 행렬로 표기하는 것이 더 편할 수 있다.
    <ul>
      <li>수식 계산에는 정확도 행렬이 편하지만 최종적으로 사용할 때에는 공분산 행렬이 더 편하다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{pmatrix}\Lambda_{aa} & \Lambda_{ab} \\ \Lambda_{ba} & \Lambda_{bb} \end{pmatrix}^{-1} =\begin{pmatrix} \Sigma_{aa} & \Sigma_{ab} \\ \Sigma_{ba} & \Sigma_{bb} \end{pmatrix} \qquad{(2.90)} %]]></script>

<ul>
  <li>아래 식을 이용해서 전개해보자.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{pmatrix}A & B \\ C & D \end{pmatrix}^{-1} =\begin{pmatrix} M & -MBD^{-1} \\ -D^{-1}CM & D^{-1}CMBD^{-1} \end{pmatrix} %]]></script>

<ul>
  <li>우리는 이제 다음과 같은 식을 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">(\Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})^{-1} = \Sigma_{aa} \qquad{(2.91)}</script>

<ul>
  <li>최종적으로 전개를 하면 다음과 같은 식을 얻는다.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf x}_a] = {\pmb \mu}_a \qquad{(2.92)}</script>

<script type="math/tex; mode=display">cov[{\bf x}_a] = \Sigma_{aa} \qquad{(2.93)}</script>

<ul>
  <li>엄청나게 복잡한 수식 전개를 했지만 얻는 결과는 너무 깔끔하다.</li>
  <li>게다가 직관적으로 너무나 타당한 식을 얻었다.</li>
</ul>

<hr />

<p><strong>Partitioned Gaussians</strong></p>

<ul>
  <li>이제 정리의 시간을 가져보자.</li>
  <li>가우시안 분포를 취하는 어떤 결합 분포가 다음과 같이 주어졌을 때 (단, \( N({\bf x}|{\pmb \mu}, \Sigma) \) with \( \Lambda \equiv \Sigma^{-1} \) )</li>
</ul>

<script type="math/tex; mode=display">{\bf x}=\dbinom{ {\bf x}_a }{ {\bf x}_b } \qquad{(2.94)}</script>

<script type="math/tex; mode=display">{\pmb \mu}=\dbinom{ {\pmb \mu}_a }{ {\pmb \mu}_b } \qquad{(2.94)}</script>

<script type="math/tex; mode=display">% <![CDATA[
\Sigma=\begin{pmatrix}\Sigma_{aa} & \Sigma_{ab}\\ \Sigma_{ba} & \Sigma_{bb}\end{pmatrix} \qquad{(2.95)} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\Lambda = \begin{pmatrix}\Lambda_{aa} & \Lambda_{ab}\\ \Lambda_{ba} & \Lambda_{bb}\end{pmatrix} \qquad{(2.95)} %]]></script>

<p><em>조건부 확률 분포 (Conditional distribution)</em></p>

<script type="math/tex; mode=display">p({\bf x}_a|{\bf x}_b) = N({\bf x}_a\;|\;{\pmb \mu}_{a|b}, \Lambda_{aa}^{-1}) \qquad{(2.96)}</script>

<script type="math/tex; mode=display">{\pmb \mu}_{a|b} = {\pmb \mu}_a - \Lambda_{aa}^{-1}\Lambda_{ab}({\bf x}_b-{\pmb \mu}_b) \qquad{(2.97)}</script>

<p><em>결합 확률 분포 (Marginal distribution)</em></p>

<script type="math/tex; mode=display">p({\bf x}_a) = N({\bf x}_a\;|\;{\pmb \mu}_a, \Sigma_{aa}) \qquad{(2.98)}</script>

<hr />

<ul>
  <li>그림으로 살펴보자.</li>
</ul>

<div class="text-center">
  <img src="/kyo-ml-study/images/Figure2.9a.png" alt="Figure 2.9a" height="180px" />
  <img src="/kyo-ml-study/images/Figure2.9b.png" alt="Figure 2.9b" height="180px" />
</div>

<ul>
  <li>위의 그림은 \( p(x_a, x_b) \) 에 대한 분포를 그림으로 그린 것이다. (녹색)</li>
  <li>여기서 푸른 선은 \( p(x_a) \) 를 나타낸다.</li>
  <li>붉은 선은 \( p({x_a|x_b=0.7}) \) 일 때의 값이다.</li>
  <li>모두 가우시안 분포의 모양을 따르고 있다는 것을 확인할 수 있다.</li>
</ul>

<h2 id="bayes-theorem-for-gaussian-variables">2.3.3. 가우시안 변수를 위한 베이즈 이론 (Bayes’ theorem for Gaussian variables)</h2>

<ul>
  <li>지금까지 가우시안 분포 \( p({\bf x}) \) 에서 \( x=({\bf x}_a, {\bf x}_b) \) 로 나누어 \( p({\bf x}_a|{\bf x}_b) \) 와 \( p({\bf x}_a) \) 도 가우시안 분포가 된다는 것을 확인했다.</li>
  <li>또 조건부 분포 \( p({\bf x}_a|{\bf x}_b) \) 의 평균 값이 \( {\bf x}_b \) 에 대한 선형 함수임을 확인했다.</li>
  <li>이제 가우시안 주변 확률 분포인 \( p({\bf x}) \) 와 가우시안 조건부 분포 \( p({\bf y}|{\bf x}) \) 에 대해 살펴볼 것이다.
    <ul>
      <li>이 때 \( p({\bf y}|{\bf x}) \) 의 평균 값은 \( {\bf x} \) 에 대한 선형함수이고 분산값은 \( {\bf x} \) 에 독립적이다.</li>
    </ul>
  </li>
  <li>이는 가우시안 선형 모델 (linear Gaussian model) 의 한 예이다.
    <ul>
      <li>이와 관련된 내용은 8장에서 다시 다룰 것이다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p({\bf x}) = N({\bf x}\;|\;{\pmb \mu}, \Lambda^{-1}) \qquad{(2.99)}</script>

<script type="math/tex; mode=display">p({\bf y}|{\bf x}) = N({\bf y}\;|\;{\bf A} {\bf x}+{\bf b} , L^{-1}) \qquad{(2.100)}</script>

<ul>
  <li>위와 같은 식이 주어졌다고 생각하면 된다.
    <ul>
      <li>\( p({\bf x}) \) 는  가우시안 주변 확률</li>
      <li>\( p({\bf y}|{\bf x}) \) 는 가우시안 조건부 확률
        <ul>
          <li>평균 : \( {\bf x} \) 에 대해 선형 함수</li>
          <li>분산 : \( {\bf x} \) 와 독립적</li>
          <li>만약 \( {\bf x} \) 는 \( D \) 차원이고, \( {\bf y} \) 는 \( M \) 차원 데이터라면 행렬 \( {\bf A} \) 는 \( D \times M \) 행렬이 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이번 절에서 하고자 하는 것은?
    <ul>
      <li>베이즈 이론를 활용하여,</li>
      <li>\( p({\bf z}) = p({\bf x})p({\bf y}|{\bf x}) \) 인 식을 \( p({\bf x}|{\bf y})p({\bf y}) \) 와 같은 식으로 전개함</li>
      <li>곱의 법칙에 따라서 \( p({\bf z}) \) 즉, \( p({\bf x}, {\bf y}) \) 를 구할 수 있고,</li>
      <li>\( p({\bf y}) \) 와 \( p({\bf x}|{\bf y}) \) 도 구할 수 있다.</li>
      <li>이게 왜 필요할까 싶지만 이후 장에서 가끔 사용된다.
        <ul>
          <li>예를 들어 현재 분포를 \( p({\bf x}) \) 와 \( p({\bf y}|{\bf x}) \) 를 만족하도록 만들어놓고, 최종적으로 \( p({\bf y}) \) 등을 만들어낸다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>이후 과정은 증명 과정이다.</p>
  </li>
  <li>우선 \( {\bf x} \) 와 \( {\bf y} \) 의 결합 확률을 \( {\bf z} \) 로 정의하자.</li>
</ul>

<script type="math/tex; mode=display">{\bf z} = \dbinom{ {\bf x} }{ {\bf y} } \qquad{(2.101)}</script>

<ul>
  <li>이제 결합 분포에 로그를 씌운다.</li>
</ul>

<script type="math/tex; mode=display">\ln{p({\bf z})} = \ln p({\bf x}) + \ln p({\bf y})\\
= -\frac{1}{2}({\bf x}-{\pmb \mu})^T\Lambda({\bf x}-{\pmb \mu}) -\frac{1}{2}({\bf y}-{\bf A}{\bf x}-{\bf b})^T{L}({\bf y}-{\bf A}{\bf x}-{\bf b})+const \qquad{(2.102)}</script>

<ul>
  <li>여기서 <code class="highlighter-rouge">const</code> 영역은 \( {\bf x} \) 나 \( {\bf y} \) 와는 상관없는 텀이다.</li>
  <li>그 외의 텀은 \( {\bf z} \) 의 요소들에 대한 이차형식(<code class="highlighter-rouge">quadratic</code>)의 함수이다.
    <ul>
      <li>앞서 이런 형태의 식에 대한 가우시안 분포 여부를 확인했었다.</li>
      <li>따라서 이 분포도 가우시안 분포가 된다는 것을 알 수 있다.</li>
    </ul>
  </li>
  <li>어쨌거나 위의 식을 모두 전개하여 이 중 이차항만을 추려보자.
    <ul>
      <li>알다시피 공분산을 구하기 위해서이다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
-\frac{1}{2}{\bf x}^T(\Lambda + {\bf A}^T\Lambda{\bf A}){\bf x} - \frac{1}{2}{\bf y}^T{\bf L}{\bf y} + \frac{1}{2}{\bf x}^T{\bf A}{\bf L}{\bf y}\\
= -\frac{1}{2}\dbinom{ {\bf x} }{ {\bf y} }^T \left(\begin{array}{cc}\Lambda+{\bf A}^T{\bf L}{\bf A} & -{\bf A}^T{\bf L}\\-{\bf L}{\bf A} & {\bf L}\end{array} \right) \dbinom{ {\bf x} }{ {\bf y} } = -\frac{1}{2}{\bf z}^T{\bf R}{\bf z} \qquad{(2.103)} %]]></script>

<ul>
  <li>신기하게도 \( {\bf z} \) 에 대한 이차형식(<code class="highlighter-rouge">quadratic</code>) 형태의 식이 전개되었다.</li>
  <li>따라서 \( R \) 은 정확도 행렬이 된다. (공분산의 역행렬)</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
{\bf R} = \left(\begin{array}{cc}\Lambda+{\bf A}^T{\bf L}{\bf A} & -{\bf A}^T{\bf L}\\-{\bf L}{\bf A} & {\bf L}\end{array}\right) \qquad{(2.104)} %]]></script>

<ul>
  <li>역행렬을 만드는 식을 이용하여 공분산도 구할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
cov[{\bf z}]={\bf R}^{-1} = \left(\begin{array}{cc}\Lambda^{-1} & \Lambda^{-1}{\bf A}^T \\ {\bf A}\Lambda^{-1} & {\bf L}^{-1}+{\bf A}\Lambda^{-1}{\bf A}^T  \end{array}\right) \qquad{(2.105)} %]]></script>

<ul>
  <li>복잡해보이긴 해도 구할수 없는 식은 아니다.</li>
  <li>이제 일차항을 묶어 얻어진 계수와, 앞서 구한 공분산을 이용하여 평균을 구할 수 있다.</li>
  <li>일차항은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">{\bf x}^T\Lambda{\pmb \mu} - {\bf x}^T{\bf A}^T{\bf L}{\bf b} + {\bf y}^T{\bf L}{\bf b} = \dbinom{ {\bf x} }{ {\bf y} }^T\dbinom{\Lambda{\pmb \mu}-{\bf A}^T{\bf L}{\bf b}}{ {\bf L}{\bf b} } \qquad{(2.106)}</script>

<ul>
  <li>따라서 평균값은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf z}] = {\bf R}^{-1}\dbinom{ {\bf x} }{ {\bf y} }^T\dbinom{\Lambda{\pmb \mu}-{\bf A}^T{\bf L}{\bf b}}{ {\bf L}{\bf b} } \qquad{(2.107)}</script>

<ul>
  <li>\( {\bf R}^{-1} \) 을 대입하여 전개해보자. 최종 식으로 다음을 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf z}] = \dbinom{ {\pmb \mu} }{ {\bf A} {\pmb \mu} - {\bf b}} \qquad{(2.108)}</script>

<ul>
  <li>각 요소의 평균이 결국 \( {\bf z} \) 의 평균이 된다.
    <ul>
      <li>직관적으로 매우 당연한 결과이지만 이를 얻어내기까지의 수식 계산이 쉽지 않네.</li>
    </ul>
  </li>
  <li>지금까지 결합 분포 \( p({\bf x}, {\bf y}) \) 를 살펴보았으므로 이제 주변 확률 분포(marginal distribution)를 살펴보도록 하자.</li>
  <li>여기에서는 앞서 다루었던 결합 분포의 성질을 이용하게 된다.</li>
  <li>얻은 결과는 다음과 같다. (식을 전개하기가 귀찮다.)</li>
</ul>

<script type="math/tex; mode=display">E[{\bf y}] = {\bf A}{\pmb \mu} + {\bf b} \qquad{(2.109)}</script>

<script type="math/tex; mode=display">cov[{\bf y}] = {\bf L}^T + {\bf A}\Lambda^{-1}{\bf A}^T \qquad{(2.110)}</script>

<ul>
  <li>여기서 \( {\bf A}={\bf I} \) 인 경우 두 가우시안의 관계가 <em>convolution</em> 관계라고 한다.
    <ul>
      <li>여기서 <em>convolution</em> 은 두 개의 가우시안 함수가 서로 오버랩되는 영역을 나타내는 식이라고 생각하면 된다.</li>
      <li>이렇게 오버랩되는 영역도 마찬가지로 가우시안 분포를 따르게 된다.</li>
      <li>이 때 생성되는 분포의 평균는 각각의 분포의 평균의 합의 평균이 되고 분산은 각각의 분포의 분산의 합의 평균이 된다.</li>
    </ul>
  </li>
  <li>이제 조건 분포 \( p({\bf x}|{\bf y}) \) 에 대해 좀 알아보도록 하자.</li>
  <li>이 때의 평균과 분산은 너무나도 쉽게 구해지는데, 왜냐하면 이미 앞에서 다 구했기 때문이다. (식 2.73, 2.15)</li>
</ul>

<script type="math/tex; mode=display">\Sigma_{a|b}=\Lambda_{aa}^{-1}</script>

<script type="math/tex; mode=display">{\pmb \mu}_{a|b}={\pmb \mu}_a - \Lambda_{aa}^{-1}\Lambda_{ab}({\bf x}_b-{\pmb \mu}_b)</script>

<ul>
  <li>이걸 \( p({\bf x}|{\bf y}) \) 에 맞게 대입만 하면 된다.</li>
</ul>

<script type="math/tex; mode=display">E[{\bf x}|{\bf y}] = (\Lambda+{\bf A}^T{\bf L}{\bf A})^{-1}\{ {\bf A}^T{\bf L}({\bf y}-{\bf b})+\Lambda{\pmb \mu}\} \qquad{(2.111)}</script>

<script type="math/tex; mode=display">cov[{\bf x}|{\bf y}] = (\Lambda+{\bf A}^T{\bf L}{\bf A})^{-1} \qquad{(2.112)}</script>

<hr />

<p><strong>Mareginal and Conditional Gaussians</strong></p>

<ul>
  <li>이제 정리를 해보자.</li>
  <li>다음과 같은 가우시안 확률 분포가 주어졌을 때,</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}) = N({\bf x}\;|\;{\pmb \mu}, \Lambda^{-1}) \qquad{(2.113)}</script>

<script type="math/tex; mode=display">p({\bf y}|{\bf x}) = N({\bf y}\;|\;{\bf A}{\bf x}+{\bf b}, {\bf L}^{-1}) \qquad{(2.114)}</script>

<ul>
  <li>주변 확률 분포와 조건부 확률 분포는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf y}) = N({\bf y}\;|\;{\bf A}{\pmb \mu}+{\bf b}, {\bf L}^{-1}+{\bf A}\Lambda^{-1}{\bf A}^T) \qquad{(2.115)}</script>

<script type="math/tex; mode=display">p({\bf x}|{\bf y}) = N({\bf x}\;|\;\Sigma\{ {\bf A}^T {\bf L}({\bf y}-{\bf b})+\Lambda{\pmb \mu}\}, \Sigma) \qquad{(2.116)}</script>

<ul>
  <li>단, \( \Sigma \) 는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">\Sigma = (\Lambda + {\bf A}^T{\bf L}{\bf A})^{-1} \qquad{(2.117)}</script>

<hr />

<ul>
  <li>이런 복잡한 식을 도대체 어디다 써 먹을까하는 생각이 들 수 있지만, 이후에 나온다.
    <ul>
      <li>보통 \( p({\bf x}) \) 가 가우시안 분포로 주어지고 이 때 \( p({\bf y}|{\bf x}) \) 를 \( {\bf x} \) 에 대한 선형식을 가지는 평균값의 꼴로 만들기만 하면,</li>
      <li>이 때의 \( p({\bf y}) \) 와 \( p({\bf x}|{\bf y}) \) 를 위의 식을 통해 만들어낼 수 있다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>2.3절의 분량이 너무 길어지는 관계로 이후 절은 Part.II 으로 넘긴다.</li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoongithub'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-ml-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
