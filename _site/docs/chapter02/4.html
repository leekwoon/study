<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>4. The Exponential Family</title>
  <link rel="stylesheet" href="/kyo-ml-study/css/main.css">
  <link rel="stylesheet" href="/kyo-ml-study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/kyo-ml-study/docs/chapter02/4.html">
  <script src="/kyo-ml-study/js/jquery-1.12.0.min.js"></script>     
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/kyo-ml-study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/kyo-ml-study/"><strong>kyo-ml-study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <!-- for drop-down navi -->
            <li class="dropdown">
              <a href="/kyo-ml-study" class="dropdown-toggle" data-toggle="dropdown"><strong>Chapter
                  <b class="caret"></b></strong>
              </a>
              <ul class="dropdown-menu">
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter01/0">1. Introduction</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter02/0">2. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter03/0">3. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter04/0">4. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter05/0">5. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter06/0">6. Probabilty Distribution</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter07/0">7. Linear Models for Regression</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter08/0">8. Linear Models for Classification</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter09/0">9. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter10/0">10. Neural Networks</a>
                </li>
                
                
                
                <li >
                  <a href="/kyo-ml-study/docs/chapter11/0">11. Neural Networks</a>
                </li>
                
                
              </ul>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter01/0"><strong>1</strong></a>
            </li>
            
                      
          
          
            
            <li  class="active" >
              <a href="/kyo-ml-study/docs/chapter02/0"><strong>2</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter03/0"><strong>3</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter04/0"><strong>4</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter05/0"><strong>5</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">6</strong></a></li>
                      
          
          
            <li><a href="#"><strong class="text-danger">7</strong></a></li>
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter08/0"><strong>8</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter09/0"><strong>9</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/kyo-ml-study/docs/chapter10/0"><strong>10</strong></a>
            </li>
            
                      
          
          
            <li><a href="#"><strong class="text-danger">11</strong></a></li>
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Chapter. 2</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/0">0. Probability Distributions</a>
      </li>
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/1">1. Binary Variables</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/2">2. Multinomial Variables</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/3_1">3. The Gaussian Distribution [I]</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/3_2">3. The Gaussian Distribution [II]</a>
      </li>
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/3_3">3. The Gaussian Distribution [III]</a>
      </li>
      
      
      
      <li class="active " >
        <a href="/kyo-ml-study/docs/chapter02/4">4. The Exponential Family</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class=" " >
        <a href="/kyo-ml-study/docs/chapter02/5">5. Nonparametric Methods</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/kyo-ml-study/blob/gh-pages/docs/chapter02/4.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>4. The Exponential Family</h1>
    </header>
    <hr class="title">
    <article>
      <ul>
  <li>지금까지 확률 분포가 무엇인지 줄기차게 배웠다.</li>
  <li>사실 지금까지 다루었던 확률 분포들 중에 일부는 지수 분포족(exponential family)이라 불리우는 집합군에 속한다.</li>
  <li>지수 분포족에 속하는 분포들은 공통된 특징을 내포하고 있는데 이번 절에서 이를 일반화하여 설명하고자 한다.</li>
  <li>지수분포족에 속하는 분포는 다음과 같은 형태의 일반화된 분포 식으로 표현할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf \eta})=h({\bf x})g({\bf \eta})\exp\{ {\bf \eta}^T{\bf u}({\bf x})\} \qquad{(2.194)}</script>

<ul>
  <li>여기서 입력 데이터 \( {\bf x} \) 는 벡터일수도 있고 스칼라 값일수도 있다. (물론 연속/불연속 여부도 상관 없다.)</li>
  <li>\( {\bf \eta} \) 는 자연 모수 (natural parameters)라고 부른다.</li>
  <li>\( {\bf u}({\bf x}) \) 는 \( {\bf x} \) 에 대한 어떤 함수이다.</li>
  <li>\( g({\bf \eta}) \) 는 확률 분포에 대한 정규화 계수로 확률의 전체 합을 1로 맞추는데 사용되는 요소이다.</li>
</ul>

<script type="math/tex; mode=display">g({\bf \eta})\int h({\bf x})\exp\{ {\bf \eta}^T{\bf u}({\bf x})\}d{\bf x}=1 \qquad{(2.195)}</script>

<ul>
  <li>위와 같은 형태를 취하는 분포를 지금까지 배운 적이 없다고 생각할 수도 있다.</li>
  <li>하지만 지금까지 배운 많은 분포들이 위와 같은 일반화 식 형태으로 변경 가능하다.</li>
</ul>

<hr />

<p><strong>베르누이 분포</strong></p>

<ul>
  <li>가장 먼저 베르누이 분포(Bernoulli distribution)가 지수 분포에 속하는지를 확인해보도록 한다.</li>
</ul>

<script type="math/tex; mode=display">p(x|\mu) = Bern(x|\mu)=\mu^x(1-\mu)^{1-x} \qquad{(2.196)}</script>

<ul>
  <li>이를 지수분포족의 형태로 변환 가능할까?</li>
</ul>

<script type="math/tex; mode=display">p(x|\mu) = \exp \{x\ln\mu + (1-x)\ln(1-\mu)\}\\\\=(1-\mu)\exp\left\{\ln\left(\dfrac{\mu}{1-\mu}\right)x\right\} \qquad{(2.197)}</script>

<ul>
  <li>충분히 가능하다.</li>
  <li>분포 식에서 \( \exp(\cdot) \) 형태를 만들어내기 위해 \( \exp(\cdot) \) 과 \( \ln(\cdot) \) 을 적절하게 이용하여 변형한다.
    <ul>
      <li>\( f(x)=\exp(\ln{f(x)}) \) 를 활용한다.</li>
    </ul>
  </li>
  <li>위의 식에서 \( \eta \) 는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">\eta = \ln\left(\dfrac{\mu}{1-\mu}\right) \qquad{(2.198)}</script>

<ul>
  <li>\( \mu=\sigma(\eta) \) 를 놓고 다른 형태로 식을 만들 수도 있다.</li>
</ul>

<script type="math/tex; mode=display">\sigma(\eta)=\dfrac{1}{1+\exp(-\eta)} \qquad{(2.199)}</script>

<ul>
  <li>이거 어디서 많이 보던 식이다.
    <ul>
      <li>이를 로지스틱 회귀(logistic regression)라고 부른다.</li>
      <li>베르누이를 적절하게 변형하면 로지스틱 회귀 식이 나오는 것을 확인할 수 있다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p(x|\eta)=\sigma(-\eta)\exp(\eta x) \qquad{(2.200)}</script>

<ul>
  <li>\( 1-\sigma(\eta)=\sigma(-\eta) \) 를 이용하면 다음과 같이 식을 확인할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">u(x) = x \qquad{(2.201)}</script>

<script type="math/tex; mode=display">h(x) = 1 \qquad{(2.202)}</script>

<script type="math/tex; mode=display">g(\eta) = \sigma(-\eta) \qquad{(2.203)}</script>

<ul>
  <li>따라서 베르부이 분포는 지수 분포족에 속한다.</li>
</ul>

<hr />

<p><strong>다항분포</strong></p>

<ul>
  <li>다음으로 다항분포(multinomial distribution)도 살펴보도록 하자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf \mu}) = \prod_{k=1}^{M}\mu_k^{x_k}=\exp\left\{\sum_{k=1}^{M}x_k\ln\mu_k\right\} \qquad{(2.204)}</script>

<ul>
  <li>여기서 \( {\bf x}=(x_1,…,x_M)^T \) 이다.</li>
  <li>다시한번 표준화된 식으로 변형해보면 다음과 같이 식을 전개할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf \eta})=\exp({\bf \eta}^T{\bf x}) \qquad{(2.205)}</script>

<ul>
  <li>여기서 \( \eta_k=\ln\mu_k \) 이다. 단, \( {\bf \eta}=(\eta_1,…,\eta_M)^T \) 이다.</li>
  <li>최종으로 얻어지는 식은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">u(x) = {\bf x} \qquad{(2.206)}</script>

<script type="math/tex; mode=display">h(x) = 1 \qquad{(2.207)}</script>

<script type="math/tex; mode=display">g(\eta) = 1 \qquad{(2.208)}</script>

<ul>
  <li>다항분포의 조건에 따라 다음이 성립한다. (제약 사항임)</li>
</ul>

<script type="math/tex; mode=display">\sum_{k=1}^{M}\mu_k=1 \qquad{(2.209)}</script>

<ul>
  <li>이 식은 다음과 같이 생각할 수도 있음.</li>
</ul>

<script type="math/tex; mode=display">0\le\mu_k\le 1 \qquad{(2.210)}</script>

<script type="math/tex; mode=display">\sum_{k=1}^{M-1}\mu_k \le 1 \qquad{(2.210)}</script>

<ul>
  <li>위와 같은 제약을 염두에 두고 다시 표준화된 식으로 전개하면 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">\exp\left\{\sum_{k=1}^{M}x_k\ln\mu_k\right\} = \exp\left\{\sum_{k=1}^{M-1}x_k\ln\mu_k + \left(1-\sum_{k=1}^{M-1}x_k\right)\ln\left(1-\sum_{k=1}^{M-1}\mu_k\right) \right\}\\
= \exp\left\{\sum_{k=1}^{M-1}x_k\ln\left(\frac{\mu_k}{1-\sum_{j=1}^{M-1}\mu_j}\right)+\ln\left(1-\sum_{k=1}^{M-1}\mu_k\right)\right\} \qquad{(2.211)}</script>

<ul>
  <li>상수 부분은 밖으로 빼내고 식을 살펴보면 다음을 알 수 있다.</li>
</ul>

<script type="math/tex; mode=display">\ln\left(\frac{\mu}{1-\sum_{j}\mu_j}\right)=\eta_k \qquad{(2.212)}</script>

<ul>
  <li>이를 \( u_k \) 에 대해 전개하면 다음과 같은 식을 얻는다.</li>
</ul>

<script type="math/tex; mode=display">\mu_k=\dfrac{\exp(\eta_k)}{1+\sum_j\exp(\eta_j)} \qquad{(2.213)}</script>

<ul>
  <li>오, 이것도 어디서 많이 보던 식이다.</li>
  <li>이를 <em>softmax</em> 라고 부른다. (혹은 정규지수 : Normalized Exponential 라고 부른다.)</li>
  <li>어쨌거나 전체 식을 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf x}|{\bf \eta}) = \left(1+\sum_{k=1}^{M-1}\exp(\eta_k)\right)^{-1}\exp({\bf \eta}^T{\bf x}) \qquad{(2.214)}</script>

<ul>
  <li>이제 최종 식을 기술할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">{\bf u}({\bf x})={\bf x} \qquad{(2.215)}</script>

<script type="math/tex; mode=display">h({\bf x})=1 \qquad{(2.216)}</script>

<script type="math/tex; mode=display">g({\bf \eta}) = \left(1+\sum_{k=1}^{M-1}\exp(\eta_k)\right)^{-1} \qquad{(2.217)}</script>

<ul>
  <li>이 때 \( {\bf \eta} \) 는 \( {\bf \eta} = (\eta_1,…,\eta_{M-1},0)^T \) 가 된다.</li>
  <li>따라서 다항 분포도 마찬가지로 지수 분포이다.</li>
</ul>

<hr />

<p><strong>정규분포</strong></p>

<ul>
  <li>마지막으로 정규 분포에 대해서도 살펴본다.</li>
  <li>마찬가지로 정규 분포는 식 자체로 지수 분포족임을 알 수 있다.</li>
  <li>여기서는 최대한 간단히 정리만 한다.</li>
</ul>

<script type="math/tex; mode=display">p(x|\mu,\sigma^2)=\dfrac{1}{(2\pi\sigma^2)^{1/2}}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\} \qquad{(2.218)}\\
= \dfrac{1}{(2\pi\sigma^2)^{1/2}}\exp\left\{-\frac{1}{2\sigma^2}x^2+\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}\mu^2\right\} \qquad{(2.219)}</script>

<ul>
  <li>최종 식은 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">{\bf \eta} = \dbinom{\mu/\sigma^2}{-1/2\sigma^2} \qquad{(2.220)}</script>

<script type="math/tex; mode=display">{\bf u}(x) = \dbinom{x}{x^2} \qquad{(2.221)}</script>

<script type="math/tex; mode=display">h(x)=(2\pi)^{-1/2} \qquad{(2.222)}</script>

<script type="math/tex; mode=display">g({\bf \eta})=(-2\eta_2)^{1/2}\exp\left(\frac{\eta_1^2}{4\eta_2}\right) \qquad{(2.223)}</script>

<h2 id="maximum-likelihood-and-sufficient-statistics">2.4.1. 가능도 함수와 충분 통계량 (Maximum likelihood and sufficient statistics)</h2>

<ul>
  <li>이제 지수족 분포에서 파라미터 벡터 \( {\bf \eta} \) 를 추정하는 문제를 다루어 볼 것이다.
    <ul>
      <li>이 때 <em>MLE</em> 를 사용하여 이 값을 추정한다.</li>
      <li>\( {\bf \eta} \) 에 대한 미분값을 살펴보자.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\nabla g({\bf \eta})\int h({\bf x})\exp\{ {\bf \eta}^T {\bf u}({\bf x})\}d{\bf x} + g({\bf \eta})\int h({\bf x})\exp\{ {\bf \eta}^T{\bf u}({\bf x})\}{\bf u}({\bf x})d{\bf x} = 0 \qquad{(2.224)}</script>

<ul>
  <li>\( \int h({\bf x})\exp{ {\bf \eta}^T{\bf u}({\bf x})}d{\bf x} = 1/g({\bf \eta}) \) 이므로 (식, 2.195 에 의해) 위의 식을 다음과 같이 정리할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">-\frac{1}{g({\bf \eta})} \nabla g({\bf \eta}) = g({\bf \eta})\int h({\bf x})\exp\{ {\bf \eta}^T{\bf u}({\bf x})\}{\bf u}({\bf x})d{\bf x}=E[{\bf u}({\bf x})] \qquad{(2.225)}</script>

<ul>
  <li>최종적으로 다음과 같은 식을 얻는다. (좌변을 보면 \( \ln(\cdot) \)  의 미분값임.)</li>
</ul>

<script type="math/tex; mode=display">-\nabla \ln g({\bf \eta}) = E[{\bf u}({\bf x})] \qquad{(2.226)}</script>

<ul>
  <li>\( g({\bf \eta}) \) 를 두번 미분한 결과는 \( {\bf u}({\bf x}) \) 의 공분산 값이 된다.
    <ul>
      <li>이 때 이 공분산은 양 정부호 (positive definite)를 만족한다.</li>
      <li>따라서 \( g({\bf \eta}) \) 함수는 <code class="highlighter-rouge">convex</code> 조건을 만족한다.</li>
      <li>교재에 증명은 생략되어 있다.</li>
    </ul>
  </li>
  <li>이제 서로 독립인(i.i.d) 관찰 데이터 \( {\bf X}={ {\bf x_1}, …, {\bf x}_N} \) 이 주어졌을 때 가능도 함수를 살펴보자.</li>
</ul>

<script type="math/tex; mode=display">p({\bf X}|{\bf \eta}) = \left(\prod_{n=1}^{N}h({\bf x}_n)\right) g({\bf \eta})^N \exp\left\{ {\bf \eta}^T\sum_{n=1}^{N}{\bf u}({\bf x}_n)\right\} \qquad{(2.227)}</script>

<ul>
  <li>로그를 붙인 후 \( {\bf \eta} \) 에 대해 미분하여 값을 0이 된다고 해서 전개를 하면 \( {\bf \eta}_{ML} \) 값을 얻을 수 있다.</li>
</ul>

<script type="math/tex; mode=display">-\nabla \ln g({\bf \eta}_{ML}) = \frac{1}{N}\sum_{n=1}^{N}{\bf u}({\bf x}_n) \qquad{(2.228)}</script>

<ul>
  <li>위의 식을 통해 \( \sum_n {\bf u}({\bf x}_n) \) 을 분포에 대한 충분 통계(<em>sufficient statistic</em> ) 라고 한다.
    <ul>
      <li>충분 통계는 앞서서 몇 번 언급이 되기는 했는데, 모수 값을 완전히 설명할 수 있는 최소한의 함수식이라고 생각하면 된다.</li>
      <li>따라서 우리는 모든 데이터를 계산시에 다 저장해서 사용할 필요가 없이 이 식에 나온 결과만을 저장하여 모수 추정에 활용하면 된다.</li>
      <li><em>베르누이</em>  : 베르누이에서는 \( {\bf u}(\bf x) \) 가 \( {\bf x} \) 이므로 우리는 단지 입력되는 데이터를 계속 누적하기만 하면 된다.</li>
      <li><em>가우시안</em> : 가우시안에서는 \( {\bf u}(\bf x) \) 가 \( {\bf u}({\bf x}) = (x, x^2)^T \) 이므로 \( x \) 의 합과 \( x^2 \) 의 합만을 유지하면 된다.</li>
    </ul>
  </li>
  <li>만약 \( N\rightarrow\infty \) 인 경우 충분통계 식은 \( E[{\bf u}({\bf x})] \) 가 된다.
    <ul>
      <li>이미 원래 식에서도 \( -\ln g({\bf \eta}) \) 의 값이 \( E[{\bf u}({\bf x})] \) 였으므로 \( \eta = \eta_{ML} \) 로 생각하면 된다.</li>
    </ul>
  </li>
  <li>사실 충분 통계량은 베이지안 패러다임 관점과도 연관이 있지만 여기서는 설명하지 않도록 한다.</li>
  <li>
    <p>이는 8장에서 그래프 모델을 살펴보면서 확인을 하도록 하자.</p>
  </li>
  <li><strong>참고</strong>
    <ul>
      <li>이번 절에서는 \( g({\bf \eta}) \) 를 기준으로 수식을 전개하여 \( (-\ln g({\bf \eta})) \) 를 얻어냈으나 처음부터 이 식을 \( G({\bf \eta}) \) 정의하여 전개할 수도 있다.</li>
      <li>다른 문서에서는 이렇게 전개하는 경우가 많더라.</li>
    </ul>
  </li>
</ul>

<h2 id="conjugate-priors">2.4.2. 공액 사전 분포 (Conjugate priors)</h2>
<ul>
  <li>앞서 여러 번 공액사전 분포(conjugate prior distribution)에 대해 이야기를 했었다.</li>
  <li>예를 들면 베루누이 분포의 공액 사전 분포는 베타분포, 다항 분포의 공액 사전 분포는 디리슈레 분포라는 것을 확인했다.</li>
  <li>가우시안의 경우는 평균에 대한 공액 사전 분포는 가우시안, 정확도(precision)에 대한 공액 사전 분포는 위샤트 분포이다.</li>
  <li>이제 이번 절에서는 일반화된 지수족 분포 \( p({\bf x}|{\bf \eta}) \) 에 대해서도 공액 사전 분포를 얻을 수 있는지 확인해 볼 것이다.
    <ul>
      <li>공액적 관계에 놓이려면 사전 분포의 형태와 사후 분포의 형태가 같아야 한다.</li>
      <li>따라서 둘 다 지수족 분포에 속하는 형태를 고려한다.</li>
      <li>최종적으로 \( p({\bf x}|{\bf \eta}) \) 에서 모수인 \( {\bf \eta} \) 에 대한 공액 사전 분포 \( p({\bf \eta}) \) 를 구해본다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p({\bf \eta}|{\bf \chi}, v) = f({\bf \chi}, v)g({\bf \eta})^v \exp\{v{\bf \eta}^T{\bf \chi}\} \qquad{(2.229)}</script>

<ul>
  <li>\( f({\bf \chi}, v) \) 는 정규화 계수이다.</li>
  <li>
    <p>\( g({\bf \eta}) \) 는 앞서 사용된 식과 동일한 식으로 확률의 전체 합이 1이 되기 위한 계수이다.</p>
  </li>
  <li>실제 이를 이용해서 얻어지는 사후 확률 분포는 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">p({\bf \eta}|{\bf X}, {\bf \chi}, v) \propto g({\bf \eta})^{v+N}\exp\left\{ {\bf \eta}^T\left(\sum_{n=1}^{N}{\bf u}({\bf x})+v{\bf \chi}\right)\right\} \qquad{(2.230)}</script>

<ul>
  <li>형태를 잘 보면 사전 분포와 동일한 형태의 식이 된다는 것을 알 수 있다.</li>
  <li>추가로 \( v \) 는 <em>effective number</em> 가 된다.
    <ul>
      <li>앞서 설명을 했지만 이는 사전 확률 분포와 실제 발현된 데이터가 확률값에 미치는 영향력이 동일한 지점에서의 모수 값을 의미하게 된다.</li>
    </ul>
  </li>
</ul>

<h2 id="noninfomative-priors">2.4.3. 무정보적 사전 분포 (Noninfomative priors)</h2>
<ul>
  <li>확률 추정의 문제에 있어서 어떤 경우에는 사전 분포의 모수 값을 손쉽게 지정할 수 있는 경우가 있다.
    <ul>
      <li>예를 들어 모수의 값으로 그냥 0을 사용하면 된다거나 해도 무리가 전혀 없는 경우들</li>
    </ul>
  </li>
  <li>하지만 반대로 사전 분포의 형태를 전혀 예측하기 어려운 경우도 존재한다.</li>
  <li>이 때 사용되는 분포가 무정보 사전 분포(non-infomative prior distribution)이다.</li>
  <li>사실 이런 경우 가장 손쉬운 방법은,
    <ul>
      <li>\( p(x|\lambda) \) 와 같이 모수 \( \lambda \) 가 주어지는 형태인 경우 \( p(\lambda)=const \) 로 결정하는 것이다.</li>
      <li>쉽게 말해 어떤 모수가 가질 값들이 모두 동일하다고 가정하는 것이다.
        <ul>
          <li>\( \lambda \) 가 \( K \) 개의 상태를 가지는 이산(distrete) 변수라면 \( 1/K \) 를 \( p(\lambda) \) 의 확률 값으로 사용하면 된다.</li>
        </ul>
      </li>
      <li>일견 타당해 보이는 방법이긴 하지만 수학적으로 문제가 되는 경우가 존재한다.</li>
    </ul>
  </li>
  <li>\( p(\lambda)=const \) 로 가정했을 때의 문제
    <ul>
      <li>\( \lambda \) 의 범위가 unbounded 일 때
        <ul>
          <li>만약 \( \lambda \) 가 몇 개의 상태를 가지게 되는지 모를 경우라면?</li>
          <li>단순한 상수 값을 사용하게 되면 분포를 전체 \( \lambda \) 영역에 대해 적분시 값이 \( \infty \) 가 되어버린다.
            <ul>
              <li>알다시피 확률 함수는 합이 1이 되어야 한다. 범위가 주어지지 않기 때문에 \( \infty \) 가 되어버린다.</li>
            </ul>
          </li>
          <li>이런 경우를 <strong>Improper prior</strong> 라고 부른다.</li>
          <li>물론 Improper prior 라고 해서 마냥 쓸모 없는 것은 아니다.
            <ul>
              <li>이런 분포를 사용하더라도 실제 사후 확률 분포가 이런 사전 분포와 잘 결합하여 최종적으로 proper 한 분포가 될 수도 있기 때문이다.</li>
              <li>물론 수학적으로 실제 이렇게 되는지 확인을 해봐야 한다.</li>
              <li>예를 들어 가우시안 분포의 평균 파라미터에 대해 상수 값의 사전 분포를 사용한다고 하더라도 실제 사후 분포는 문제가 없다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>비선형적으로 변환되는 변수를 분포에 적용할 때 (밀도 변환 문제)
        <ul>
          <li>조금 어려운 문제이긴 한데, 보통 확률 분포는 어떤 랜덤 변수를 입력 값으로 받아 분포의 결과 값을 반환하게 된다.</li>
          <li>이 때 변환에 대해서도 확률 함수에 대한 성질이 잘 들어맞아야 한다.</li>
          <li>만약 \( h(\lambda)=const \) 라고 하자. 이런 경우 \( \lambda=\eta^2 \) 이라고 하면 \( h(\eta^2) \) 또한 상수이다.</li>
          <li>이를 새로운 함수로 정의해서 \( \widehat{h}(\eta)=h(\eta^2) \) 이라고 하자. 이러면 \( \widehat{h}(\eta) \) 또한 상수 함수가 된다.</li>
          <li>이런 자연스러운 변환이 확률 식에도 통용이 될까?</li>
          <li>이제 어떤 확률 함수를 정의한다. \( p_{\lambda}(\lambda)=const \) 이다. 이를 \( \eta \) 에 대한 식으로 전개해보자.
            <ul>
              <li>아래 식을 보면 알겠지만 \( \lambda \) 에 대해 상수였던 확률이 \( \eta \) 에 대해서는 선형 함수(1차원의 선형함수)로 전환된다.</li>
              <li>혹시 아래와 같이 확률 식이 전환되는 이유가 궁금하다면 식 1.27을 참고하도록 하자.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p_{\eta}(\eta)=p_{\lambda}(\lambda)\left|\frac{d\lambda}{d\eta}\right|=p_{\lambda}(\eta^2)2\eta \propto \eta \qquad{(2.231)}</script>

<ul>
  <li>여기서 하고자 하는 이야기는 명확하다.
    <ul>
      <li>상수 값을 가지는 사전 분포를 사용하는 것이 나쁜 선택은 아니지만 상황에 맞도록 사용해야 한다.</li>
      <li>어떤 경우에는 전혀 엉뚱한 결과를 초래할 수 있으므로 신중을 기할 것</li>
    </ul>
  </li>
  <li>이제 무정보적 사전 분포에 대한 2가지 예를 살펴볼 것이다.</li>
</ul>

<hr />

<p><strong>예제 1 : Local parameter</strong></p>

<script type="math/tex; mode=display">p(x|\mu) = f(x-\mu) \qquad{(2.232)}</script>

<ul>
  <li>이러한 파라미터 \( \mu \) 를 지역 파라미터 (location paramter)라고 부른다.</li>
  <li>이런 형태의 밀도 함수를 가지는 형태를 <strong>translation invariance</strong> 만족한다고 하는데 아래와 같은 성질이 만족하기 때문이다.</li>
  <li>\( \widehat{x}=x+c \) 인 경우 \( \widehat{\mu}=\mu+c \) 가 성립</li>
</ul>

<script type="math/tex; mode=display">p(\widehat{x}|\widehat{\mu})=f(\widehat{x}-\widehat{\mu}) \qquad{(2.233)}</script>

<ul>
  <li>즉, 밀도의 모양이 동일한 형태로 유지되고 이동만 되는 형태.
    <ul>
      <li>\( x \) 가 \( c \) 만큼 이동하면 \( \mu \) 도 알아서 \( c \) 만큼 이동</li>
      <li>어떠한 위치에 있던지 확률 밀도의 값이 동일해져야 한다.</li>
    </ul>
  </li>
  <li>사전 분포 정하기
    <ul>
      <li>사전 분포를 적용한 뒤에도 <em>translation invariance</em>  속성이 유지되어야 한다.</li>
      <li>즉, \( A \le \mu \le B \) 의 범위가 \( A-c \le \mu \le B-c \) 로 이동해도 같은 밀도 함수 값이 나와야 한다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\int_{A}^{B}p(\mu)d\mu = \int_{A-c}^{B-c}p(\mu)d\mu = \int_{A}^{B}p(\mu-c)d\mu \qquad{(2.234)}</script>

<ul>
  <li>위의 식이 항상 성립하려면 다음 조건이 만족되어야 한다.</li>
</ul>

<script type="math/tex; mode=display">p(\mu-c)=p(\mu) \qquad{(2.235)}</script>

<ul>
  <li>이 말은 결국 \( p(\mu)=const \) 라는 의미가 된다.</li>
  <li>
    <p><em>translation invariance</em>  속성을 만족하는 \( p(x|\mu) \) 분포에 대해서는 사전 분포로 상수 값을 가지는 분포를 사용하면 된다.</p>
  </li>
  <li>가우시안 분포에서의 무정보적 사전 분포의 사용
    <ul>
      <li>가우시안 분포에서 평균 파라미터를 살펴보면, <em>translation invariance</em> 속성을 만족한다는 것을 알 수 있다.</li>
      <li>모양을 이동해도 중앙에 평균이 온다는 것은 자명한 사실. 평균 값은 local parameter 이다.</li>
      <li>정규 분포를 공액 사전 분포로 사용한다고 해보자. \( p(\mu|\mu_0, \sigma_0^2) = N(\mu|\mu_0, \sigma_0^2) \)</li>
      <li>이 때 \( \sigma_0^2\rightarrow\infty \) 로 하면 사후 확률 분포의 평균 값은 \( \mu_{N\rightarrow\infty}=\mu_{ML} \) 이 된다. (식 2.141 참고)</li>
      <li>이러면 사전 분포의 영향력이 0가 된다. 이것이 무정보적 사전 분포.</li>
      <li>그냥 간단하게 상상을 해봐도 가우시안 분포에서 \( \sigma^2\rightarrow\infty \) 이면 종 모양이 점점 옆으로 퍼지면서 직선의 형태가 될 것임.</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>예제 2 : Scale parameter</strong></p>
<ul>
  <li>확률 밀도가 다음을 만족한다고 하자.</li>
</ul>

<script type="math/tex; mode=display">p(x|\sigma)=\frac{1}{\sigma}f\left(\frac{x}{\sigma}\right) \qquad{(2.236)}</script>

<ul>
  <li>여기서 \( \sigma&gt;0 \) 이다. 이 때의 \( \sigma \) 파라미터를 <strong>scale parameter</strong> 라고 한다.</li>
  <li>\( \widehat{x}=cx \) 이고, \( \widehat{\sigma}=c\sigma \) 일 때 다음을 만족한다면,</li>
</ul>

<script type="math/tex; mode=display">p(\widehat{x}|\widehat{\sigma})=\frac{1}{\widehat{\sigma}}f\left(\frac{\widehat{x}}{\widehat{\sigma}}\right) \qquad{(2.237)}</script>

<ul>
  <li>
    <p>이를 <strong>scale invariance</strong> 를 만족한다고 한다.</p>
  </li>
  <li>
    <p>사전 분포 정하기</p>
    <ul>
      <li>local parameter 와 유사하게 사전 분포를 적용후에도 이 속성을 만족시키려면,</li>
      <li>사전 분포를 적용한 뒤에서 scale invarience 속성이 계속 유지되어야 한다.</li>
      <li>이 의미는 \( A \le \sigma \le B \) 일 때 \( A/c \le \sigma \le B/c \) 를 만족해야 한다는 것이다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\int_{A}^{B}p(\sigma)d\sigma = \int_{A/c}^{B/c}p(\sigma)d\sigma = \int_{A}^{B}p\left(\frac{1}{c}\sigma\right)\frac{1}{c}d\sigma \qquad{(2.238)}</script>

<ul>
  <li>따라서 위의 식이 항상 성립해야 한다.</li>
  <li>이를 정리하면 다음과 같다.</li>
</ul>

<script type="math/tex; mode=display">p(\sigma) = p\left(\frac{1}{c}\sigma\right)\frac{1}{c} \qquad{(2.239)}</script>

<ul>
  <li>이러한 조건을 만족하기 위해서는 \( p(\sigma) \propto 1/\sigma \) 여야 한다.
    <ul>
      <li>\( f(\sigma) = K/\sigma \) 로 해서 대입해보면 성립함을 알 수 있다.</li>
    </ul>
  </li>
  <li>스케일 파라미터의 경우 사전 분포로 \( 1/\sigma \) 를 사용하면 된다.</li>
  <li>
    <p>또한 \( p(\sigma) \) 또한 전체 적분 값에 대해서는 발산하므로 improper prior 임을 알 수 있다.</p>
  </li>
  <li>한 가지 재미난 사항을 언급하고 넘어가도록 하자.
    <ul>
      <li>보통 scale 파라미터의 경우 로그( \( \ln(\cdot) \) )를 적용하여 사용하는 것이 더 편리할 때가 있는데,</li>
      <li>다음과 같은 이유이다.
        <ul>
          <li>\( x=g(\sigma)=\ln(\sigma) \) 라고 하면</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">p_{\sigma}(\sigma) = p_x(x)\left|\frac{dx}{d\sigma}\right|=p_x(g(\sigma))|g'(\sigma)|=p_x(\ln(\sigma))\cdot\frac{1}{\sigma} \propto p_x(\ln(\sigma))p_{\sigma}(\sigma)</script>

<ul>
  <li>결국 \( p_x(\ln(\sigma)) = const \) 라는 결과를 얻게 된다.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{llc}\sigma & \ln\sigma & range \\
1\le\sigma\le 10 & 0 \le \ln\sigma \le \ln 10 & \ln 10 \\
10\le\sigma\le 100 & \ln 10 \le \ln\sigma \le 2\ln 10 & \ln 10 \\
100\le\sigma\le 1000 & 2\ln 10 \le \ln\sigma \le 3\ln 10 & \ln 10 \end{array} %]]></script>

<ul>
  <li>위의 표를 표면 \( \ln \) 함수를 사용하는 경우 범위의 간격이 동일해진다는 것을 알 수 있다.</li>
  <li>즉, 각 범위에서의 밀도 값도 같아진다.</li>
  <li>
    <p>따라서 스케일 파라미터의 경우 \( ln \) 를 이용하여 변환된 밀도 함수를 계산하는게 실제 계산에 더 편리한 경우가 많다.</p>
  </li>
  <li>가우시안 분포에서의 무정보적 사전 분포의 사용
    <ul>
      <li>평균값에 대한 무정보적 사전 분포를 확인했던 것과 마찬가지로, 분산값에 대한 무정보적 사전 분포를 고려해 볼 수 있는데</li>
      <li>이 때 분산이 바로 스케일 파라미터 속성을 만족한다.</li>
      <li>우선 평균에 대해 \( \tilde{x}=x-\mu \) 를 적용하고 이를 스케일 파라미터인 분산값에 대해 전개하면</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">N(x|\mu, \sigma^2) \propto \sigma^{-1}\exp\{-(\tilde{x}/\sigma)^2\} \qquad{(2.240)}</script>

<ul>
  <li>앞서 이야기한대로 수식을 전개할 때에는 \( \lambda=1/\sigma \) 인 정확도(precision)로 표현하는게 더 편하다.</li>
  <li>\( p(\sigma) \propto 1/\sigma \) 는 \( p(\lambda) \propto 1/\lambda \) 이므로 무리없게 전개되고, 공액 사전 분포인 감마(Gamma) 사전 분포가 비정보적 사전 분포가 되려면,
    <ul>
      <li>사후분포에 대해 사전 분포의 영향력이 없어야 한다.</li>
      <li>따라서 사전 분포 \( Gam(\lambda|a_0, b_0) \) 에서 \( a_0=b_0=0 \) 이어야 한다.</li>
    </ul>
  </li>
</ul>


    </article>
  </div>
</div>

      
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'leekwoongithub'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/kyo-ml-study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
