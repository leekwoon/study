<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>개발 일지 (2017-12-06)</title>
  <link rel="stylesheet" href="/study/css/main.css">
  <link rel="stylesheet" href="/study/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/study/docs/temp/dc-plan.html">
  <script src="/study/js/jquery-1.12.0.min.js"></script>
  <script type="text/javascript"src="http://cdn.mathjax.org/mathjax/latest/MathJax.js">
  MathJax.Hub.Config({extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>
  <link rel="shortcut icon" href="/study/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/study/"><strong>study</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/study/categories/temp"><strong>Temp</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/statistical_inference"><strong>Statistical Inference</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/unity"><strong>Unity</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/rl_study"><strong>RL</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/opt_study"><strong>Optimization</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/web_study"><strong>Web Study</strong></a>
            </li>
            
                      
          
          
            
            <li >
              <a href="/study/categories/piano"><strong>Piano</strong></a>
            </li>
            
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  <div class="col-sm-3">
    <h2>Digital Curling</h2>
    <ul class="nav nav-pills nav-stacked">
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      <li class="active " >
        <a href="/study/docs/temp/dc-plan">개발 일지 (2017-12-06)</a>
      </li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
  <div class="col-sm-9">
    
    <div class="navbar-right">
      <a target="_blank" href="http://github.com/leekwoon/study/blob/gh-pages/docs/temp/dc-plan.md">
        <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
      </a>
    </div>
    

    <header>
      <h1>개발 일지 (2017-12-06)</h1>
    </header>
    <hr class="title">
    <article>
      <h1 id="section">개발 일지</h1>

<h2 id="to-do">To-Do</h2>
<ul>
  <li>[ ] deep hasing + MCTS to consider similar states
    <ul>
      <li>baseline 참고: http://cs.unc.edu/~zhenni/blog/notes/Deep%20Hashing.html</li>
      <li>Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image Retrieval and Person Re-identification</li>
      <li>Deep Hashing Network for Efficient Similarity Retrieval</li>
    </ul>
  </li>
</ul>

<h2 id="section-1">컬링이란</h2>

<ul>
  <li>Turn based strategy game</li>
  <li>Legal moves exist infinitely (<strong>continuous action</strong>)</li>
  <li>Many kinds of <strong>uncertainty</strong> exists</li>
</ul>

<h2 id="section-2">디지털 컬링이란</h2>
<ul>
  <li>Curling Game SW to analyze and discuss the strategy of curling</li>
  <li>Sweeping is not considered</li>
  <li>Uncertainty is more simple</li>
</ul>

<h2 id="section-3">관련 연구들</h2>
<ul>
  <li>KR UCT (Kernel Regression Upper Confidence Bounds Applied to Trees)<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>
    <ul>
      <li>Approach: Kernel regression + MCTS</li>
      <li>Assumptions: good actions models are given</li>
      <li>장점: efficiently explore continuous actions/states</li>
      <li>단점: slow and hard to learn from executions</li>
    </ul>
  </li>
  <li>Jiritsu-kun<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>
    <ul>
      <li>Approach: game tree search + evaluation function based on Domain Knowledge</li>
      <li>Assumption: goodness of the current state can be evaluated based on Domain</li>
      <li>장점: fast and intuitive</li>
      <li>단점: less accurate in complex game state</li>
    </ul>
  </li>
  <li>Alphago<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>
    <ul>
      <li>Approach: Deep Learning + Monte Carlo Tree Search (MCTS)</li>
      <li>Assumption: discrete states/actions</li>
      <li>Benefite: learning the game strategy from experience</li>
      <li>Limitation: discretization of continuous space/action</li>
    </ul>
  </li>
  <li>Ayumu:
    <ul>
      <li>장점:</li>
      <li>단점:</li>
    </ul>
  </li>
</ul>

<h2 id="deep-learning-model-for-digital-curling">Deep Learning model for Digital curling</h2>
<ul>
  <li>기존의 KR_UCT 방식을 따라가려했지만, KR_UCT 모델의 성능은 잘 짜여진 rule-based rollout policy에 크게 영향을 받는다.</li>
  <li>컬링의 Donmain Knowledge 에 기반하여 룰을 하나하나 구현하기보다, Deep learning Model 만을 이용하여 최적의 전략을 추천하고자 한다.</li>
  <li>2016년도 우승 모델을 baseline 모델로 잡고, 딥러닝 모델을 학습시키기 위하여 (Game AI Tournament) GAT 2016 우승 프로그램 데이터를 수집하였다.</li>
</ul>

<p><strong>Training Policy Network</strong></p>

<ul>
  <li>KR UCT dependent on rule based rollout policy</li>
  <li>To learn the policy, We collected data using AyumuGAT1 (winner of 2016 competition) to learn a policy (about 500, 000 shot logs)</li>
  <li>Rule 기반 방법론을 극복하기 위하여, Policy Netork를 학습</li>
  <li><strong>Input</strong> is 32 x 32 x 31 image stack consisting  of 31 feature planes
<img src="/study/images/temp/dc-1.png" alt="" class="center-block" /></li>
  <li><strong>Output</strong> is 32 x 32 x 2 image with a probability for each shot
<img src="/study/images/temp/dc-2.png" alt="" class="center-block" /></li>
  <li><strong>Network architecture</strong>
<img src="/study/images/temp/dc-3.png" alt="" class="center-block" /></li>
  <li>There exists several problem of Policy Network
    <ul>
      <li>Most of shots deliver stone in the house (only small number of take out shot in the game)</li>
      <li>In the case take out shot their can be many candidates. Thus, It  has relatively small probability compared to shot of other strategy</li>
      <li>Shot data is imbalanced</li>
      <li>There must be existing the loss of variety of the strategies. (continuous -&gt; 32x32)</li>
      <li>The optimal shot from our policy net might not consider the riskiness of the strategy by the shot uncertainty.</li>
    </ul>
  </li>
</ul>

<p><strong>Data Analysis</strong></p>

<p><img src="/study/images/temp/dc-4.png" alt="" class="center-block" />
<img src="/study/images/temp/dc-5.png" alt="" class="center-block" /></p>

<p><strong>Training Value Network</strong></p>

<ul>
  <li>Assumption
    <ul>
      <li>Learning expected VALUE given game configuration is much easier than learning where to shot the stone</li>
    </ul>
  </li>
  <li>Problem
    <ul>
      <li>HIGH VARIANCE of the score makes training difficult</li>
    </ul>
  </li>
  <li>To overcome problem
    <ul>
      <li>Given state <script type="math/tex">s_{t+1}</script>, instead of learning final score, learning from the value of next state <script type="math/tex">f_\theta(s_{t+1})</script> using current value network <script type="math/tex">f_{\theta}</script></li>
      <li>Use virtual simulation to consider uncertainty. Thus, given <script type="math/tex">s_t</script> simulation is done with <script type="math/tex">a_t + N(a_t, \Sigma)</script> to move to <script type="math/tex">s_{t+1}</script></li>
      <li>Multi task learning (one network learns both policy and value)</li>
    </ul>
  </li>
  <li>Another Problem
    <ul>
      <li>We have to consider infinite number of actions to choose action since <script type="math/tex">\pi(s_t) = \text{argmax}_a \Sigma_a p(s_{t+1}\vert s_t, a)[f_{\theta}(s_{t+1})]</script></li>
      <li>Thus, We decide to use <strong>Policy Net + Value Net + Continuouse Search</strong></li>
    </ul>
  </li>
</ul>

<hr />
<p><img src="/study/images/temp/dc-6.png" alt="" class="center-block" /></p>

<p><strong>알고리즘</strong></p>

<ol>
  <li><script type="math/tex">\text{Initialize}</script> <script type="math/tex">k</script> <script type="math/tex">\text{actions using Policy Network and evaluate using virtual simulation}</script>
<script type="math/tex">\;\;a_{1,t} = \text{argmax}_{a\in \text{grid}}P(a\vert s_t)</script>
<script type="math/tex">\;\;a_{2,t} = \text{argmax}_{a\in (\text{grid}-a_{1,t})}P(a\vert s_t)</script>
…
<script type="math/tex">\;\;v_{i,t}=f(s_{t+1}\vert s_t, a_{i,t})</script></li>
  <li><script type="math/tex">\text{Repeat}</script> <script type="math/tex">n</script> <script type="math/tex">\text{times}</script>
 <script type="math/tex">\;\;\;\text{Select action based on UCB (Upper Confidence Bound).}</script>
 <script type="math/tex">\;\;\;\text{Here, expected value is calculated by Kernel Regression}</script>
 <script type="math/tex">\;\;\;\; a_{selected, t} = \text{argmax}_{a \in A} E[v_t\vert a] + C \sqrt{\frac{\log\Sigma_{b\in A}W(b)}{W(a)}}</script>
 <script type="math/tex">\;\;\;\;\text{where,}\; E[v_t\vert a] = \frac{\Sigma_{b\in A}K(a,b)v_{b,t}}{\Sigma_{b \in A}K(a,b)}</script>
 <script type="math/tex">\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;W(a)=\Sigma_{b \in A}K(a,b)</script>
 <script type="math/tex">\;\;\;\text{Sample a new action from known gaussian distribution (which is known) and evaluate}</script>
 <script type="math/tex">\;\;\;\; a_{sample,t} \sim N(a_{selected,t, \Sigma})</script>
 <script type="math/tex">\;\;\;\; v_{sample,t}=f(s_{t+1}\vert s_t, a_{sample,t})</script></li>
  <li><script type="math/tex">\text{Select best action}</script> <script type="math/tex">a_{best, t}</script> <script type="math/tex">\text{based on LCB (Lower Confidence Bound)}</script>
<script type="math/tex">\;\;\;\; a_{best, t} = \text{argmax}_{a \in A} E[v_t\vert a] - C \sqrt{\frac{\log\Sigma_{b\in A}W(b)}{W(a)}}</script></li>
</ol>

<hr />
<p><strong>Improve performance with Self-Play</strong></p>

<ul>
  <li>Self-Play
    <ul>
      <li>Use previous algorithm, collect data using self-play games</li>
      <li>At each move, the following information is stored
        <ul>
          <li>The game state <script type="math/tex">s_t</script></li>
          <li>The best action <script type="math/tex">a_{best, t} = \text{argmax}_{a \in A} E[v_t\vert a] - C \sqrt{\frac{\log\Sigma_{b\in A}W(b)}{W(a)}}</script></li>
          <li>The expected value of the best action using kernel Regression <script type="math/tex">v_{best,t}=f(s_{t+1}\vert s_t, a_{best,t})</script></li>
          <li>final score <script type="math/tex">z_t</script></li>
        </ul>
      </li>
      <li>Here, label of value can be <script type="math/tex">\alpha z_t + (1-\alpha)v_{best,t}</script></li>
    </ul>
  </li>
  <li>Retrain Network
    <ul>
      <li>Sample a mini-batch from the last 1000,000 game states</li>
      <li>Retrain the current neural network on these game states</li>
    </ul>
  </li>
</ul>

<h2 id="section-4">참고문헌</h2>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>T. Yee, Timothy, V. Lisy, M. Bowling, Monte Carlo Tree Search in Continuous Action Spaces with Execution Uncertainty, International Joint Conference on Artificial Intelligence (IJCAI), 2016. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Masahito Yamamoto, Shu Kato, and Hiroyuki Iizuka. Digital curling strategy based on game tree search. In Computational Intelligence and Games (CIG), 2015 IEEE Conference on, pages 474–480.IEEE, 2015. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>D. Silver etc al., Mastering the game of Go with deep neural networks and tree search, Nature, 2016. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

    </article>
  </div>
</div>

    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/study" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
