<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Decoupled Neural Interfaces using Synthetic Gradients</title>
  <link rel="stylesheet" href="/stock/css/main.css">
  <link rel="stylesheet" href="/stock/css/font-awesome.min.css">
  <link rel="canonical" href="http://leekwoon.github.io/stock/docs/synthetic_gradients.html">
  <script src="/stock/js/jquery-1.12.0.min.js"></script>    
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link rel="shortcut icon" href="/stock/images/favicon.png" type="image/x-icon">
</head>

  <body>
    <header>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	        <span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand" href="/stock/"><strong>stock</strong></a>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
	      <ul class="nav navbar-nav navbar-right">
          

          
          
            
            <li >
              <a href="/stock/"><strong>Home</strong></a>
            </li>
            
                      
          
          
            
            <!-- for drop-down navi -->
            <li class="dropdown">
              <a href="/stock" class="dropdown-toggle" data-toggle="dropdown"><strong>Title
                  <b class="caret"></b></strong>
              </a>
              <ul class="dropdown-menu">
                
                
                <li >
                  <a href="/stock/docs/synthetic_gradients">Decoupled Neural Interfaces using Synthetic Gradients</a>
                </li>
                
                
              </ul>
            </li>
            
                      
          
	      </ul>
      </div>
    </div>
  </nav>
</header>


    <div class="container">
      <div class="row">
  
  <div class="navbar-right">
    <a target="_blank" href="http://github.com/leekwoon/stock/blob/gh-pages/docs/synthetic_gradients.md">
      <button type="button" class="btn btn-default"><i class="fa fa-github">&nbsp;Edit</i></button>
    </a>
  </div>
  
  <header>
    <h1 class="text-right">Decoupled Neural Interfaces using Synthetic Gradients</h1>
  </header>
  <hr class="title">
  <h4 class="text-right"><a href="http://arxiv.org/abs/1608.05343" target="_blank">http://arxiv.org/abs/1608.05343</a></h4>  
  <article>
    <ul>
  <li>참고 사항
    <ul>
      <li>논문 저작자인 Max Jadeberg 가 이미 이와 관련되어 <a href="https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/" target="_blank">블로그</a>에 따로 글을 올렸다.</li>
      <li>아래 내용 중 일부 이미지는 이 블로그에서 발췌되었다.</li>
    </ul>
  </li>
</ul>

<h3 id="introduction">Introduction</h3>

<ul>
  <li>방향성 신경망(directed neural network)은 다음의 step을 가짐.
    <ul>
      <li>입력 데이터를 입력받아 순 방향으로 데이터를 진행하면 계산.</li>
      <li>정의된 loss 함수로 나온 생성값을 역방향으로 전파. (backprop)</li>
    </ul>
  </li>
  <li>이 과정에 여러 가지 Locking이 생겨남.
    <ul>
      <li>(1) <strong><em>Forward Locking</em></strong> : 이전 노드에 입력 데이터가 들어오기 전까지는 작업을 시작할 수 없다.</li>
      <li>(2) <strong><em>Update Locking</em></strong> : forward 과정이 끝나기 전까지는 작업을 시작할 수 없다. (예로 backprop)</li>
      <li>(3) <strong><em>Backwards Locking</em></strong> : forward와 backward 과정이 끝나기 전까진 작업을 시작할 수 없다.</li>
    </ul>
  </li>
  <li>위와 같은 제약으로 인해 신경망은 순차적으로 동기적 방식으로 진행된다.</li>
  <li>학습이 이러한 과정이 당연해 보이지만 각 레이어에 비동기 방식을 도입하고 싶거나 분산 환경등을 고려하게 되면 이러한 제약이 문제가 된다.</li>
  <li>이 논문의 목표는 위에서 설명한 모든 Locking 을 해결하는 것은 아니고 backprop 과정 중 발생하는 update locking을 제거하고자 하는 것.</li>
  <li>이를 위해 레이어 \(i\) 의 weight \(w_i\) 를 backprop을 통해 업데이트 할 때 근사값을 사용하게 된다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f01.png" alt="figure.1" class="center-block" height="150px" /></p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial w_i} = \frac{\partial E}{\partial a_j} \frac{\partial a_j}{\partial w_i} = \frac{\partial E}{\partial z_j} \frac{\partial z_j}{\partial a_j} \frac{\partial a_j}{\partial w_i}</script>

<script type="math/tex; mode=display">a_j = \sum{w_i z_i}, \qquad z_j = h(a_j)</script>

<script type="math/tex; mode=display">\frac{\partial E}{\partial z_j} \equiv \delta_j, \qquad \frac{\partial z_j}{\partial a_j} = h'(a_j), \qquad \frac{\partial a_j}{\partial w_i} = z_i</script>

<script type="math/tex; mode=display">\frac{\partial E}{\partial w_i} = \delta_j h'(a_j) z_i</script>

<ul>
  <li>위의 식들은 논문 표기법을 좀 더 이해하기 쉬운 값으로 변경해 놓은 것이다. 그림을 참고하여 살펴보도록 하자.</li>
  <li>이제 마지막 레이어로부터 backprop 계산을 위한 \(\delta\) 값을 살펴보도록 하자.</li>
  <li>마지막 레이어가 \(k\) 라고 하고 하면,</li>
</ul>

<script type="math/tex; mode=display">\delta_k = y_k - t_k</script>

<ul>
  <li>중간 레이어에서의 \(\delta_j\) 도 이를 재귀적으로 이용하여 풀이할 수 있다.</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial E}{\partial z_j} = \delta_j = \sum_k \frac{\partial E}{\partial z_k} \frac{\partial z_k}{\partial z_j} = \sum_k \delta_k \frac{\partial z_k}{\partial a_k} \frac{\partial a_k}{\partial z_j}=\sum_k \delta_k h'(a_k) w_{jk} = \sum_k w_{jk} \delta_k h'(a_k)</script>

<ul>
  <li>이 논문은 이 수식을 다음과 같은 근사식으로 전환한다.</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial E}{\partial w_i} = f_{Bprop}\left(...\right) \frac{\partial z_j}{\partial w_i} \simeq \hat{f}_{Bprop}(z_j)\frac{\partial z_j}{\partial w_i}</script>

<ul>
  <li>이 아이디어는 한 레이어에서 출력 값을 전달한 뒤 backprop 단계에서 전달되는 에러 값 \(\delta\) 를 기다리지 않고,</li>
  <li>합성 그라디언트 (synthetic graidents) 값을 이용하여 바로 현재 레이어에서의 backprop 수행한다는 것이다. (Update Locking이 사라진다)
    <ul>
      <li>여기서 합성(synthetic)이란 표현은 그냥 비슷하게 gradient 값을 흉내낸 가짜 값을 의미한다. (바나나맛 우유에 바나나가 안들어있는 것처럼)</li>
    </ul>
  </li>
</ul>

<h3 id="decoupled-neural-interfaces-aka-dni">Decoupled Neural Interfaces (a.k.a DNI)</h3>

<ul>
  <li>합성 그라디언트를 반환해주는 모듈을 아래와 같은 그림으로 표기한다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f02.png" alt="figure.2" class="center-block" height="200px" /></p>

<ul>
  <li>이러한 모듈을 DNI (Decoupled Neural Interfaces) 라고 부른다.</li>
  <li>이를 이용하여 실제 어떻게 작업이 이루어지는지 살펴보자.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f03.png" alt="figure.3" class="center-block" height="300px" /></p>

<ul>
  <li>그림 설명
    <ul>
      <li>\(f_A\) : 이전 레이어로 논문에서는 모듈이라고 부른다.</li>
      <li>\(h_A\) : 모듈 A가 출력하는 출력값이다.</li>
      <li>\(M_B\) : synthetic gradient 를 생성해주는 모듈이다.</li>
      <li>\(S_B\) : 모듈 B의 일부 상태 정보를 전달한다.</li>
      <li>\(c\) : 연산에 필요한 부가적인 정보를 그냥 묶어서 \(c\) 라고 표현한다.</li>
      <li>\(\|\delta_A - \hat{\delta}_A \|\) : \(\hat{\delta}_A\) 를 위한 Loss 함수이다.</li>
      <li>이제 synthetic gradient 는 \(\hat{\delta}_A = M_B(h_A, s_B, c)\) 로 정의할 수 있다.</li>
    </ul>
  </li>
  <li>실제 동작은 아래 그림만 보면 바로 이해된다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f04.gif" alt="figure.4" class="center-block" height="200px" /></p>

<h3 id="synthetic-gradient-for-feed-forward-networks">2.1 Synthetic Gradient for Feed-Forward Networks</h3>

<ul>
  <li>이제 \(N\) 개의 레이어를 가진 feed-forward network에서 이를 활용하는 방법을 살펴보자.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f05.png" alt="figure.5" class="center-block" height="300px" /></p>

<ul>
  <li>그림과 같이 여러 층의 레이어에 모두 적용 가능하다.</li>
  <li>실제 동작되는 방식은 다음과 같다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f06.png" alt="figure.6" class="center-block" height="300px" /></p>

<ul>
  <li>그냥 딱 봐도 어떻게 진행되는지 쉽게 알 수 있다.</li>
  <li>일단 \(f_i\) 가 출력 \(h_i\) 를 \(M_{i+1}\) 에 전달하면 synthetic graident \(\hat{\delta}_i\) 를 바로 제공한다.</li>
  <li>\(f_i\) 는 forward 진행과는 무관하게 바로 backprop을 수행한다. (색상이 변경되었다.)</li>
  <li>이제 \(h_i\) 를 입력으로 받은 \(f_{i+1}\)는 동일하게 \(h_{i+1}\) 을 \(M_{i+1}\) 을 전달하여 \(\hat{\delta}_{i+1}\) 을 얻는다.</li>
  <li>사실 \(\delta_{i+1}\) 또한 synthetic gradient 이지만 이를 이용하며 바로 \(\delta_i\) 를 계산한뒤 \(M_{i+1}\) 을 업데이트 한다.</li>
  <li>이 과정을 계속 반복한다.</li>
</ul>

<h3 id="synthetic-gradient-for-reccurrent-networks">Synthetic Gradient for Reccurrent Networks</h3>

<ul>
  <li>RNN 에도 이를 적용할 수 있다. 일단 무한히 전개되는 RNN 을 상상해보자.</li>
  <li>즉, \(N \to \infty\) 가 되어 \(F_1^{\infty}\) 인 RNN이 된다.</li>
  <li>이를 그림으로 나타내면 다음과 같다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f07.png" alt="figure.7" class="center-block" height="70px" /></p>

<ul>
  <li>하지만 현실적으로는 이러한 모델을 계산하기 어렵다.</li>
  <li>따라서 보통은 다음과 같은 형태로 제한하여 실제 계산을 수행하게 된다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f08.png" alt="figure.8" class="center-block" height="100px" /></p>

<ul>
  <li>이를 식으로 표현해보자.</li>
  <li>\(N \to \infty\) 를 나타내는 식을 스텝 \(t\) 에서부터 \(T\) 까지의 식으로 나누어 tractable한 형태로 바꾼다.</li>
</ul>

<script type="math/tex; mode=display">\theta - \alpha \sum_{\tau=t}^{\infty} \frac{\partial L_\tau}{\partial \theta} = \theta - \alpha\left( \sum_{\tau=t}^{t+T} \frac{\partial L_{\tau}}{\partial \theta} + \left( \sum_{\tau=T+1}^{\infty}\frac{\partial L_{\tau}}{\partial \theta}\right)\frac{\partial h_T}{\partial \theta}\right) = \theta -\alpha\left( \sum_{\tau=t}^{t+T} \frac{\partial L_{\tau}}{\partial \theta} + \delta_T \frac{\partial h_T}{\partial \theta}\right)</script>

<ul>
  <li>
    <p>앞서 그림을 통해 설명했듯이 보통의 경우라면 이렇게 무한히 진행되는 RNN 계산이 어렵기 때문에 임의의 \(T\) 값을 정한 뒤 \(\delta_T=0\) 로 가정하고 식을 전개한다.</p>
  </li>
  <li>하지만 synthetic gradient 를 이용하면 그럴 필요 없다. \(\delta_T\) 를 근사할 수 있게 된다.</li>
  <li>이와 관련된 내용도 다음의 그림을 보면 쉽게 이해할 수 있다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f09.png" alt="figure.9" class="center-block" height="300px" /></p>

<ul>
  <li>이것도 <em>gif</em> 파일로 보면 더 이해하기 쉬울 것이다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f10.gif" alt="figure.10" class="center-block" height="100px" /></p>

<h3 id="experiments">Experiments</h3>

<h4 id="feed-forward-network">Feed-Forward Network</h4>

<ul>
  <li>
    <p>실험 결과를 살펴보자.</p>
  </li>
  <li>
    <p>환경</p>
    <ul>
      <li>데이터는 \(MNIST\) 와 \(CIFAR-10\).</li>
      <li>\(FCN\) 과 \(CNN\) 으로 각각 테스트.</li>
      <li>Hidden Layer의 수는 256 개로 고정</li>
      <li>모든 Layer에 \(BN\) 과 \(ReLU\) 를 사용함.</li>
      <li>모든 Layer에 \(DNI\) 적용.</li>
      <li>\(cDNI\) 는 \(DNI\) 에 이미지의 Label을 추가로 넣어 학습한 것을 의미.
        <ul>
          <li><em>conditional DNI</em> 라는 의미이다.</li>
          <li>사용된 Label 은 one-hot representation 방식이다. ( \(MNIST\) 와 \(CIFAR\) 모두 10개의 class)</li>
          <li><em>CNN</em> 에서는 이를 바로 넣기 힘드므로 \(C\) (channel) 에 one-hot 방식으로 mask를 추가하였다.
            <ul>
              <li>결국 10개의 추가 채널이 들어감.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><em>Optimizer</em> 는 \(Adam\) 방식을 사용함.</li>
      <li><em>batch_size</em> 는 256을 사용</li>
      <li><em>learning rate</em> 는 초기값으로 \(3 \times 10^{-5}\) 를 사용하고 \(300K\) 와 \(400K\) 에서 10배 감소</li>
      <li>사실 이러한 하이퍼 파라미터 값은 최적화 된 상태는 아니다.</li>
    </ul>
  </li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f11.png" alt="figure.11" class="center-block" height="300px" /></p>

<ul>
  <li>간단히 결과만 보면 \(cDNI\) 가 기존의 방식보다 정확도가 더 좋거나 비슷한 수준으로 보여짐.
    <ul>
      <li>가장 좋은 결과는 \(cDNI\) 모델에 <strong><em>linear synthetic graident</em></strong> 를 사용한 모델이다.</li>
    </ul>
  </li>
  <li><em>synthetic gradient</em> 도 단순한 신경망으로 구성한다. (여러 레이어로 테스트)
    <ul>
      <li>만약 0개의 Layer 인 경우 \(\widehat{\delta} = M(h) = \phi_w h + \phi_b\) 가 된다.</li>
      <li>분류를 위한 <em>Loss</em> 함수는 <em>cross-entropy</em> 를 사용하였고 <em>synthetic gradient</em> 를 위한 <em>Loss</em> 함수는 \(L_2\) 를 사용하였다.</li>
      <li>초기에 맨 마지막 레이어에서 내려오는 <em>synthetic gradient</em> 는 0으로 설정.</li>
    </ul>
  </li>
</ul>

<hr />

<p><img src="/stock/images/synthetic_gradients/f12.png" alt="figure.12" class="center-block" height="220px" /></p>

<ul>
  <li>위 그림은 <em>synthetic graident</em> 가 실제 효과가 있는지를 확인하는 실험.</li>
  <li>\(x\) 축은 확률 값으로 \(p_{update}\) 확률을 나타낸다.</li>
  <li>왼쪽 그림은 <em>backprop</em> 만 <em>synthetic graident</em> 를 적용한 것이고,</li>
  <li>오른쪽 그림은 <em>forward</em> 도 <em>synthetic graident</em> 를 적용한 것이다.
    <ul>
      <li>따라서 완전히 <em>async</em> 로 동작하는 <em>NN</em> 이다.</li>
      <li>이 때 확률 \(p_{update}\) 는 <em>forward</em> , <em>backward</em> 를 함께 선택하는 확률값으로 사용.</li>
    </ul>
  </li>
  <li><strong>Complete Unlock</strong></li>
  <li><em>forward locking</em> 도 없애도록 하여 완전하게 <em>async</em> 로 동작할 수 있는 모드를 의미한다.</li>
  <li>이 모드는 모든 레이어에서  <em>synthetic gradient</em> 를 사용하는 것 뿐만 아니라 입력 또한 이런 방식으로 <em>synthetic input</em> 을 생성한다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f13.png" alt="figure.13" class="center-block" height="220px" /></p>

<hr />

<h4 id="recurrent-neural-net">Recurrent Neural Net</h4>

<ul>
  <li>Reccerent 모델로 모두 \(LSTM\) 을 사용. 여기에 <em>synthetic graident</em> 를 적용한다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f14.png" alt="figure.14" class="center-block" height="120px" /></p>

<ul>
  <li>문자열 복사 문제를 처리해본다.
    <ul>
      <li>\(N\) 개의 문자열을 읽어 복사를 수행하는 연산이다.</li>
      <li>Repeat 모드의 경우 숫자 \(R\) 을 읽어 복사를 \(R\) 번 반복하게 된다.</li>
    </ul>
  </li>
  <li>위 수치값은 해당 \(T\) 를 사용하였을 때 실제 제대로 복원되는 Seq 의 길이를 의미함. (즉, 클수록 좋은 값이다)</li>
  <li>
    <p>단, Penn Treebank 는 에러값을 의미한다. (작을수록 좋은 값이다.)</p>
  </li>
  <li>여기서 Aux 는 다음과 같은 작업을 수행하는 보조 기능이다.</li>
</ul>

<p><img src="/stock/images/synthetic_gradients/f15.png" alt="figure.15" class="center-block" height="200px" /></p>


  </article>
</div>

    </div>
    <div class="container">    
      <footer class="container-fluid">
  <div class="row">
    <div class="col-xs-6 text-left">
      <a href="http://github.com/leekwoon/stock" target="_blank">
        <p><i class="fa fa-github fa-lg">&nbsp;</i>Github</p>
      </a>
    </div>
    <div class="col-xs-6 text-right">
      <a href="http://github.com/leekwoon" target="_blank"><i class="fa fa fa-user">&nbsp;&nbsp;Who am I</i></a>
    </div>
  </div>
</footer>


    </div>      
  </body>
</html>
