---
layout: page-sidenav
group: "Introduction to Optimization"
title: "1. Optimization Basics"
---

Overview
--------
- Optimization Basics
- Basic Mathmetics
- Unconstrained optimization: Necessary and Sufficient Conditions

What is Optimization?: Mathematical Formulation
-----------------------------------------------

$$\underset{x}{\text{min}}f(x)$$

$$\text{subject to }x \in X \subseteq \mathbf{R^n}$$

- $$f:\mathbf{R^n\to R}$$: objective (or cost) function
- $$x$$에 대해서 minimization
- $$x \in \mathbf{R^n}$$: decision variables
- $$X \subseteq \mathbf{R^n}$$: constraint set
- optimization 문제는 $$X$$가 empty가 아닐때 feasible 하며, $$x \in X$$를 feasible point라 부른다 
- $$X = \mathbf{R^n}$$이면 unconstrained optimization problem이라 부름
- A feasible point $$x^* \in X$$는 optimal point if

$$f(x^*) \le f(x), \forall x \in X $$

- 이때, $$f^* := f(x^*)$$을 optimal value라 한다
- 목표: Constraint set $$X$$에 대해 objective function을 minimize하는 optimal point 찾기
 
$$\underset{x}{\text{min}}f(x)$$

$$\text{s.t. }g_i(x) \le 0, \forall i = 1,2,...,m$$

$$h_j(x) \le 0, \forall j = 1,2,...,m$$

- 좀더 구체적으로 optimization problem을 정의하면 위와같이 나타낼 수 있다.
	- The feasible set: $$Y=\{x \in X \vert g_i(x)\le 0,h_j(x)=0,\forall i,j\}$$
- 단, minimization problem만 고려하도록 하자.
- 왜냐하면, maximization problem은 minimization problem으로 쉽게 convert될 수 있기에

$$\underset{x}{\text{max}}f(x) = -\underset{x}{\text{min}}-f(x)$$

- Major issues for optimization problem
	- Existence: optimal solution이 있나 없나? (**중요함!!!**)
	- Uniqueness: 만약 optimal solution이 있다면 unique한가?
	- Characterization: optimal solution을 characterize하기 위한 analytical tool이 있나? (differentiability, simple problem)
	- Algorithms: 만약 analytical하게 풀 수 없다면 어떤 방법으로 풀 것인가? (gradient descent)

Existence of an Optimal Solution
--------------------------------
Example:

$$\underset{x}{\text{min}}x \Rightarrow f(x)=x \to -\infty \text{ as } x \to -\infty$$

$$\underset{x}{\text{min}}e^{-x} \Rightarrow f(x)=e^{-x} \to 0 \text{ as } x \to \infty$$ 

- 위의 예시에서는 optimal solutions이 존재하지 않는다.
	- 비록 두번째 예시에서, objective function이 0에 bound 되어있어도 minimum은 존재하지 않는다.

**Infinum**
- **Definition:** A real number m is called the infimum of a function $$f(x)$$ over a set $$X \text{ if } \forall \epsilon, \exists x_\epsilon \in S \text{ s.t}$$

$$f(x_\epsilon) \le m+\epsilon$$
$$f(x) \ge m ,\forall x$$
- 이때 $$m=\underset{x \in X}{\text{inf}}f(x)$$라 표기한다.
	- infimum은 gratest lower bound로 생각하면 됨
	- sumpremum은 smallest upper bound
- supremum도 같은 방식으로 정의 가능

**Weierstrass's theorem**
- $$\text{if } f:X\to \mathbf{R}$$ is a continous function defined on a compact set(closed and bounded) $$X \subseteq \mathbf{R^n}$$, then there exist $$x^*,y^* \in X$$ such that

$$f(x^*)=\underset{x \in X}{\text{inf}}f(x), f(y^*)=\underset{x \in X}{\text{sup}}f(x)$$

That is 

$$f(x^*) \le f(x) \le f(y^*), \forall x \in X$$

- 이런 이유로, compact set $$X$$에 대해 minimizer와 maximizer 둘다 존재
	- minimimum, maximum point를 guarantee해주는 강력한 theorem
	- 증명 생략
	- 이 theorem을 이해하기 위해서는 아랫것들을 알아야 함.
		- compact set
		- continuity
		- norm
		- bounded set
		- closed set

Basic Mathematics
----------------

- vector space = linear space = linear vector space
- A linear space over a field $$\mathbf{F,(V,F)}$$, consist of a set $$\mathbf{V}$$ of vectors, a field $$\mathbf{F}$$, and two operations, vector addition and scalar multiplication

**Normed vector space**
- Length of the vector A function $$\vert\vert x\vert\vert: \mathbf{R^n} \to \mathbf{R}$$ is said to be a norm if the following properties hold
	- $$\vert\vert x\vert\vert \ge 0 \text{ and } \vert\vert x\vert\vert=0 \text{ iff } x=0$$ (separate points)
	- $$\vert\vert ax\vert\vert=\vert a\vert\vert\vert x\vert\vert$$ (absolute homogeniety)
	- $$\vert\vert x+y\vert\vert \le \vert\vert x\vert\vert+\vert\vert y\vert\vert$$ (triangular inequality)
- Example:

$$||x||_1:=\sum|x_i| \text{, } ||x||_2:= (\sum |x_i|^2)^{1/2}\text{, } ||x||_\infty := max|x_i|$$
- **Fact**: For any norm에 대해서,

$$c_1||x||^{\prime} \le ||x|| \le c_2||x||^{\prime}, \forall x \in \mathbf{R^n}$$
- 따라서 finit-dimensional space에서 convergence 또는 continuity를 정의하기 위해 임의의 norm을 사용해도 된다.

**Inner Product:**
- measure angle of two vectors

**Convergence**
- **Definition:** A real-valued sequnce $$x^1,x^2,...$$ converges to $$x^*$$ if for each $$\epsilon >0$$ there exist $$K = K(\epsilon)$$ such that

$$\vert x^k-x^*\vert < \epsilon, \forall k \ge K$$

- and we write $$\underset{k \to \infty}{\text{lim}}x^k = x^*$$ or $$x^k \to x^*$$ as $$k \to \infty$$

**Ball**
- **Definition:** A ball $$B_\delta(x)$$ with a radius of $$\delta > 0$$ is a subset of $$\mathbf{R^n}$$, defined by

$$B_\delta(x) = \{s \in \mathbf{R^n} : ||x-s|| \le \delta \}$$

- 참고로 $$B_\delta(x)$$모양은 어떤 norm을 쓰냐에 따라 다름 (하지만 이전에 **Fact**에서 보였듣이 어떤 norm을 써도 상관 없다.)

![ball]({{site.baseurl}}/images/opt_study/intro_opt/1.1.png)


**open**
- **Definition:** A set $$X \in \mathbf{R^n}$$ is open if, for each $$x \in X$$, there exists $$\delta > 0$$ such that there is a ball $$B_\delta(x)$$ around $$x$$ such that $$B_\delta(x) \subseteq X$$

![open]({{site.baseurl}}/images/opt_study/intro_opt/1.2.png)

**closed**
- **Definition:** A set $$X$$ is closed if $$X^c$$ is open.
	- E.g. (0,1) is open
	- E.g. [0,1] is closed
	- E.g. (0,1] is neither open nor closed

**Bounded**
- **Definition:** A set $$X \subseteq \mathbf{R^n}$$ is bounded if there exists $$M \le \infty$$ such that

$$||x|| \le M, \forall x \in X$$

**Continous**
- **Definition:** A function $$f:X \to \mathbf{R}$$ where $$X \subseteq \mathbf{R^n}$$ is said to be continous at $$x^* \in X$$ if for every sequence $$\{y^k\}$$ with $$\underset{k \to \infty}{\text{lim}}y^k=x^*$$, we have

$$\underset{k \to \infty}{\text{lim}}f(y^k)=f(\underset{k \to \infty}{\text{lim}}y^k) = f(x^*)$$

Equivalently, for each $$\epsilon>0$$, there exists $$\delta>0$$ such that for all $$x \in X$$,

$$||x-x^*||<\delta \to |f(x)-f(x^*) \le \epsilon$$

Existence of an Optimal Solution (continue...)
----------------------------------------------

- 이제는 Weierstrass's theorem을 위의 baisc mathematics을 바탕으로 이해할 수 있다.
- 이와 관련된 두가지 theorem을 더 살펴보자.
- **Theorem:** Let $$f:X\to \mathbf{R}$$ be a continuous function. Suppose that there exists $$c>0$$ such that $$L_c(f)\land X$$ is a compact set where

$$L_c(f) = \{x \in X \vert f(x) \le c \}$$

then there is a minimizer to the problem

$$\underset{x \in X}{\text{min}}f(x)$$

- **Theorem:** If a function is continuous and coercive, i,e, $$\underset{\vert\vert x\vert\vert \to \infty}{\lim}f(x) = \infty$$, then a minimizer exists for the problem

$$\underset{x \in \mathbf{R^n}}{\text{min}}f(x)$$

First and Second Order Conditions
---------------------------------

- 다음과 같은 optimization problem을 고려해보자.

$$\underset{x \in X}{\text{min}}f(x)$$

**Local minimum**
- **Definition:** A point $$x^* \in X$$ is said to be a local minimum point of $$f$$ over $$X$$ if there exists $$\delta>0$$ such that

$$f(x^*) \le f(x), \forall x \in B_{\delta}(x^*) \land X$$

that is

$$f(x^*) \le f(x), \forall x  \in X, \vert x-x^*\vert \le \delta$$

**Global minimum**
- **Definition:** A point $$x^* \in X$$ is said to be a global minimum point of $$f$$ if

$$f(x^*) \le f(x), \forall x \in X$$

**Feasible direction**
- **Definition:** Given $$x\in X$$, a vector $$d$$ is a feasible direction at $$x$$ if there exists a constant $$\overline{\alpha}$$ such that $$x+\alpha d \in X$$ for all $$0 \le \alpha \le \overline{\alpha}$$

![feasible direction]({{site.baseurl}}/images/opt_study/intro_opt/1.3.png)

**gradient vector**
- **Definition:** if a function $$f:X \to \mathbf{R}$$ is differentiable at $$x \in X$$, then we write its gradient vector as follows

$$\nabla f(x)=\begin{pmatrix} \frac{\partial f(x)}{\partial x_1} \\ \vdots \\ \frac{\partial f(x)}{\partial x_n} \end{pmatrix}$$

**First order necessary condition**
- Let $$X \subseteq \mathbf{R^n}$$, and suppose that $$f$$ is differentiable on $$X$$. If $$x^*$$ is a local minimum point of $$f$$ over $$X$$, then $$x^*$$ satisfies

$$\nabla f^T(x^*)d=\sum_{i=1}^{n}\frac{\partial f(x)}{\partial x_i} \ge 0$$

for all feasible directions $$d$$ at $$x^*$$

- 참고로 이것은 **필요조건**이다. 즉, 위의 조건을 만족하는 어떠한 $$\overline{x}$$가 꼭 local minimum인건 아니다.
- **Proof**:

Let $$d$$ be a feasible direction, and we define

$$x(\epsilon) = x^* + \alpha d, \forall \alpha \in [0,\overline{\alpha}]$$

and

$$g(\alpha) = f(x(\alpha))$$

Then the Taylor series around $$\alpha \approx 0$$ implies

$$g(\alpha) = g(0)+g^{\prime}(0)\alpha+o(\alpha)$$

where the term $$o(\alpha)$$ is negligible when $$\alpha$$ is close to zero.
We have

$$g^{\prime}(\alpha)=\sum_{i=1}^{n}\frac{\partial g}{\partial x_i}\frac{dx_i}{d\alpha}=\nabla f^T(x)d$$

and

$$g^{\prime}(0)=\nabla f^T(x^*)d$$

Hence, for sufiiciently small $$\alpha$$, we have

$$g(\alpha) = f(x^*)+\alpha \nabla f^T(x^*)d$$

$$\Leftrightarrow g(\alpha)-f(x^*) = \alpha \nabla f^T(x^*)d$$

if $$\nabla f^T(x^*)d <0 $$, then for sufiiciently small value of $$\alpha >0$$,

$$g(\alpha)-f(x^*) < 0$$

which contradicts the fact that $$f(x^*)$$ is the local minimum point. Hence,

$$\nabla f^T(x^*)d \ge 0$$

This completes the proof.

- 추가적인 제약조건 아래서 더 나은 정리를 이끌어 낼 수 있다.
- **Definition:** $$\text{ int}(X)$$ means interior of $$X$$. The interior of $$X$$ consists of all interior points of $$X$$.
- **Definition:** A point $$x\in X$$ is an interior point of $$X$$ if there exists $$\delta >0$$  such that $$B_{\delta}(x) \subseteq X$$

![]({{site.baseurl}}/images/opt_study/intro_opt/1.4.png)

- **Corollary:** if $$x^* \in \text{int}(X)$$ and is a local minimum point. then

$$\nabla f(x^*) = 0$$

- Proof: Since $$d$$ and $$-d$$ are both feasible directions,

$$\nabla f^T(x^*)d\ge 0,-\nabla f^T(x^*)d\ge 0 \Rightarrow \nabla f^T(x^*)=0$$

- 필요조건임을 명심!

**Hessian matrix**
- **Definition:** if a function $$f:\mathbf{R^n} \to \mathbf{R}$$ is twice differentiable at $$x \in X$$, then its second derivative is as follows
 
$$\nabla^2 f(x)_{ij}=\frac{\partial^2 f(x)}{\partial x_i\partial x_j},i=1,2,...,n,j=1,2,...n$$

- 이때 $$H:=\nabla^2 f(x)$$.
	- $$H$$를 Hessian matrix라 부름
	- $$H$$는 n by n square satrix 
	- $$H$$는 symmetric matrix.
- A symmetric matrix $$H$$ is positive definite ($$H>0$$) if

$$z^THz > 0, \forall z \in \mathbf{R^n}$$

and is positive semi-definite ($$H \ge 0$$) if 

$$z^THz \ge 0, \forall z \in \mathbf{R^n}$$

- Note, $$H \ge 0$$ iff all the (real part) eigenvalues of $$H$$ are $$\ge 0$$

**Second order necessary condition**

- **Theorem:** Let $$X \subseteq \mathbf{R^n}$$, and let $$f$$ is twice differentiable. If $$x^*$$ is a local minimum point of $$f$$ over $$X$$, then for any feasible direction $$d$$ of $$x^*$$, we have
	- (1) $$\nabla f^T(x^*)d \ge 0$$
	- (2) if  $$\nabla f^T(x^*)d = 0$$, then $$d^T \nabla^2 f(x)d \ge 0$$
- **Proof:**

(1) is shown in the previous theorem. For the second part, first note that $$\nabla f^T(x^*)d = 0$$. Then with the same definition of $$g(\alpha)$$, its Taylor expansion to the second order leads to

$$g(\alpha)-g(0) = \frac{1}{2}g^{\prime\prime}(0)\alpha^2+o(\alpha^2)$$

Note that be definition

$$g^{\prime\prime}(0) =  d^T \nabla^2 f(x)d$$

First order necessary condition 증명때와 같은 방법으로 증명해보면,

$$d^T \nabla^2 f(x)d \ge 0$$

- **Corollary:** Let $$X \subseteq \mathbf{R^n}$$, and let $$f$$ is twice differentiable. if $$x^* \in \text{int}(X)$$ is a local minimum point of $$f$$ over $$X$$, then
	- (1) $$\nabla f^T(x^*)d = 0$$
	- (2) for all $$d$$, $$d^T \nabla^2f(x)d \ge 0 \Leftrightarrow H \ge 0$$

**Sufficient condition for a local minimum**
- **Theorem:** Let $$X \subseteq \mathbf{R^n}$$, and let $$f$$ is twice differentiable. Then $$x^* \in \text{int}(X)$$ is a local minimum point if
	- (1) $$\nabla f^T(x^*)d = 0$$
	- (2) $$H = \nabla^2f(x^*) > 0$$
- In this case, there exist $$\gamma > 0$$ and $$\epsilon > 0$$ such that

$$f(x) \ge f(x^*)+\frac{\gamma}{2}\vert\vert x-x^*\vert\vert^2, \forall x \vert\vert x-x^*\vert\vert \le \epsilon$$

End of Unconstrained Optimization Problems
------------------------------------------

- 지금까지 global 말고 local optimality 다룸
- 하지만 우리가 관심있어하는 문제는 global optimality
- 하지만 some convexity conditions이 만족되면 local analysis는 global analysis로 확장될 수 있다
- 다음 챕터에서는 **convex**를 배워보자 