---
layout: page-sidenav
group: "Introduction to Optimization"
title: "7. Projection method"
---

> Constrained optimization problem 을 위한 Projection method 를 배워보자.

Gradient Projection
-------------------

- Consider : $$$$f:\mathbf{R^n} \to \mathbf{R}$$$$

$$\underset{x\in C}{\text{min}} \ f(x)$$

- **Constrained optimization problem**
- $$C$$ : closed and convex subset of $$\mathbf{R^n}$$
- constrained optimization problem 을 풀기 위해서는 gradient algorithm 을 아래와 같이 수정할 필요가 있음

$$x_{k+1}=P_C(x_k-t\nabla f(x_k))$$

- where $$P_C$$ is the projection of $$x_k-t\nabla f(x_k)$$ on the set $$C$$
- $$P_C$$ 는 다음과 같은 문제로 나타낼수 있다.

$$\underset{z \in C}{\text{min}} \vert\vert z-x\vert\vert^2$$

- 이때 $$x$$ 는 gradient method 로 얻어진 $$x_k-t\nabla f(x_k)$$ 이고 
- $$x \notin C$$ 이면, 가장 가까운 $$z \in C$$ 를 찾고
- $$x \in C$$ 이면, $$z=x$$ 이다. (projection 하지 않아도 괜찮음)
- Projection 을 찾는 문제는 대부분의 경우 그리 간단하지 않다.
- 하지만, $$C$$ 가 linear equality 인 경우에는 쉽게 풀 수 있다.
- 예를 들어,

$$\underset{z}{\text{min}} \vert\vert z-x\vert\vert^2 \\
\text{s.t. } Az=b$$

- Let $$A$$ : $$p \times n$$ matrix with $$\text{rank}(A)=p$$
- The Lagrangian:

$$L(z,\nu) = (z-x)^T(z-x) + \nu^T(Az-b)$$

$$\nabla_z L = 0 \Leftrightarrow 2(z-x)+A^T\nu = 0$$

- 따라서 간단히 연립 방정식으로 나타낼 수 있다.

$$2z+A^T\nu = 2x, \; Az=b$$

$$\begin{pmatrix} 2I & A^T\\ A & 0 \end{pmatrix}\begin{pmatrix} z\\ \nu\end{pmatrix}= \begin{pmatrix} 2x\\ b \end{pmatrix}$$

- 연립 방정식을 풀어보면

$$2z+A^T\nu = 2x \Leftrightarrow A^T\nu = 2(x-z) \\
\Leftrightarrow AA^T\nu = 2A(x-z) \\
\Leftrightarrow \nu = 2(AA^T)^{-1}A(x-z)$$

- Then

$$\nu^* = 2(AA^T)^{-1}A(x-z)$$

$$z^* = \frac{1}{2}(2x-A^T\nu^*)$$

Projection Theorem
------------------

- 잠깐 복습하자면,

$$\underset{x\in C}{\text{min}} \ f(x)$$

- convex function $$f$$ 와 convex and closed set $$C$$ 에 대하여

$$\nabla f(x^*)^T(x-x^*) \ge 0, \forall x \in C$$

- 위와 같은 조건을 만족할때 $$x^*$$ 는 global optimal point 가 된다. (convex 경우 optimality 에 대한 필요충분조건)

**Projection**

- For every $$x \in \mathbf{R^n}$$, there exists a unique vector over $$z \in C$$ that minimize $$\vert\vert z-x\vert\vert^2$$. This is called a projecttion of $$x$$ on $$C$$, $$P_C(x)$$, that is,

$$P_C(x)=\arg \underset{z \in C}{\text{min}} \vert\vert z-x\vert\vert^2$$

- unique vector 가 존재하는 이유는 $$\vert\vert z-x\vert\vert^2$$ 가 strictly convex 하기 때문이다.
	- 간단히 2번 미분해서 보일 수 있다.
- Projection 은 중요한 2가지 특성을 가진다. (나중에 projection method 의 convergence 를 보일 때 이용될 것)

**property 1**

- Given some $$x \in \mathbf{R^n}$$, a vector $$z^* \in C$$ is $$z^*=P_C(x)$$ if and only if

$$(z^*-x)^T(z-z^*) \ge 0, \forall \in C$$

- **Proof** : convex optimization problem 이기 때문에 아래 식을 만족해야 한다.

$$\nabla f(z^*)^T(z-z^*) \ge 0, \forall z \in C$$

- $$\nabla f(z^*)$$ 을 실제 대입 보면

$$(z^*-x)^T(z-z^*) \ge 0, \forall \in C$$

**property 2**

- $$P_C(x)$$ is a **nonexpansive mapping**, i.e.,

$$\vert\vert P_C(x)-P_C(y)\vert\vert \le \vert\vert x-y\vert\vert, \; \forall x,y \in \mathbf{R^n}$$

- 이 특성은 Projection operator 가 continuous function 임을 내포한다.
- **Proof** :
- $$x$$ 에 대해서는 $$z^* \to P_C(x)$$ , $$z \to P_C(y)$$

$$(P_C(y)-P_C(x))^T(x-P_C(x)) \le 0$$

- $$y$$ 에 대해서는 $$z^* \to P_C(y)$$ , $$z \to P_C(x)$$

$$(P_C(x)-P_C(y))^T(y-P_C(y)) \le 0$$

- 위의 두 식을 더하면

$$(P_C(y)-P_C(x))^T(x-P_C(x)-y+P_C(y)) \le 0$$

- C-S inquality 를 이용하면

$$\vert\vert P_C(y)-P_C(x)\vert\vert^T \le (P_C(y)-P_C(x))^T(y-x) \\
\le \vert\vert P_C(y)-P_C(x)\vert\vert \; \vert\vert  y-x\vert\vert$$

Convergence of the Projection Gradient Descent Method
-----------------------------------------------------

- $$f'$$ 가 Lipschitz continuity 하고 $$\nabla^2 f(x) \ge mI$$ (strong convexity) 하다면
- $$P_C(x^*-t\nabla f(x^*)) = x^*$$ where $$x^*$$ solves $$\underset{x\in C}{\text{min}} \ f(x)$$ (optimal point 로 수렴!)
- **Proof** : **nonexpansive mapping** 을 이용하면

$$\vert\vert x_{k+1}-x^*\vert\vert^2 = \vert\vert P_C(x_{k}-t\nabla f(x_k))-P_C(x^*-t\nabla f(x^*)) \vert\vert^2 \\
\le \vert\vert (x_{k}-x^*)-t(\nabla f(x_k)-t\nabla f(x^*)) \vert\vert^2$$

- 나머지 증명 부분은 gradient method 의 convergence 를 보이는 방법과 같다.
- $$\vert 1-mt+M^2t^2 \vert < 1 $$ 일때 geometric convergence to optimal solution.
