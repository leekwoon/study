---
layout: page-sidenav
group: "Introduction to Optimization"
title: "5. Duality"
---

Outline
-------
> 지금까지 Existence, Convex sets and functions, Convex optimization 을 배웠다.
> Constrained Optimization 과 더불어 Optimization 의 꽃이라 할 수 있는 Duality 를 배워보자.

Lagrangian
----------

- 다음과 같은 constrained optimization 문제를 고려해보자.

$$\underset{x}{\text{min}} \; f_0(x)$$

$$\text{s.t. } f_i(x) \le 0, \; \forall i=1,2,...,m$$

$$h_i(x) = 0,  \; i = 1,2,...,p$$

- where the domain is $$D$$
- And its optimal point is $$p^*$$

**Lagrangian**

- Lagrangian $$L$$ : $$\mathbf{R^n} \times \mathbf{R^m} \times \mathbf{R^p} \to D\times \mathbf{R^m} \times \mathbf{R^p}$$

$$L(x,\lambda,\nu) = f_0(x)+\sum_{i=1}^m \lambda_i f_i(x)+\sum_{i=1}^m \nu_i h_i(x) \\
=f_0(x)+\lambda^T f(x)+\nu^T h(x)$$

- 이때, $$\lambda_i$$, $$\nu_i$$ 는 $$f_i(x)$$, $$h_i(x)$$ 에 대한 Lagrangian multiplier 이라 한다.
- duality 를 적용하기 이전의 original problem 을 primal problem 이라 한다. (새로 최적화 할 dual problem 은 뒤에서 정의 할 것)

**Dual function**

- Dual function $$g$$ : $$\mathbf{R^m} \times \mathbf{R^p} \to \mathbf{R}$$

$$g(\lambda,\nu) = \inf_{x\in D}L(x,\lambda,\nu)\\
=\inf_{x\in D} f_0(x)+\sum_{i=1}^m \lambda_i f_i(x)+\sum_{i=1}^m \nu_i h_i(x) \\
=\inf_{x\in D} f_0(x)+\lambda^T f(x)+\nu^T h(x)$$

- dual function 은 $$\lambda,\nu$$ 에 대해 affine 하다.
- 참고로, original problem 의 **convex 여부에 상관 없이** dual function $$g$$는 **concave** 하다.
- dual function 은 concave 한 pointwise infimum 함수이다. (pointwise supremum 이 convex 하므로)
- 뒤에서 배울 duality gap 과 연결해서 생각해 보자.
- 주어진 모든 constraint 를 만족하는 $$\bar{x}$$ 와 original problem 의 optimal value $$p^*$$ 를 가정해보자.
- **$$\lambda \ge 0$$ 을 만족한다면,**
- $$f(\bar{x}) \le 0$$ , $$h(\bar{x}) =0$$ 이기 때문에,

$$\lambda^T f(\bar{x})+\nu^T h(\bar{x}) \le 0$$

- 위의 식을 이용하면

$$L(\bar{x},\lambda,\nu) = f_0(\bar{x})+\lambda^T f(\bar{x})+\nu^T h(\bar{x}) \le f(\bar{x})$$

- 모든 $$\bar{x}$$ 에 대해 위의 식이 만족하기 때문에

$$g(\lambda,\nu) \le p^*$$

- original problem 의 optimal value $$p^*$$ 와 dual function 값 사이에 **gap 이 존재!**
- (We should have $$g(\lambda,\nu) > -\infty$$)
- 지금까지의 내용을 정리하자면,
	- Lagrangian and its dual function are related to a unconstrained optimization problem, since the constraints are embedded into the objective function with Lagrangian multiplier.
	- 제약조건이 있는 문제의 경우 대부분의 경우 optimize 하기가 어려워, duality 를 이용하여 제약조건이 없는 문제로 바꾼다.
	- 하지만 optimal solution 과 gap 이 존재. (related to duality gap)
	- duality gap 이 없는 조건은 뒤에서 다룰 것이다.

Dual Function: Examples
-----------------------

**Quadratic Problem**

$$\underset{x}{\text{min}} \; x^{T}x \\
\text{s.t. } Ax=b$$

- The Lagrangian:

$$L(x,\nu)=x^{T}x+\nu^{T}(Ax-b)$$

- Lagrangian 을 $$x$$ 에 대해 미분

$$\nabla_x L(x,\nu)=2x+A^{T}\nu=0 \Rightarrow x=\frac{-1}{2}A^{T}\nu$$

- dual function 을 구하기 위해 Lagrangian 을 최소로 만드는 $$x$$ 대입
	- **이때 $$x$$ 는 제약 조건을 고려하지 않고 구한다.**

$$g(\nu)=\frac{-1}{4}\nu^{T}AA^{T}\nu - b^{T}\nu$$

- 참고로, 위의 식을 $$\nu$$ 에 관한 식으로 나타내기 위해 $$\nu^{T}b$$ 대신에 $$b^{T}\nu$$ 로 표현
- Dual function $$g$$ 는 $$\nu$$ 에 대해서 concave 하다.
- 이렇게 얻어진 Dual function 은 optimal solution 의 lower bound 이다.

$$p^{*} \ge \frac{-1}{4}\nu^{T}AA^{T}\nu - b^{T}\nu$$

**Linear Programming**

$$\underset{x}{\text{min}} \; c^{T}x \\
\text{s.t. } Ax = b, \; x \ge 0$$

- 참고로 $$x \ge 0$$ 은 $$-x \le 0$$ 으로 생각하여 $$\lambda$$ 대신에 $$-\lambda$$ 를 쓰면 된다.
- The Lagrangian:

$$L(x,\lambda,\nu) = c^{T}x-\lambda^{T}x+\nu^{T}(Ax-b)\\
=-b^{T}v+(c-\lambda+A^{T}v)^{T}x$$

- $$L$$ 은 $$x$$ 에 관해 affine 하다.
	- 따라서 $$x$$ 관해 미분하지 않고 바로 dual function 을 구할 수 있다.
- The dual function:

$$g(\lambda,\nu)=\underset{x}{\text{inf}} \; L(x,\lambda,\nu)=\begin{cases}
    -b^{T}\nu, & A^{T}v-\lambda+c = 0  \\
    -\infty,              & \text{otherwise}
\end{cases}$$

- $$c-\lambda+A^{T}v = 0$$ 일때 optimal solution 의 lower bound 가 존재한다.

$$ p^{*} \ge -b^{T}\nu$$

- 중요한 사실은, Linear Programming 경우
	- $$ p^{*} = -b^{T}\nu$$ 을 만족하는 $$v^{*}$$ 이 항상 존재한다.
	- duality gap 이 없다.

Lagrangian Dual and Conjugate Functions

**Conjugate function**

- Conjugate function 이란,

$$f^{*}(y)=\underset{x}{\text{sup}} (y^{T}x-f(x))\\
=-\underset{x}{\text{inf}} -(y^{T}x-f(x))$$

- Conjugate function $$f^{*}(y)$$ 는 convex 하다.
- dual function 을 conjugate function을 이용하여 표현해보자.
- 예를 들어,

$$\underset{x}{\text{min}} \; f(x)\\
\text{s.t. } Ax \le b, \; Cx = b$$

- The dual function:

$$g(\lambda,\nu)=\underset{x}{\text{inf}} \ f(x)+\lambda^{T}(Ax-b)+\nu^{T}(Cx-d) \\
= -b^{T}\lambda-d^{T}\nu+\underset{x}{\text{inf}} \ f(x)+(A^{T}\lambda+C^{T}\nu) \\
= -b^{T}\lambda-d^{T}\nu-f^{*}(-A^{T}\lambda-C^{T}\nu)$$

Dual Problem
------------

- 앞서서 duality 를 적용하기 이전의 original problem 을 primal problem 이라 정의하였다.
- dual problem 을 정의해보자.
- 많은 경우에 optimal solution 과 dual function (lower bound) 간의 gap 이 있었다.
- 따라서 problem 을 다음과 같이 정의한다.
- Lagrangian dual problam:

$$\underset{\lambda,\nu}{\text{max}} \ g(\lambda,\nu) \\
\text{s.t. } \lambda \ge 0$$

- dual problem 에 대해 설명하자면,
	- optimal solution $$p^{*}$$ 의 best lower bound 를 찾는 것으로 볼 수 있다.
	- 이때 dual optimal value 를 $$d^{*}$$ 라 하면,
		- $$d^{*} \le p^{*}$$

- 예를 들어 Linear Programming 의 dual problem 을 살펴보자.

$$\underset{x}{\text{min}} \ c^{T}x \\
\text{s.t. } Ax= b, \ x\ge 0$$

- The dual function:

$$g(\lambda,\nu)=\underset{x}{\text{inf}} \; L(x,\lambda,\nu)=\begin{cases}
    -b^{T}\nu, & A^{T}v-\lambda+c = 0  \\
    -\infty,              & \text{otherwise}
\end{cases}$$

- The dual function:

$$\underset{\lambda,\nu}{\text{max}} \ -b^{T}\nu \\
\text{s.t. } A^{T}v-\lambda+c = 0, \; \lambda \ge 0$$

- 다르게 표현하면,

$$\underset{\lambda,\nu}{\text{max}} \ -b^{T}\nu \\
\text{s.t. } A^{T}v+c \ge 0$$

Weak and Strong Duality
-----------------------

- weak duality : $$d^{*} \le p^{*}$$
- strong duality : $$d^{*} = p^{*}$$
- duality gap : $$d^{*} - p^{*}$$
- **Main isuues**
	- primal 이랑 dual problem 의 general relation 이 있는지?
	- 어떤 조건에서 zero duality gap?
	- 어떤 조건에서 primal 이랑 dual problem 이 optimal soultions 을 가지는지?
	- dual optimal solutions 이 primal problem 에게 어떤 유용한 정보를 제공하는지?

Geometric interpretation of Duality
-----------------------------------

- 이제부터 duality 를 geometric 관점에서 바라보자.

$$\underset{x}{\text{min}}f_{0}(x) \\
\text{s.t. } f_{1}(x) \le 0$$

- 예를 들어, 위와 같은 optimization problem 을 고려해 보자.

$$p^{*} = \underset{x}{\text{inf}}\{f_{0}(x) \; \vert \; (f_1(x),f_0(x))\in \mathbf{R} \times \mathbf{R}, f_1(x) \le 0\} \\
=\underset{x}{\text{inf}}\{(\lambda,1)^{T}(f_{1}(x),f_{0}(x)) \; \vert \; (f_1(x),f_0(x))\in \mathbf{R} \times \mathbf{R}, f_1(x) \le 0\} \\
=\underset{x}{\text{inf}}\{(\lambda,1)^{T}(f_{1}(x),f_{0}(x)) \; \vert \; (f_1(x),f_0(x))\in \mathbf{R} \times \mathbf{R}\} \\
=g(\lambda)$$

- geometric 관점으로 생각해보면,
- dual function 은 supporting hyperplane 으로 생각될 수 있다.
	- $$y$$ 축이 $$f_0(x)$$, $$x$$ 축이 $$f_1(x)$$ 일때
		- 기울기 : $$-\lambda$$
		- $$y$$ 절편 : $$g(\lambda)$$

![]({{site.baseurl}}/images/opt_study/intro_opt/5.1.png){:class="center-block"}

![]({{site.baseurl}}/images/opt_study/intro_opt/5.2.png){:class="center-block"}

![]({{site.baseurl}}/images/opt_study/intro_opt/5.3.png){:class="center-block"}

- **convex problem 에서도 duality gap 이 존재할 수 있다.**
- supproting hyperplane 이 $$y$$ 절편에서 $$(0,p^{*})$$ 을 지날때 duality gap 이 없는 strong duality 가 된다.

![]({{site.baseurl}}/images/opt_study/intro_opt/5.4.png){:class="center-block"}

![]({{site.baseurl}}/images/opt_study/intro_opt/5.5.png){:class="center-block"}

Slater's Constraint Qualification
---------------------------------

- zero duality gap 을 위한 strong duality condition 들을 배워보자.

**Slater's condition**

- **convex optimization** problem 에 대해서,
	- (Slater's condition 은 convex 조건 필요)

$$\underset{x}{\text{min}}f(x) \\
\text{s.t. } f_i(x) \le 0, \; \forall i=1,2,...,m \\
Ax=b$$

- 만약 problem 이 strictly feasible 하면.. e.g.

$$\exists x \in \text{int}(D): f_i(x) < 0, \; \forall i=1,2,...,m , \; Ax=b$$

- strong duality 를 만족한다. (증명은 hyperplane theorem 이용)

![]({{site.baseurl}}/images/opt_study/intro_opt/5.6.png){:class="center-block"}

Saddle-Point Interpretation
---------------------------

- (Slater's condition 과 다르게) **convexity 만족할 필요 없다!**

$$\underset{x}{\text{inf}} \; f_0(x) \\
\text{s.t. } f_1(x) \le 0$$

- 다르게 나타내면

$$\underset{x}{\text{inf}} \; \underset{\lambda \ge 0}{\text{sup}} \; \left( f_0(x)+\lambda^{T}f_1(x) \right)$$

- Let $$L(x,\lambda) = f_0(x)+\lambda^{T}f_1(x)$$.
- $$L(x,\lambda)$$ 은 $$\lambda$$ 에 대해 affine 하기 때문에,

$$\underset{\lambda \ge 0}{\text{sup}} \; \left( f_0(x)+\lambda^{T}f_1(x) \right) = \begin{cases}
    f_0(x), & f_1(x) \le 0 \\
    \infty,              & \text{otherwise}
\end{cases}$$

- duality gap 이용해 식을 표현해보면,

$$\underset{x}{\text{inf}} \; \underset{\lambda \ge 0}{\text{sup}} \; L(x,\lambda) \ge d^{*} =\underset{\lambda \ge 0}{\text{sup}} \; \underset{x}{\text{inf}} \; L(x,\lambda)$$

- 따라서 $$\underset{x}{\text{inf}} \; \underset{\lambda \ge 0}{\text{sup}} \; L(x,\lambda) =\underset{\lambda \ge 0}{\text{sup}} \; \underset{x}{\text{inf}} \; L(x,\lambda)$$ 일때 strong duality 를 만족한다.

**Saddle-point**

- For the Lagrangian  $$L(x,\lambda), (\bar{x},\bar{\lambda})$$ is the **saddle-point** of $$L$$ if

$$L(\bar{x},\lambda)\le L(\bar{x},\bar{\lambda})\le L(x,\bar{\lambda}), \; \forall x, \lambda \ge 0$$

- 참고로 , $$L(\bar{x},\lambda)$$ 이 optimal soultion 의 lower bound.
- $$(\bar{x},\bar{\lambda})$$ 에서 saddle point 임을 보이면 strong duality 를 만족한다.

KKT Condition: Complementary Slackness
--------------------------------------

**Complementary Slackness**

- Duality gap 이 없다고 가정해보자.

$$g(\lambda^*,\nu^*)=\underset{x}{\text{inf}} \; \left( f_0(x)+\lambda^{T}f(x)+\nu^{T}h(x) \right) \\
=f_0(x^*)+\lambda^{T}f(x^*)+\nu^{T}h(x^*)) \\
\le f_0(x^*)$$

- Duality gap 이 없기 위해서는
	- 모든 $$i$$ 에 대해서 $$f_i(x^*) \le 0 $$ 이고 $$\lambda_i \ge 0 $$ 이기 때문에,

$$\lambda_i^* f_i(x^*) = 0, \; \forall i = 1,2,...,m$$

- 이 조건을 **complementary slackness** 라 한다.
- 아래와 같이 나타낼 수도 있다.

$$\lambda_i^* > 0 \Rightarrow f_i(x^*)=0$$

$$f_i(x^*) < 0 \Rightarrow \lambda_i = 0$$

KKT Condition
-------------

- $$x^*$$ 와 $$(\lambda^*,\nu^*)$$ 는 각각 primal, dual problem 의 optimal point 이므로

$$\nabla L_x(x^*,\lambda^*,\nu^*)=\nabla f_0(x^*)+\sum_{i=1}^m \lambda_i^*\nabla f_i(x^*)+\sum_{i=1}^p \nu_i^*\nabla h_i(x^*) = 0$$

- 참고로 위의 조건은 strong duality 의 first order neceesary condition
	- strong duality 가 아닌 걸 보일 때 사용가능.
- Hence, we must have
    - Feasibility : $$f_i(x^*) \le 0, \; \forall i$$
    - Feasibility : $$h_i(x^*) = 0, \; \forall i$$
    - Feasibility : $$\lambda_i^* \ge 0, \; \forall i$$
    - Complementary slackness : $$\lambda_i^* f_i(x^*) = 0, \; \forall i$$
- (feasibility 와 complementary slackness 조건을 만족시키는) 이 조건을 **KKT** (KarushKuhn Tucker) condition 이라 한다.

**Necessary condition for the nonlinear programming**

- **Theorem** : $$x^*$$ 가 constrained optimization problem 의 local optimal point 일때, KKT condition 을 만족하는 **unique** 한 Lagrangian multipliers $$(\lambda^*,\nu^*)$$ 가 존재한다.
	- **Constrained optimization problem** 의 1st order necessary condition (일차 필요조건).
	- 달리 말하면 KKT 조건을 만족하는 $$(\lambda^*,\nu^*)$$ 가 존재하지 않으면 local optimal point 가 아님.

**Sufficient condition for the convex optimization problem**

- **Convex** Optimization 에 대해서..
- 만약 $$x$$ 와 $$(\lambda^*,\nu^*)$$ 가 KKT condition 을 만족한다면, $$x$$ 는 primal problem 의 global minimizer 가 되고 $$(\lambda^*,\nu^*)$$ 은 dual optimal 이 된다.
- 또한 strong duality (zero duality gap) 을 만족하게 된다.
- 중요한 것은..
	- **Convexity** 와 **Slater's condition** 이 만족되면 KKT condition 은 optimality 에 대한 필요충분 조건이 되고
	- unconstrained optimization problem 의 sufficient condition 처럼
		- Convexity 는 local optimal point 를 global optimal point 로 만들어준다.
- KKT condition 이 optimality 의 Sufficient condition (충분조건) 임을 증명해보자.
- Convex optimization problem 에 대해서,

$$L(x^*,\lambda^*,\nu^*) \le L(x,\lambda^*,\nu^*) = f_0(x)+\lambda^Tf(x)+\nu^Th(x)$$

- For any feasible $$x$$ and $$\lambda^*$$, due to the complementary slackness, we must have

$$f(x^*)=L(x^*,\lambda^*,\nu^*) \le L(x,\lambda^*,\nu^*) \le f_0(x)$$

- Hence, $$f(x^*) \le f(x)$$ for all feasible $$x$$ (증명 끝)

**KKT condition 장단점**

- 장점 : Unconstrained optimization problem 을 풀때, 연립 방정식 꼴로 풀 수 있다.
- 단점 : 수많은 condition ..  그리고 Complementary slackness condition 경우의 수가 참 많다..

KKT 예제
-------

- transmitter 과 receiver 사이에 $$n$$ 개의 channel 이 있다.
- 다음과 같은 opimization problem 을 고려해보자.

$$\underset{p_i}{\text{max}} \; \sum_{i=1}^n \frac{1}{2}\log(1+\frac{P_i}{\sigma_i^2}) \\
\text{s.t. } P_i \ge 0, \; \sum_{i}P_i \le P, \; \forall i$$

- 이때 $$P_i$$, $$\sigma_i$$ 는 각각 $$i$$ 번째 channel 의 power, noise 이다.
- 즉 각 channel 에 power 을 어떻게 분배해야 가장 큰 효율을 발휘할수 있을까? 에 관한 문제이다.
- 위 식은 concave 함수를 maximize 하는 문제이다. (안의 식이 log에 의해 concave)
- Slater's condition 만족한다.
	- 모든 $$i$$ 에 대해 $$P_i > 0, \; \sum_{i}P_i < P$$ 을 만족하는 $$P_i$$ 존재
- KKT condition 을 적용하기 위해 우선 convex optimization problem 으로 바꾸자

$$\underset{p_i}{\text{min}} \; -\sum_{i=1}^n \frac{1}{2}\log(1+\frac{P_i}{\sigma_i^2}) \\
\text{s.t. } P_i \ge 0, \; \sum_{i}P_i \le P, \; \forall i$$

- The Lagrangian :

$$L(P,\lambda_0,\lambda)=-\sum_{i=1}^n\frac{1}{2}\log(1+\frac{P_i}{\sigma_i^2})+\lambda_0(\sum_{i=1}^n P_i-P)-\sum_{i=1}^n \lambda_i P_i$$

$$\nabla L_{P_i}(P,\lambda_0,\lambda)=-\frac{1}{1+\frac{P_i}{\sigma_i^2}}\frac{1}{\sigma_i^2}+\lambda_0-\lambda_i$$

$$\Leftrightarrow P_i=\frac{1}{\lambda_0-\lambda_i}-\sigma_i^2$$

- KKT condtion 이 만족 되기 전에는 first order necessary condition
- convexity 와 slater's condition 이 만족 되어진 상태인 지금,
- KKT condition 이 만족되면 optimality 의 necessary and sufficient condition 이 된다.
	- Feasibility : $$\sum_{i=1}^n P_i \le P, \; \forall i$$
	- Feasibility : $$P_i \ge 0, \; \forall i$$
	- Feasibility : $$\lambda_0 \ge 0, \; \forall i$$
	- Feasibility : $$\lambda_i \ge 0, \; \forall i$$
	- Complementary slackness : $$\lambda_0(\sum_{i=1}^n P_i-P)=0$$
	- Complementary slackness : $$\lambda_i P_i=0$$
- 앞서 말했듯이, 모든 $$i$$ 에 대해 slater's condition 을 만족하는 $$P_i$$ 가 존재한다.
	- 이때 $$P_i > 0$$ 을 만족하는 $$P_i$$ 도 존재하고,
	- 이 경우에는 $$\lambda_i=0$$
- $$\log$$ 의 경우 non-decreasing 함수 이므로, $$\sum_{i=1}^n P_i=P$$ (equality) 를 만족할 때 가장 효율적.. 따라서,

$$P_i=\frac{1}{\lambda_0}-\sigma_i^2$$

$$\sum_{i=1}^n P_i = \sum_{i=1}^n \frac{1}{\lambda_0}-\sigma_i^2 = P$$

- 위의 식을 만족하는 $$\lambda_0$$ 이 있을까?
	- $$\lambda_0 \to 0$$ 일때 $$\frac{1}{\lambda_0}-\sigma_i^2 \to \infty$$
	- $$\lambda_0 \to \infty$$ 일때 $$\frac{1}{\lambda_0}-\sigma_i^2 \to 0$$
	- 즉, $$0$$ 과 $$\infty$$ 사이에 만족하는 $$\lambda_0$$ 존재
- $$\lambda_0$$ 이 존재 할때를 생각해보면 결국 noise ($$\sigma$$) 가 적은 channel 을 더사용 하는게 효율적. (water filling effect)