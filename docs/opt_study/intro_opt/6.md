---
layout: page-sidenav
group: "Introduction to Optimization"
title: "6. Gradient Descent"
---

> 이번에는 Unconstrained opimization 을 위한 Algorithm 인 Gradient Descent mothod 를 배워보자.
> 참고로 optimization problem 에 따라 다른 algorithm 이 존재한다.

- Unconstrained opimization
 	- Gradient method
 	- Newton method
- Constrained optimization
 	- Projection method
 	- Interior point method

Unconstrained Minimization Problem
----------------------------------

- Consider : $$f:\mathbf{R^n} \to \mathbf{R}$$

$$\underset{x}{\text{min}} \ f(x)$$

- $$f$$ 가 twice differentiable (두번 미분 가능)하고 convex 하면
- $$\nabla f(x^*) = 0$$ 은 optimality 의 필요충분조건이다.
- (조건에 Hessian 을 뺀 이유는 convex 이기 위한 필요충분조건 이므로, 이미 위에서 convex 언급했으니..)
- 하지만 closed form 이 나오지 않아 analytical 하게 풀 수 없는 경우 존재.
- 따라서 Numerical Algorithm 필요.

$$\underset{x}{\text{min}} \ f(x)$$

- 예를 들어, $$x_0$$ 을 시작점으로 optimal point $$x^*$$ 로 이동하고 싶다고 해보자.
- 그러기 위해서는 $$x_{k+1}=g(x_k)$$ 그리고 $$\lim_{k\to \infty}g(x_k)=x^*$$ 을 만족하는 function $$g$$ 를 design 해야한다.
- 한가지 방법으로 Gradient method 는 $$f'$$ 를 $$g$$ 로 이용한다.

$$x_{k+1}=x_{k}-t_{k}\nabla f(x_k)$$

- 이때, $$t_k$$ 가 너무 크면 optimal point 로 수렴하지 않을 수 있고,
- $$t_k$$ 가 너무 작으면 수렴이 너무 느릴 수 있다.
- t_k 를 선택하는 대표적인 방법으로는
	1. Constant
	2. Exact line search
	3. Armijo's rule
- 우선 $$t_k = t$$ 로 constant (Fixed Step Size) 인 경우를 고려해보자.

Gradient Descent with a Fixed Step Size
---------------------------------------

**Lipschitz continuous**

$$\vert\vert \nabla f(x)- \nabla f(y) \vert\vert \le M\vert\vert x-y\vert\vert$$

- 위의 조건을 만족할 때 $$f(x)$$ 의 도함수 $$f'(x)$$ 가 Lipschitz continuous 하다고 말한다.
- 다르게 표현하면 $$f'(x)$$ 가 continuous and differentiable 하고 $$f''(x)$$ 는 bounded 되어있으면 $$f'(x)$$ 는 Lipschitz
- **Theorem** :
	- 도함수가 Lipschitz continuous 하고
		- $$\vert\vert \nabla f(x)- \nabla f(y) \vert\vert \le M\vert\vert x-y\vert\vert$$
	- $$\underset{x}{\text{inf}} \ f(x) = f^* > -\infty$$ 할때
	- gradient algorithm with $$t<\frac{2}{M}$$ has the following property

$$\underset{k\to \infty}{\text{lim}} \ \nabla f(x_k)=0$$

$$\underset{k\to \infty}{\text{lim}}  \vert\vert \nabla f(x_k)\vert\vert=0$$

- 이때 $$f$$는 convex 일 필요는 없다.
	- 하지만 optimality 를 보장하지는 않는다.

![]({{site.baseurl}}/images/opt_study/intro_opt/6.1.png){:class="center-block"}

- 위의 그림은 optimality 를 보장하지 않는 예시를 보여준다.
- $$f''(x)$$ 는 bounded 되어있고 $$f'(x)$$ 는 Lipschitz 하지만 ..
- 위의 Theorem 을 증명해보자.
- **Proof** : $$f'$$ 가 Lipschitz continous 할 때, Let

$$g(t) = f(x+t(y-x))$$

- Then

$$g(1) = g(0) + \int_0^1 g'(t)dt$$

- 비슷한 방법으로

$$f(y) = f(x) + \int_x^y f'(t)dt \\
= f(x) + \int_0^1 f'(x+t(y-x))^{T}(y-x)dt \\
= f(x) + \nabla f(x)^{T}(y-x) + \int_0^1 \left( f'(x+t(y-x))-\nabla f(x) \right)^{T}(y-x)dt$$

- by cauchy schwarz (C-S) inequality
	- $$y-x \le \vert\vert y-x\vert\vert$$
- by cauchy schwarz (C-S) inequality and Lipschitz continous of $$f'$$
    - $$f'(x+t(y-x))-\nabla f(x) \le \vert\vert f'(x+t(y-x))-\nabla f(x)\vert\vert \le Mt\vert\vert y-x\vert\vert$$

$$\Rightarrow\\
\le f(x) + \nabla f(x)^{T}(y-x) + M\int_0^1 t\vert\vert y-x\vert\vert^2 dt \\
=f(x) + \nabla f(x)^{T}(y-x) + \frac{M}{2}\vert\vert y-x\vert\vert^2$$

- This implies

$$f(x_{k+1}) \le f(x_k) + \nabla f(x_k)^{T}(x_{k+1}-x_{k}) + \frac{M}{2}\vert\vert x_{k+1}-x_k\vert\vert^2$$

- by the gradient method
	- $$x_{k+1} = x_k - t\nabla f(x_k)$$

$$\Rightarrow\\
= f(x_k) + \nabla f(x_k)^{T}(-t\nabla f(x_k)) + \frac{M}{2}t^2\vert\vert \nabla f(x_k)\vert\vert^2 \\
= f(x_k)-t(1-\frac{M}{2}t)\vert\vert \nabla f(x_k)\vert\vert^2 \\
\Leftrightarrow t(1-\frac{M}{2}t)\vert\vert \nabla f(x_k)\vert\vert^2 \le f(x_k)-f(x_{k+1}) \\
\Leftrightarrow t(1-\frac{M}{2}t)\vert\vert \nabla f(x_k)\vert\vert^2 \le f(x_k)-f(x_{k+1}) \le f(x_k) - f^*$$

- $$f^*$$ 은 finite 하고 또한 $$t$$ 를 $$\frac{2}{M}$$ 보다 작게 잡으면, $$(1-\frac{M}{2}t) > 0 $$ 이기 때문에

$$\underset{n\to \infty}{\text{lim}} \sup \sum_{k=1}^n \vert\vert \nabla f(x_k)\vert\vert^2 < \infty$$

- Therefore

$$\underset{k\to \infty}{\text{lim}}\vert\vert \nabla f(x_k)\vert\vert^2 = 0$$

- **Theorem** :
	- 이전 조건들을 충족하면서..
        - 도함수가 Lipschitz continuous 하고
            - $$\vert\vert \nabla f(x)- \nabla f(y) \vert\vert \le M\vert\vert x-y\vert\vert$$
        - $$\underset{x}{\text{inf}} \ f(x) = f^* > -\infty$$ 할때
	- **$$f$$ 가 convex 하다면 ! ** 
	- $$t < \frac{2}{M}$$ 인 경우에 gradient descent algorithms 은 optimal point 로 수렴한다.
		- $$x^* \in X_{\text{opt}} = \{ \bar{x} : f(\bar{x})= \underset{x}{\text{inf}} f(x) \}$$
- 따라서 특정한 condition 이 만족 될때, Convexity 는 optimal point 로의 수렴을 보장해준다.
- 그렇다면 얼마나 **빨리** 수렴할까?
- 얼마나 수렴하는이 알아보기 위해 Strong convexity 라는 condition 을 추가로 이용해보자.
	- $$\nabla^2 f(x) \ge mI$$
- By definition, strong convexity implies strict convexity.

$$f(y)\ge f(x) + \nabla f(x)^T(y-x)+\frac{1}{2}m\vert\vert y-x\vert\vert^2$$

- By the Lipschitz condition

$$f(y)\le f(x) + \nabla f(x)^T(y-x)+\frac{1}{2}M\vert\vert y-x\vert\vert^2$$

- Implication of the storng convexity:

$$f(y)\ge f(x) + \nabla f(x)^T(y-x)+\frac{1}{2}m\vert\vert y-x\vert\vert^2 \\
\ge f(x) + \nabla f(x)^T(\bar{y}-x)+\frac{1}{2}m\vert\vert \bar{y}-x\vert\vert^2, \text{where } \; \bar{y}=x-(1/m)\nabla f(x) \\=f(x)-\frac{1}{m}\vert\vert\nabla f(x)\vert\vert^2$$

- Hence,

$$f^* \ge f(x)-\frac{1}{2m}\vert\vert\nabla f(x)\vert\vert^2$$

- This implies

$$\vert\vert\nabla f(x)\vert\vert \le (2m\epsilon)^{1/2} \Rightarrow f(x) \le f^*+\epsilon$$

- **따라서 $$\vert\vert\nabla f(x)\vert\vert $$ bounded 됨**
- Also, Implication of the storng convexity:

$$f^* \ge f(x) + \nabla f(x)^T(x^*-x)+\frac{1}{2}m\vert\vert x^*-x\vert\vert^2 \\
=f(x) - \vert\vert\nabla f(x)\vert\vert \vert\vert(x^*-x)\vert\vert+\frac{1}{2}m\vert\vert x^*-x\vert\vert^2 $$

- And we have

$$\vert\vert x^*-x\vert\vert \le \frac{2}{m}\vert\vert\nabla f(x)\vert\vert$$

- **또한 $$\vert\vert x^*-x\vert\vert$$ 도 bounded 됨**
- Note that the strong convexity implies the strict convexity
- Hence, $$x^*$$ is unique.

$$\vert\vert x_{k+1}-x^*\vert\vert^2 = \vert\vert x_{k}-t\nabla f(x_k)-x^*+t\nabla f(x^*) \vert\vert^2 \\
=\vert\vert (x_{k}-x^*)-t(\nabla f(x_k)-t\nabla f(x^*)) \vert\vert^2 \\
=\vert\vert x_{k}-x^*\vert\vert^2+t^2\vert\vert\nabla f(x_k)-t\nabla f(x^*) \vert\vert^2 - 2t(x_k-x^*)^T(\nabla f(x_k)-t\nabla f(x^*)) \\
\le \vert\vert x_{k}-x^*\vert\vert^2+t^2M^2\vert\vert x_k-x^* \vert\vert^2 + 2t(x^*-x_k)^T\nabla f(x_k)+2t(x_k-x^*)^Tt\nabla f(x^*) \\
= \le \vert\vert x_{k}-x^*\vert\vert^2+t^2M^2\vert\vert x_k-x^* \vert\vert^2 -2tf(x^*)+ 2t(x^*-x_k)^T\nabla f(x_k)+2tf(x^*)+2t(x_k-x^*)^Tt\nabla f(x^*)$$

- By the first order optimality condition, we have

$$\vert\vert x_{k+1}-x^*\vert\vert^2 \le \vert\vert x_{k}-x^*\vert\vert^2+t^2M^2\vert\vert x_k-x^* \vert\vert^2 -2tf(x^*)+ 2t(x^*-x_k)^T\nabla f(x_k)+2tf(x^*)+2tf(x_k)$$

- $$\vert\vert x^*-x\vert\vert \le \frac{2}{m}\vert\vert\nabla f(x)\vert\vert$$ 을 이용하여

$$\vert\vert x_{k+1}-x^*\vert\vert^2 \le \vert\vert x_{k}-x^*\vert\vert^2+t^2M^2\vert\vert x_k-x^* \vert\vert^2 -\frac{1}{2}m\vert\vert x_k-x^*\vert\vert^2 2t \\
=(1-mt+M^2t^2)\vert\vert x_k-x^*\vert\vert^2$$

- This implies

$$\vert\vert x_k-x^*\vert\vert^2 \le (1-mt+M^2t^2)^k\vert\vert x_0-x^*\vert\vert^2$$

- **따라서, $$\vert 1-mt+M^2t^2 \vert < 1 $$ 일때 converges exponentially**

Gradient Descent with a Variable Step Size
------------------------------------------

**Exact line search**

- 비현실적 (impractical)
- 배보다 배꼽이 더 클수도

$$t=\arg \underset{s \ge 0}{\text{min}} f(x+s\Delta x)$$

**Backtracking search (Armijo's rule)**

- quite practical

![]({{site.baseurl}}/images/opt_study/intro_opt/6.2.png){:class="center-block"}

![]({{site.baseurl}}/images/opt_study/intro_opt/6.3.png){:class="center-block"}

Newton's Method
---------------

- $$f$$ 가 twice differentiable (두번 미분 가능)하고 convex
- gradient method 보다 좀더 빠른 algorith 은?

Newton's Method
---------------

$$x_k+1 = x_k-t_k \nabla^2 f(x_k)^{-1} \nabla f(x_k)$$

- Newton's method
- Hessian 역행렬 이용

![]({{site.baseurl}}/images/opt_study/intro_opt/6.4.png){:class="center-block"}

- $$f''$$ 가 Lipschitz continuity 를 만족하고
	- $$\vert\vert \nabla^2 f(x) - \nabla^2 f(y)\vert\vert \le L\vert\vert x-y\vert\vert$$
- $$f$$ 가 strong convexity 을 만족할때
	- $$\nabla^2 f(x) \ge mI$$
- Then we have for $$I\ge k$$

$$f(x_I)-p^* \le \frac{2m^3}{L^2}(\frac{1}{2})^{2^{I-k+1}}$$

- much faster than the geometric convergence (gradient method)

