---
layout: page-sidenav
group: "Scrapy"
title: "Web crawling"
---

> requests와 Beatifule Soup을 이용해 웹 크롤링이 가능한데 Scrapy가 왜 필요할까?
> 구글의 경우만 보아도 규칙적으로 짧은 시간안에 자동으로 많은 데이터를 위와 같은 방식은 쉽게 감지되고, 어느 순간부터 사이트 접속을 할 수 없게 만든다.
> 따라서 이런 문제를 바탕으로 웹 크롤링을 전문적으로 다루는 라이브러리가 Scrapy이다.

Scrapy
------

- web scraper framework
- 다양한 selector 지원
- 파이프라인
	- 데이터 후처리(filtering)
- 로깅
	- 데이터가 잘 처리되고 있는지
- 이메일
	- 데이터가 들어왔을때 메일 기능

#####Scrapy 프로젝트 생성

`scrapy startproject tutorial`

- items.py : 저장할 데이터를 클래스 형태로 관리
- pipeline.py : scrapy를 통해서 데이터를 가져오고 후처리를 할경우 여기에 구현
- setting.py : scrapy에 대한 설정들 e.g., 파이프 라인 순서, bot이름 등등
- spiders폴더 안에 실질적으로 웹 크롱링 할 내용들을 구현하면 된다.
	- 참고로 name변수 안에 유니크한 이름을 지정해야함

#####Scrapy 프로젝트 실행

`scrapy crwal 지정한-유니크한-이름`

크롬에서 developer tool(컨트롤+시프트+i)를 이용해 html을 확인할 수 있다. 그리고 해당 html에서 원하는 부분의 xpath를 알고 싶으면 마우스 우클릭을 통해 확인가능

## Continue..