---
layout: page-sidenav
group: "Part 1. Classic Statistical Inference"
title: 2. Frequentist Inference (2018-03-26)
---

# Frequentist Inference

At the beginning in 1900, there was the calculator age before the computer age. At that time, the theory relied almost entirely on frequentist inferential ideas.

Statistical inference usually begins with the assumption that some probability
model has produced the observed data $x=(x_1, ..., x_n)$.

* $F$: underlying distribution
* $X$: theoretical sample, $n$ independent draws from $F$, $F \to X$
* $x$: realization of $X$ which has been observed.
* $\theta=E_{F}\{X\}$: the property of $F$ we are interested in
* $\hat{\Theta}=t(X)$: the estimator of $\theta$
* $\hat{\theta}=t(x)$: a realization of $\hat{\Theta}$
* $t(x)$: known algorithm for estimator

Suppose the desired property is the *expectation* of a single random draw
$X$ from $F$ , denoted

$$\theta = E_{F}\{X\}$$

We have chosen $t(X)$, we hope, to make $\hat{\Theta}$ a good estimator of $\theta$, the desired property of $F$.

$$\hat{\Theta}=t(X)$$

Let $\mu$ be the expectation of $$\hat{\Theta}=t(X)$$ under $F \to X$. Then

* bias = $\mu - \theta$
* var = $E_{F}\{\(\hat{\Theta} - \mu)^2\}$

## Frequentism in Practice

Frequentism has an obvious defect: it requires aclculating the properties of estimators $\hat{\Theta}=t(X)$ obtained from the true distribution $F$, even though $F$ is unknown. Practical frequentism uses a collection of more or less ingenious devices to circumvent the defect.

**1. The plug-in principle.** The related example is the *standard error* of *mean* $\bar{X}=\sum X_i / n$: $se(\bar{X}) = [var_{F}(X)/n]^{1/2}$. We can estimate $var_{F}(X)$ without bias by sample variance $\hat{var}_F=\sum(x_i-\bar{x})^2 / (n-1)$. In other words, the frequentist accuracy estimate for $\bar{x}$ is itself estimated from the observed data.

**2. Taylor-series approximations.** For example, statistics $\hat{\theta}=t(x)$ can be more complicated than *mean* $\bar{x}$. One way is to use local linear approximations, **delta method**. 

The delta method uses a first-order Taylor series to approximate the variance of a function $s(\hat{\theta})$ of a statistic $\hat{\theta}$. Suppose $\hat{\theta}$ has mean/variance $(\theta, \sigma^2)$, and consider the approximation $s(\hat{\theta}) \approx s(\theta)+s'(\theta)(\hat{\theta}-\theta)$. Then, $var[s(\hat{\theta})]\approx var[s(\theta)+s'(\theta)\hat{\theta}-s'(\theta)\theta]=\vert s'(\theta)\vert^2 \sigma^2$. 

For example, we can compute the *standard error* of *mean square* $\bar{x}^2$ using delta method. Let $s(\hat{\theta})=\hat{\theta}^2$, $\hat{\theta}=\bar{x}^2$. Then, $var[s(\hat{\theta})]=\vert s'(\theta)\vert^2 \sigma^2=\vert 2\hat{\theta}\vert^2 \hat{se}^2$ ... (by using the definition of $se$). Finally, $se(s(\hat{\theta}))=2\theta\hat{se}$ and $se(\bar{x}^2) \doteq 2\vert\bar{x}\vert\hat{se}$ 

**3. Parametric families and maximum likelihood theory.** Theoretical expressions for the standard error of a maximum likelihood estimate (MLE) are discussed in Chapters 4 and 5. 

**4. Simulation and the bootsrap.** Thanks for the modern computation technology, we can implement semi-infinite sequence of trials. Estimator is no need to be smoothly defined function. 

**5. Pivotal statistics.** A pivotal statistic $\hat{\theta}=t(x)$ is one whose distribution does not depend upon the underlying probability distribution $F$. The one example is Student's two-sample t-test. 

In a two-sample problem the statistician observes two sets of numbers,

$$x_1=(x_{11}, x_{12}, ..., x_{1n_{1}}), x_2 = (x_{21}, x_{22}, ..., x_{2n_{2}})$$

It is **assumed** that the underlying probability distribution $F_1$ and $F_2$ is normal. Under null hypothesis $H_0$ ($x_1, x_2$ come from the same distribution), two-sample t-statistic $t=\frac{\bar{x}_1-\bar{x}_2}{\hat{sd}}$ where $\hat{sd}=\hat{\sigma}(\frac{1}{n_1}+\frac{1}{n_2})^{1/2}$ follows Student's t distribution with $n_1+n_2-2$ degree of freedom. This statistics is used to test the null hypothesis. 

## Frequentist Optimility

The *frequentist optimality* is finiding the best choice of $t(x)$ given model $F$. For example,

* the MLE is the optimum estimate in terms of minimum (asymtotic) standard error.
* Neyman-Pearson lemma provides an optimum hypothesis-testing algorithm.

